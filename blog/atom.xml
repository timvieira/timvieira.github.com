<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Graduate Descent</title><link href="http://timvieira.github.io/blog/" rel="alternate"></link><link href="/blog/atom.xml" rel="self"></link><id>http://timvieira.github.io/blog/</id><updated>2018-03-16T00:00:00-04:00</updated><entry><title>Black-box optimization</title><link href="http://timvieira.github.io/blog/post/2018/03/16/black-box-optimization/" rel="alternate"></link><published>2018-03-16T00:00:00-04:00</published><updated>2018-03-16T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2018-03-16:/blog/post/2018/03/16/black-box-optimization/</id><summary type="html">&lt;p&gt;Black-box optimization algorithms are a fantastic tool that everyone should be
aware of. I frequently use black-box optimization algorithms for prototyping and
when gradient-based algorithms fail,
e.g., because the function is not differentiable,
because the function is truly opaque (no gradients),
because the gradient would require too much memory to compute efficiently.&lt;/p&gt;
&lt;p&gt;From a young age, we are taught to love gradients and never learn about any
optimization algorithms other than gradient descent. I believe this obsession
has put us in a local optimum. I've been amazed at how few people know about
non-gradient algorithms for optimization. Although, this is slowly improving
thanks to the prevalence of hyperparameter optimization, so most people have
used random search and at least know of Bayesian optimization.&lt;/p&gt;
&lt;p&gt;There are many ways to optimize a function! The gradient just happens to have a
&lt;a href="/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/"&gt;beautiful&lt;/a&gt; and
&lt;a href="/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;computationally efficient&lt;/a&gt;
shortcut for finding &lt;em&gt;the direction of steepest descent&lt;/em&gt; in Euclidean space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is a descent direction anyway?&lt;/strong&gt; For minimizing an function &lt;span class="math"&gt;\(f:
\mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;, a descent direction for &lt;span class="math"&gt;\(f\)&lt;/span&gt; is a
&lt;span class="math"&gt;\((d+1)\)&lt;/span&gt;-dimensional hyperplane. The gradient gives a unique hyperplane that is
tangent to the surface of &lt;span class="math"&gt;\(f\)&lt;/span&gt; at the point &lt;span class="math"&gt;\(x\)&lt;/span&gt;; the &lt;span class="math"&gt;\((d+1)^{\text{th}}\)&lt;/span&gt;
coordinate comes from the value &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;&amp;mdash;think of it like a first-order
Taylor approximation to &lt;span class="math"&gt;\(f\)&lt;/span&gt; at &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The baseline:&lt;/strong&gt; Without access to gradient code, approximating&lt;em&gt; the gradient
&lt;/em&gt;takes &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; function evaluations via the finite-difference approximation to the
&lt;em&gt;gradient,&lt;sup id="fnref-twosidedfd"&gt;&lt;a class="footnote-ref" href="#fn-twosidedfd"&gt;1&lt;/a&gt;&lt;/sup&gt; which I've discussed a
&lt;/em&gt;&lt;a href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/"&gt;few&lt;/a&gt;
&lt;em&gt;&lt;a href="http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/"&gt;times&lt;/a&gt;. This
&lt;/em&gt;shouldn't be surprising since that's the size of the object we're looking for
*anyways!&lt;sup id="fnref-faster-but-noisy"&gt;&lt;a class="footnote-ref" href="#fn-faster-but-noisy"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can we do better?&lt;/strong&gt; Suppose we had &lt;span class="math"&gt;\((d+1)\)&lt;/span&gt; arbitrary points
&lt;span class="math"&gt;\(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(d+1)}\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt; with
values &lt;span class="math"&gt;\(f^{(i)} = f(\boldsymbol{x}^{(i)}).\)&lt;/span&gt; Can we find efficiently find a
descent direction without extra &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluations?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Nelder-Mead trick:&lt;/strong&gt; Take the worst-performing point in this set
&lt;span class="math"&gt;\(\boldsymbol{x}^{(\text{worst})}\)&lt;/span&gt; and consider moving that point through the
center-of-mass of the &lt;span class="math"&gt;\(d\)&lt;/span&gt; remaining points. Call this the NM direction. At some
point along that direction (think line search) there will be a good place to put
that point, which will make it the new best point. We can now repeat this
process: pick the worst point, reflect it through the center of mass, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The cost of finding the NM descent direction requires no additional function
   evaluations, which allows the method to be very frugal with function
   evaluations. Of course, stepping in the search direction should use line
   search, which will require additional function evaluations; gradient-based
   methods also benefit from line search.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finding the worst point can be done in time &lt;span class="math"&gt;\(\mathcal{O}(\log d)\)&lt;/span&gt; using a
   &lt;a href="https://en.wikipedia.org/wiki/Heap_(data_structure)"&gt;heap&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This NM direction might is not the steepest descent direction&amp;mdash;like the
   gradient&amp;mdash;but it does give a reasonable descent direction to
   follow. Often, the NM direction is more useful than the gradient direction
   because it is not based on an infinitesimal ball around the current point
   like the gradient. NM often "works" on noisy and nonsmooth functions where
   gradients do not exist.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On high-dimensional problems, NM requires a significant number of "warm up"
 &lt;em&gt;function&lt;/em&gt; evaluations before it can take its first informed step. Whereas,
 gradient descent could plausibly CONVERGE in fewer &lt;em&gt;gradient&lt;/em&gt; evaluations
 (assuming sufficiently "nice" functions)! So, if you have high-dimensional
 problem and efficient gradients, use them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In three dimensions, we can visualize this as a tetrahedron with corners that
   "stick" to the surface of the function. At each iterations, the highest
   (i.e., worst performing) point is the one most likely to be affected by
   "gravity" which causes it to flip through the middle of the blob, as the
   other points stay stuck.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;
   &lt;img alt="Nelder-Mead animation" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Nelder-Mead_Himmelblau.gif/320px-Nelder-Mead_Himmelblau.gif"&gt;
   &lt;br/&gt;&lt;em&gt;(animation source: Wikipedia page for Nelder-Mead)&lt;/em&gt;
   &lt;/center&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is exactly the descent direction used in the
   &lt;a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method"&gt;Nelder-Mead algorithm&lt;/a&gt;
   (Nelder &amp;amp; Mead, 1965), which happens to be a great default algorithm for
   locally optimizing functions without access to gradients. Matlab and scipy
   users may know it better as
   &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html"&gt;&lt;code&gt;fmin&lt;/code&gt;&lt;/a&gt;.
   There are some additional "search moves" required to turn NM into a robust
   algorithm; these include shrinking and growing the set of points. I won't try
   to make yet another tutorial on the specifics of Nelder-Mead, as several
   already exist, but rather bring it to your attention as a plausible approach
   for efficiently finding descent directions. You can find a tutorial with
   plenty of visualization on its
   &lt;a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method"&gt;Wikipedia page&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;I described the Nelder-Mead search direction as an efficient way to leverage
past function evaluations to find a descent directions, which serves as a
reasonable alternative to gradients when they are unavailable (or not useful).&lt;/p&gt;
&lt;h3&gt;Further reading&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There are plenty of other black-box optimization algorithms out there. The
   wiki page on
   &lt;a href="https://en.wikipedia.org/wiki/Derivative-free_optimization"&gt;derivative-free optimization&lt;/a&gt;
   is a good starting point for learning more.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-twosidedfd"&gt;
&lt;p&gt;Of course, it's better to use the two-sided difference
approximation to the gradient in practice, which requires &lt;span class="math"&gt;\(2 \cdot d\)&lt;/span&gt; function
evaluations, not &lt;span class="math"&gt;\(d+1\)&lt;/span&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref-twosidedfd" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-faster-but-noisy"&gt;
&lt;p&gt;Note that we can get noisy, approximations with much fewer
than &lt;span class="math"&gt;\(\mathcal{O}(d)\)&lt;/span&gt; evaluations, e.g.,
&lt;a href="https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation"&gt;SPSA&lt;/a&gt;
or even REINFORCE obtain gradients approximations with just &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt;
evaluations per iteration.&amp;#160;&lt;a class="footnote-backref" href="#fnref-faster-but-noisy" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Black-box optimization algorithms are a fantastic tool that everyone should be
aware of. I frequently use black-box optimization algorithms for prototyping and
when gradient-based algorithms fail,
e.g., because the function is not differentiable,
because the function is truly opaque (no gradients),
because the gradient would require too much memory to compute efficiently.&lt;/p&gt;
&lt;p&gt;From a young age, we are taught to love gradients and never learn about any
optimization algorithms other than gradient descent. I believe this obsession
has put us in a local optimum. I've been amazed at how few people know about
non-gradient algorithms for optimization. Although, this is slowly improving
thanks to the prevalence of hyperparameter optimization, so most people have
used random search and at least know of Bayesian optimization.&lt;/p&gt;
&lt;p&gt;There are many ways to optimize a function! The gradient just happens to have a
&lt;a href="/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/"&gt;beautiful&lt;/a&gt; and
&lt;a href="/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;computationally efficient&lt;/a&gt;
shortcut for finding &lt;em&gt;the direction of steepest descent&lt;/em&gt; in Euclidean space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is a descent direction anyway?&lt;/strong&gt; For minimizing an function &lt;span class="math"&gt;\(f:
\mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;, a descent direction for &lt;span class="math"&gt;\(f\)&lt;/span&gt; is a
&lt;span class="math"&gt;\((d+1)\)&lt;/span&gt;-dimensional hyperplane. The gradient gives a unique hyperplane that is
tangent to the surface of &lt;span class="math"&gt;\(f\)&lt;/span&gt; at the point &lt;span class="math"&gt;\(x\)&lt;/span&gt;; the &lt;span class="math"&gt;\((d+1)^{\text{th}}\)&lt;/span&gt;
coordinate comes from the value &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;&amp;mdash;think of it like a first-order
Taylor approximation to &lt;span class="math"&gt;\(f\)&lt;/span&gt; at &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The baseline:&lt;/strong&gt; Without access to gradient code, approximating&lt;em&gt; the gradient
&lt;/em&gt;takes &lt;span class="math"&gt;\(d+1\)&lt;/span&gt; function evaluations via the finite-difference approximation to the
&lt;em&gt;gradient,&lt;sup id="fnref-twosidedfd"&gt;&lt;a class="footnote-ref" href="#fn-twosidedfd"&gt;1&lt;/a&gt;&lt;/sup&gt; which I've discussed a
&lt;/em&gt;&lt;a href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/"&gt;few&lt;/a&gt;
&lt;em&gt;&lt;a href="http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/"&gt;times&lt;/a&gt;. This
&lt;/em&gt;shouldn't be surprising since that's the size of the object we're looking for
*anyways!&lt;sup id="fnref-faster-but-noisy"&gt;&lt;a class="footnote-ref" href="#fn-faster-but-noisy"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Can we do better?&lt;/strong&gt; Suppose we had &lt;span class="math"&gt;\((d+1)\)&lt;/span&gt; arbitrary points
&lt;span class="math"&gt;\(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(d+1)}\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt; with
values &lt;span class="math"&gt;\(f^{(i)} = f(\boldsymbol{x}^{(i)}).\)&lt;/span&gt; Can we find efficiently find a
descent direction without extra &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluations?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Nelder-Mead trick:&lt;/strong&gt; Take the worst-performing point in this set
&lt;span class="math"&gt;\(\boldsymbol{x}^{(\text{worst})}\)&lt;/span&gt; and consider moving that point through the
center-of-mass of the &lt;span class="math"&gt;\(d\)&lt;/span&gt; remaining points. Call this the NM direction. At some
point along that direction (think line search) there will be a good place to put
that point, which will make it the new best point. We can now repeat this
process: pick the worst point, reflect it through the center of mass, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The cost of finding the NM descent direction requires no additional function
   evaluations, which allows the method to be very frugal with function
   evaluations. Of course, stepping in the search direction should use line
   search, which will require additional function evaluations; gradient-based
   methods also benefit from line search.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finding the worst point can be done in time &lt;span class="math"&gt;\(\mathcal{O}(\log d)\)&lt;/span&gt; using a
   &lt;a href="https://en.wikipedia.org/wiki/Heap_(data_structure)"&gt;heap&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This NM direction might is not the steepest descent direction&amp;mdash;like the
   gradient&amp;mdash;but it does give a reasonable descent direction to
   follow. Often, the NM direction is more useful than the gradient direction
   because it is not based on an infinitesimal ball around the current point
   like the gradient. NM often "works" on noisy and nonsmooth functions where
   gradients do not exist.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On high-dimensional problems, NM requires a significant number of "warm up"
 &lt;em&gt;function&lt;/em&gt; evaluations before it can take its first informed step. Whereas,
 gradient descent could plausibly CONVERGE in fewer &lt;em&gt;gradient&lt;/em&gt; evaluations
 (assuming sufficiently "nice" functions)! So, if you have high-dimensional
 problem and efficient gradients, use them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In three dimensions, we can visualize this as a tetrahedron with corners that
   "stick" to the surface of the function. At each iterations, the highest
   (i.e., worst performing) point is the one most likely to be affected by
   "gravity" which causes it to flip through the middle of the blob, as the
   other points stay stuck.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;
   &lt;img alt="Nelder-Mead animation" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Nelder-Mead_Himmelblau.gif/320px-Nelder-Mead_Himmelblau.gif"&gt;
   &lt;br/&gt;&lt;em&gt;(animation source: Wikipedia page for Nelder-Mead)&lt;/em&gt;
   &lt;/center&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is exactly the descent direction used in the
   &lt;a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method"&gt;Nelder-Mead algorithm&lt;/a&gt;
   (Nelder &amp;amp; Mead, 1965), which happens to be a great default algorithm for
   locally optimizing functions without access to gradients. Matlab and scipy
   users may know it better as
   &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html"&gt;&lt;code&gt;fmin&lt;/code&gt;&lt;/a&gt;.
   There are some additional "search moves" required to turn NM into a robust
   algorithm; these include shrinking and growing the set of points. I won't try
   to make yet another tutorial on the specifics of Nelder-Mead, as several
   already exist, but rather bring it to your attention as a plausible approach
   for efficiently finding descent directions. You can find a tutorial with
   plenty of visualization on its
   &lt;a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method"&gt;Wikipedia page&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;I described the Nelder-Mead search direction as an efficient way to leverage
past function evaluations to find a descent directions, which serves as a
reasonable alternative to gradients when they are unavailable (or not useful).&lt;/p&gt;
&lt;h3&gt;Further reading&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There are plenty of other black-box optimization algorithms out there. The
   wiki page on
   &lt;a href="https://en.wikipedia.org/wiki/Derivative-free_optimization"&gt;derivative-free optimization&lt;/a&gt;
   is a good starting point for learning more.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-twosidedfd"&gt;
&lt;p&gt;Of course, it's better to use the two-sided difference
approximation to the gradient in practice, which requires &lt;span class="math"&gt;\(2 \cdot d\)&lt;/span&gt; function
evaluations, not &lt;span class="math"&gt;\(d+1\)&lt;/span&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref-twosidedfd" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-faster-but-noisy"&gt;
&lt;p&gt;Note that we can get noisy, approximations with much fewer
than &lt;span class="math"&gt;\(\mathcal{O}(d)\)&lt;/span&gt; evaluations, e.g.,
&lt;a href="https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation"&gt;SPSA&lt;/a&gt;
or even REINFORCE obtain gradients approximations with just &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt;
evaluations per iteration.&amp;#160;&lt;a class="footnote-backref" href="#fnref-faster-but-noisy" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="optimization"></category><category term="calculus"></category></entry><entry><title>Backprop is not just the chain rule</title><link href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/" rel="alternate"></link><published>2017-08-18T00:00:00-04:00</published><updated>2017-08-18T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2017-08-18:/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/</id><summary type="html">&lt;p&gt;Almost everyone I know says that "backprop is just the chain rule." Although
that's &lt;em&gt;basically true&lt;/em&gt;, there are some subtle and beautiful things about
automatic differentiation techniques (including backprop) that will not be
appreciated with this &lt;em&gt;dismissive&lt;/em&gt; attitude.&lt;/p&gt;
&lt;p&gt;This leads to a poor understanding. As
&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;I have ranted before&lt;/a&gt;:
people do not understand basic facts about autodiff.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt; is provably as fast as evaluating &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Let that sink in. Computing the gradient&amp;mdash;an essential incredient to
efficient optimization&amp;mdash;is no slower to compute than the function
itself. Contrast that with the finite-difference gradient approximation, which
is quite accurate, but its runtime is $\textrm{dim}(x)$ times slower than
evaluating $f$
([discussed here](http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/))!
--&gt;

&lt;ul&gt;
&lt;li&gt;Code for &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt; can be derived by a rote program transformation, even
  if the code has control flow structures like loops and intermediate variables
  (as long as the control flow is independent of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). You can even do this
  "automatic" transformation by hand!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Autodiff &lt;span class="math"&gt;\(\ne\)&lt;/span&gt; what you learned in calculus&lt;/h3&gt;
&lt;p&gt;Let's try to understand the difference between autodiff and the type of
differentiation that you learned in calculus, which is called &lt;em&gt;symbolic&lt;/em&gt;
differentiation.&lt;/p&gt;
&lt;p&gt;I'm going to use an example from
&lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's notes&lt;/a&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) = \exp(\exp(x) + \exp(x)^2) + \sin(\exp(x) + \exp(x)^2).
$$&lt;/div&gt;
&lt;!--
If we plug-and-chug with the chain rule, we get a correct expression for the
derivative,
$$\small
\frac{\partial f}{\partial x} =
\exp(\exp(x) + \exp(x)^2) (\exp(x) + 2 \exp(x) \exp(x)) \\
\quad\quad\small+ \cos(\exp(x) + \exp(x)^2) (\exp(x) + 2 \exp(x) \exp(x)).
$$

However, this expression leaves something to be desired because it has lot of
repeated evaluations of the same function. This is clearly bad, if we want to
turn it into source code.
--&gt;

&lt;p&gt;If we were writing &lt;em&gt;a program&lt;/em&gt; (e.g., in Python) to compute &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we'd definitely
take advantage of the fact that it has a lot of repeated evaluations for
efficiency.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Symbolic differentiation would have to use the "flat" version of this function,
so no intermediate variable &lt;span class="math"&gt;\(\Rightarrow\)&lt;/span&gt; slow.&lt;/p&gt;
&lt;p&gt;Automatic differentiation lets us differentiate a program with &lt;em&gt;intermediate&lt;/em&gt;
variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The rules for transforming the code for a function into code for the gradient
  are really minimal (fewer things to memorize!). Additionally, the rules are
  more general than in symbolic case because they handle as superset of
  programs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quite &lt;a href="http://conal.net/papers/beautiful-differentiation/"&gt;beautifully&lt;/a&gt;, the
  program for the gradient &lt;em&gt;has exactly the same structure&lt;/em&gt; as the function,
  which implies that we get the same runtime (up to some constants factors).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I won't give the details of how to execute the backpropagation transform to the
program. You can get that from
&lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's notes&lt;/a&gt;
and many other good
resources. &lt;a href="https://gist.github.com/timvieira/39e27756e1226c2dbd6c36e83b648ec2"&gt;Here's some code&lt;/a&gt;
that I wrote that accompanies to the &lt;code&gt;f(x)&lt;/code&gt; example, which has a bunch of
comments describing the manual "automatic" differentiation process on &lt;code&gt;f(x)&lt;/code&gt;.&lt;/p&gt;
&lt;!--
Caveat: You might have seen some *limited* cases where an input variable was
reused, but chances are that it was something really simple like multiplication
or division, e.g., $\nabla\! \left[ f(x) \cdot g(x) \right] = f(x) \cdot g'(x)
+ f'(x) \cdot g(x)$, and you just memorized a rule. The rules of autodiff are
simpler and actually explains why there is a sum in the product rule. You can
also rederive the quotient rule without a hitch. I'm all about having fewer
things to memorize!
--&gt;

&lt;!--
You might hope that something like common subexpression elimination would save
the symbolic approach. Indeed that could be leveraged to improve any chunk of
code, but to match efficiency it's not needed! If we had needed to blow up the
computation to then shrink it down that would be much less efficient! The "flat"
version of a program can be exponentially larger than a version with reuse.
--&gt;

&lt;!-- Only sort of related: think of the exponential blow up in converting a
Boolean expression from conjunctive normal form to and disjunction normal.  --&gt;

&lt;h2&gt;Autodiff by the method of Lagrange multipliers&lt;/h2&gt;
&lt;p&gt;Let's view the intermediate variables in our optimization problem as simple
equality constraints in an equivalent &lt;em&gt;constrained&lt;/em&gt; optimization problem. It
turns out that the de facto method for handling constraints, the method Lagrange
multipliers, recovers &lt;em&gt;exactly&lt;/em&gt; the adjoints (intermediate derivatives) in the
backprop algorithm!&lt;/p&gt;
&lt;p&gt;Here's our example from earlier written in this constraint form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\underset{x}{\text{argmax}}\ &amp;amp; f \\
\text{s.t.} \quad
a &amp;amp;= \exp(x) \\
b &amp;amp;= a^2     \\
c &amp;amp;= a + b   \\
d &amp;amp;= \exp(c) \\
e &amp;amp;= \sin(c) \\
f &amp;amp;= d + e
\end{align*}
$$&lt;/div&gt;
&lt;h4&gt;The general formuation&lt;/h4&gt;
&lt;div class="math"&gt;\begin{align*}
  &amp;amp; \underset{\boldsymbol{x}}{\text{argmax}}\ z_n &amp;amp; \\
  &amp;amp; \text{s.t.}\quad z_i = x_i                          &amp;amp;\text{ for $1 \le i \le d$} \\
  &amp;amp; \phantom{\text{s.t.}}\quad z_i = f_i(z_{\alpha(i)}) &amp;amp;\text{ for $d &amp;lt; i \le n$} \\
  \end{align*}&lt;/div&gt;
&lt;p&gt;The first set of constraint (&lt;span class="math"&gt;\(1, \ldots, d\)&lt;/span&gt;) are a little silly. They are only
there to keep our formulation tidy. The variables in the program fall into three
categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;input variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;): &lt;span class="math"&gt;\(x_1, \ldots, x_d\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;intermediate variables&lt;/strong&gt;: (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;): &lt;span class="math"&gt;\(z_i = f_i(z_{\alpha(i)})\)&lt;/span&gt; for
  &lt;span class="math"&gt;\(1 \le i \le n\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\alpha(i)\)&lt;/span&gt; is a list of indices from &lt;span class="math"&gt;\(\{1, \ldots,
  n-1\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_{\alpha(i)}\)&lt;/span&gt; is the subvector of variables needed to evaluate
  &lt;span class="math"&gt;\(f_i(\cdot)\)&lt;/span&gt;. Minor detail: take &lt;span class="math"&gt;\(f_{1:d}\)&lt;/span&gt; to be the identity function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;output variable&lt;/strong&gt; (&lt;span class="math"&gt;\(z_n\)&lt;/span&gt;): We assume that our programs has a singled scalar
  output variable, &lt;span class="math"&gt;\(z_n\)&lt;/span&gt;, which represents the quantity we'd like to maximize.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- (It is possible to generalize this story to compute Jacobians of functions
with multi-variate outputs by "scalarizing" the objective, e.g., multiply the
outputs by a vector. This Gives an efficient program for computing Jacobian
vector products that can be used to extra Jacobians.)  --&gt;

&lt;p&gt;The relation &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a
&lt;a href="https://en.wikipedia.org/wiki/Dependency_graph"&gt;dependency graph&lt;/a&gt; among
variables. Thus, &lt;span class="math"&gt;\(\alpha(i)\)&lt;/span&gt; is the list of &lt;em&gt;incoming&lt;/em&gt; edges to node &lt;span class="math"&gt;\(i\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\beta(j) = \{ i: j \in \alpha(i) \}\)&lt;/span&gt; is the set of &lt;em&gt;outgoing&lt;/em&gt; edges. For now,
we'll assume that the dependency graph given by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is ① acyclic: no &lt;span class="math"&gt;\(z_i\)&lt;/span&gt;
can transitively depend on itself.  ② single-assignment: each &lt;span class="math"&gt;\(z_i\)&lt;/span&gt; appears on
the left-hand side of &lt;em&gt;exactly one&lt;/em&gt; equation.  We'll discuss relaxing these
assumptions in &lt;a href="#lagrange-backprop-generalization"&gt;§ Generalizations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The standard way to solve a constrained optimization is to use the method
Lagrange multipliers, which converts a &lt;em&gt;constrained&lt;/em&gt; optimization problem into
an &lt;em&gt;unconstrained&lt;/em&gt; problem with a few more variables &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; (one
per &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; constraint), called Lagrange multipliers.&lt;/p&gt;
&lt;h4&gt;The Lagrangian&lt;/h4&gt;
&lt;p&gt;To handle constaints, let's dig up a tool from our calculus class,
&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;the method of Lagrange multipliers&lt;/a&gt;,
which converts a &lt;em&gt;constrained&lt;/em&gt; optimization probelm into an &lt;em&gt;unconstrainted&lt;/em&gt;
one. The unconstrained version is called "the Lagrangian" of the constrained
problem. Here is its form for our task,&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathcal{L}\left(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\lambda}\right)
= z_n - \sum_{i=1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right).
$$&lt;/div&gt;
&lt;p&gt;Optimizing the Lagrangian amounts to solving the following nonlinear system of
equations, which give necessary, but not sufficient, conditions for optimality,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla \mathcal{L}\left(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\lambda}\right) = 0.
$$&lt;/div&gt;
&lt;p&gt;Let's look a little closer at the Lagrangian conditions by breaking up the
system of equations into salient parts, corresponding to which variable types
are affected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intermediate variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;): Optimizing the
multipliers&amp;mdash;i.e., setting the gradient of Lagrangian
w.r.t. &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; to zero&amp;mdash;ensures that the constraints on
intermediate variables are satisfied.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\nabla_{\! \lambda_i} \mathcal{L}
= z_i - f_i(z_{\alpha(i)}) = 0
\quad\Leftrightarrow\quad z_i = f_i(z_{\alpha(i)})
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We can use forward propagation to satisfy these equations, which we may regard
as a block-coordinate step in the context of optimizing the &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;.&lt;/p&gt;
&lt;!--GENERALIZATION:
However, if they are cyclic dependencies we may need to
solve a nonlinear system of equations. (TODO: it's unclear what the more general
cyclic setting is. Perhaps I should having a running example of a cyclic program
and an acyclic program.)
--&gt;

&lt;p&gt;&lt;strong&gt;Lagrange multipliers&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt;, excluding &lt;span class="math"&gt;\(\lambda_n\)&lt;/span&gt;):
  Setting the gradient of the &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; w.r.t. the intermediate variables
  equal to zeros tells us what to do with the intermediate multipliers.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray*}
0 &amp;amp;=&amp;amp; \nabla_{\! z_j} \mathcal{L} \\
&amp;amp;=&amp;amp; \nabla_{\! z_j}\! \left[ z_n - \sum_{i=1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right) \right] \\
&amp;amp;=&amp;amp; - \sum_{i=1}^n \lambda_i \nabla_{\! z_j}\! \left[ \left( z_i - f_i(z_{\alpha(i)}) \right) \right] \\
&amp;amp;=&amp;amp; - \left( \sum_{i=1}^n \lambda_i \nabla_{\! z_j}\! \left[ z_i \right] \right) + \left( \sum_{i=1}^n \lambda_i \nabla_{\! z_j}\! \left[ f_i(z_{\alpha(i)}) \right] \right) \\
&amp;amp;=&amp;amp; - \lambda_j + \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} \\
&amp;amp;\Updownarrow&amp;amp; \\
\lambda_j &amp;amp;=&amp;amp; \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} \\
\end{eqnarray*}&lt;/div&gt;
&lt;p&gt;Clearly, &lt;span class="math"&gt;\(\frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(j \notin
\alpha(i)\)&lt;/span&gt;, which is why the &lt;span class="math"&gt;\(\beta(j)\)&lt;/span&gt; notation came in handy. By assumption,
the local derivatives, &lt;span class="math"&gt;\(\frac{\partial f_i(z_{\alpha(i)})}{\partial z_j}\)&lt;/span&gt; for &lt;span class="math"&gt;\(j
\in \alpha(i)\)&lt;/span&gt;, are easy to calculate&amp;mdash;we don't even need the chain rule to
compute them because they are simple function applications without
composition. Similar to the equations for &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;, solving this linear
system is another block-coordinate step.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Key observation&lt;/em&gt;: The last equation for &lt;span class="math"&gt;\(\lambda_j\)&lt;/span&gt; should look very familiar:
It is exactly the equation used in backpropagation! It says that we sum
&lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; of nodes that immediately depend on &lt;span class="math"&gt;\(j\)&lt;/span&gt; where we scaled each
&lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; by the derivative of the function that directly relates &lt;span class="math"&gt;\(i\)&lt;/span&gt; and
&lt;span class="math"&gt;\(j\)&lt;/span&gt;. You should think of the scaling as a "unit conversion" from derivatives of
type &lt;span class="math"&gt;\(i\)&lt;/span&gt; to derivatives of type &lt;span class="math"&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output mutliplier&lt;/strong&gt; (&lt;span class="math"&gt;\(\lambda_n\)&lt;/span&gt;): Here we follow the same pattern as for
  intermediate multipliers.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
0 &amp;amp;=&amp;amp; \nabla_{\! z_n}\! \left[ z_n - \sum_{i=1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right) \right] &amp;amp;=&amp;amp; 1 - \lambda_n \\
 &amp;amp;\Updownarrow&amp;amp; \\
 \lambda_n &amp;amp;=&amp;amp; 1
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Input multipliers&lt;/strong&gt; &lt;span class="math"&gt;\((\boldsymbol{\lambda}_{1:d})\)&lt;/span&gt;: Our dummy constraints
  gives us &lt;span class="math"&gt;\(\boldsymbol{\lambda}_{1:d}\)&lt;/span&gt;, which are conveniently equal to the
  gradient of the function we're optimizing:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{\!\boldsymbol{x}} f(\boldsymbol{x}) = \boldsymbol{\lambda}_{1:d}.
$$&lt;/div&gt;
&lt;p&gt;Of course, this interpretation is only precise when ① the constraints are
satisfied (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; equations) and ② the linear system on multipliers is
satisfied (&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; equations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;): Unforunately, the there is no
  closed-form solution to how to set &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. For this we resort to
  something like gradient ascent. Conveniently, &lt;span class="math"&gt;\(\nabla_{\!\boldsymbol{x}}
  f(\boldsymbol{x}) = \boldsymbol{\lambda}_{1:d}\)&lt;/span&gt;, which we can use to optimize
  &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;!&lt;/p&gt;
&lt;div id="lagrange-backprop-generalization"&gt;&lt;/div&gt;

&lt;h3&gt;Generalizations&lt;/h3&gt;
&lt;p&gt;We can think of these equations for &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; as a simple &lt;em&gt;linear&lt;/em&gt;
system of equations, which we are solving by back-substitution when we use the
backpropagation method. The reason why back-substitution is sufficient for the
linear system (i.e., we don't need a &lt;em&gt;full&lt;/em&gt; linear system solver) is that the
dependency graph induced by the &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; relation is acyclic. If we had needed a
full linear system solver, the solution would take &lt;span class="math"&gt;\(\mathcal{O}(n^3)\)&lt;/span&gt; time
instead of linear time, seriously blowing-up our nice runtime!&lt;/p&gt;
&lt;p&gt;This connection to linear systems is interesting: It tells us that we &lt;em&gt;could&lt;/em&gt;
compute gradients in cyclic graphs. All we'd need is to run a linear system
solver to stich together our gradients! That is exactly what the
&lt;a href="https://en.wikipedia.org/wiki/Implicit_function_theorem"&gt;implicit function theorem&lt;/a&gt;
says!&lt;/p&gt;
&lt;p&gt;Cyclic constraints add some expressive powerful to "constraint language" and
it's interesting that we can still efficiently compute gradients in this
setting. An example of what a general type of cyclic constraint looks like is&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \underset{\boldsymbol{x}}{\text{argmax}}\, z_n \\
&amp;amp; \text{s.t.}\quad g(\boldsymbol{z}) = \boldsymbol{0} \\
&amp;amp; \text{and}\quad \boldsymbol{z}_{1:d} = \boldsymbol{x}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(g\)&lt;/span&gt; can be an any smooth multivariate function of the intermediate
variables! Of course, allowing cyclic constraints come at the cost of a
more-difficult analogue of "the forward pass" to satisfy the &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;
equations (if we want to keep it a block-coordinate step). The
&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; equations are now a linear system that requres a linear
solver (e.g., Guassian elimination).&lt;/p&gt;
&lt;p&gt;Example use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bi-level optimization: Solving an optimization problem with another one inside
  it. For example,
  &lt;a href="http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/"&gt;gradient-based hyperparameter optimization&lt;/a&gt;
  in machine learning. The implicit function theorem manages to get gradients of
  hyperparameters without needing to store any of intermediate states of the
  optimization algorithm used in the inner optimzation! This is a &lt;em&gt;huge&lt;/em&gt; memory
  saver since direct backprop on the inner gradient decent algorithm would
  require caching all intermediate states. Yikes!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cyclic constraints are useful in many graph algorithms. For example, computing
  gradients of edge weights in a general finite-state machine or, similarly,
  computing the value function in a Markov decision process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other methods for optimization?&lt;/h3&gt;
&lt;p&gt;The connection to Lagrangians brings tons of algorithms for constrained
optimization into the mix! We can imagine using more general algorithms for
optimizing our function and other ways of enforcing the constraints. We see
immediately that we could run optimization with adjoints set to values other
than those that backprop would set them to (i.e., we can optimize them like we'd
do in other algorithms for optimizing general Langrangians).&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Backprop is does not directly fall out of the the rules for differentiation
that you learned in calculus (e.g., the chain rule).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is because it operates on a more general family of functions: &lt;em&gt;programs&lt;/em&gt;
  which have &lt;em&gt;intermediate variables&lt;/em&gt;. Supporting intermediate variables is
  crucial for implementing both functions and their gradients efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I described how we could use something we did learn from calculus 101, the
method of Lagrange multipliers, to support optimization with intermediate
variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It turned out that backprop is a &lt;em&gt;particular instantiation&lt;/em&gt; of the method of
  Lagrange multipliers, involving block-coordinate steps for solving for the
  intermediates and multipliers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I also described a neat generalization to support &lt;em&gt;cyclic&lt;/em&gt; programs and I
  hinted at ideas for doing optimization a little differently, deviating from
  the de facto block-coordinate strategy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Levels of enlightenment" src="/blog/images/backprop-brain-meme.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After working out the connection between backprop and the method of Lagrange
  multipliers, I discovered following paper, which beat me to it. I don't think
  my version is too redundant.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Yann LeCun. (1988)
&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf"&gt;A Theoretical Framework from Back-Propagation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The backpropagation algorithm can be cleanly generalized from values to
  functionals!&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Alexander Grubb and J. Andrew Bagnell. (2010)
&lt;a href="https://t.co/5OW5xBT4Y1"&gt;Boosted Backpropagation Learning for Training Deep Modular Networks&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A great blog post that uses the implicit function theorem to &lt;em&gt;derive&lt;/em&gt; the
  method of Lagrange multipliers. He also touches on the connection to
  backpropgation.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ben Recht. (2016)
&lt;a href="http://www.argmin.net/2016/05/31/mechanics-of-lagrangians/"&gt;Mechanics of Lagrangians&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;I have coded up and tested the Lagrangian perspective on automatic
differentiation that I presented in this article. The code is available in this
&lt;a href="https://gist.github.com/timvieira/8addcb81dd622b0108e0e7e06af74185"&gt;gist&lt;/a&gt;.&lt;/p&gt;
&lt;script src="https://gist.github.com/timvieira/8addcb81dd622b0108e0e7e06af74185.js"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Almost everyone I know says that "backprop is just the chain rule." Although
that's &lt;em&gt;basically true&lt;/em&gt;, there are some subtle and beautiful things about
automatic differentiation techniques (including backprop) that will not be
appreciated with this &lt;em&gt;dismissive&lt;/em&gt; attitude.&lt;/p&gt;
&lt;p&gt;This leads to a poor understanding. As
&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;I have ranted before&lt;/a&gt;:
people do not understand basic facts about autodiff.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt; is provably as fast as evaluating &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Let that sink in. Computing the gradient&amp;mdash;an essential incredient to
efficient optimization&amp;mdash;is no slower to compute than the function
itself. Contrast that with the finite-difference gradient approximation, which
is quite accurate, but its runtime is $\textrm{dim}(x)$ times slower than
evaluating $f$
([discussed here](http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/))!
--&gt;

&lt;ul&gt;
&lt;li&gt;Code for &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt; can be derived by a rote program transformation, even
  if the code has control flow structures like loops and intermediate variables
  (as long as the control flow is independent of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). You can even do this
  "automatic" transformation by hand!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Autodiff &lt;span class="math"&gt;\(\ne\)&lt;/span&gt; what you learned in calculus&lt;/h3&gt;
&lt;p&gt;Let's try to understand the difference between autodiff and the type of
differentiation that you learned in calculus, which is called &lt;em&gt;symbolic&lt;/em&gt;
differentiation.&lt;/p&gt;
&lt;p&gt;I'm going to use an example from
&lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's notes&lt;/a&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) = \exp(\exp(x) + \exp(x)^2) + \sin(\exp(x) + \exp(x)^2).
$$&lt;/div&gt;
&lt;!--
If we plug-and-chug with the chain rule, we get a correct expression for the
derivative,
$$\small
\frac{\partial f}{\partial x} =
\exp(\exp(x) + \exp(x)^2) (\exp(x) + 2 \exp(x) \exp(x)) \\
\quad\quad\small+ \cos(\exp(x) + \exp(x)^2) (\exp(x) + 2 \exp(x) \exp(x)).
$$

However, this expression leaves something to be desired because it has lot of
repeated evaluations of the same function. This is clearly bad, if we want to
turn it into source code.
--&gt;

&lt;p&gt;If we were writing &lt;em&gt;a program&lt;/em&gt; (e.g., in Python) to compute &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we'd definitely
take advantage of the fact that it has a lot of repeated evaluations for
efficiency.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Symbolic differentiation would have to use the "flat" version of this function,
so no intermediate variable &lt;span class="math"&gt;\(\Rightarrow\)&lt;/span&gt; slow.&lt;/p&gt;
&lt;p&gt;Automatic differentiation lets us differentiate a program with &lt;em&gt;intermediate&lt;/em&gt;
variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The rules for transforming the code for a function into code for the gradient
  are really minimal (fewer things to memorize!). Additionally, the rules are
  more general than in symbolic case because they handle as superset of
  programs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quite &lt;a href="http://conal.net/papers/beautiful-differentiation/"&gt;beautifully&lt;/a&gt;, the
  program for the gradient &lt;em&gt;has exactly the same structure&lt;/em&gt; as the function,
  which implies that we get the same runtime (up to some constants factors).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I won't give the details of how to execute the backpropagation transform to the
program. You can get that from
&lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's notes&lt;/a&gt;
and many other good
resources. &lt;a href="https://gist.github.com/timvieira/39e27756e1226c2dbd6c36e83b648ec2"&gt;Here's some code&lt;/a&gt;
that I wrote that accompanies to the &lt;code&gt;f(x)&lt;/code&gt; example, which has a bunch of
comments describing the manual "automatic" differentiation process on &lt;code&gt;f(x)&lt;/code&gt;.&lt;/p&gt;
&lt;!--
Caveat: You might have seen some *limited* cases where an input variable was
reused, but chances are that it was something really simple like multiplication
or division, e.g., $\nabla\! \left[ f(x) \cdot g(x) \right] = f(x) \cdot g'(x)
+ f'(x) \cdot g(x)$, and you just memorized a rule. The rules of autodiff are
simpler and actually explains why there is a sum in the product rule. You can
also rederive the quotient rule without a hitch. I'm all about having fewer
things to memorize!
--&gt;

&lt;!--
You might hope that something like common subexpression elimination would save
the symbolic approach. Indeed that could be leveraged to improve any chunk of
code, but to match efficiency it's not needed! If we had needed to blow up the
computation to then shrink it down that would be much less efficient! The "flat"
version of a program can be exponentially larger than a version with reuse.
--&gt;

&lt;!-- Only sort of related: think of the exponential blow up in converting a
Boolean expression from conjunctive normal form to and disjunction normal.  --&gt;

&lt;h2&gt;Autodiff by the method of Lagrange multipliers&lt;/h2&gt;
&lt;p&gt;Let's view the intermediate variables in our optimization problem as simple
equality constraints in an equivalent &lt;em&gt;constrained&lt;/em&gt; optimization problem. It
turns out that the de facto method for handling constraints, the method Lagrange
multipliers, recovers &lt;em&gt;exactly&lt;/em&gt; the adjoints (intermediate derivatives) in the
backprop algorithm!&lt;/p&gt;
&lt;p&gt;Here's our example from earlier written in this constraint form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\underset{x}{\text{argmax}}\ &amp;amp; f \\
\text{s.t.} \quad
a &amp;amp;= \exp(x) \\
b &amp;amp;= a^2     \\
c &amp;amp;= a + b   \\
d &amp;amp;= \exp(c) \\
e &amp;amp;= \sin(c) \\
f &amp;amp;= d + e
\end{align*}
$$&lt;/div&gt;
&lt;h4&gt;The general formuation&lt;/h4&gt;
&lt;div class="math"&gt;\begin{align*}
  &amp;amp; \underset{\boldsymbol{x}}{\text{argmax}}\ z_n &amp;amp; \\
  &amp;amp; \text{s.t.}\quad z_i = x_i                          &amp;amp;\text{ for $1 \le i \le d$} \\
  &amp;amp; \phantom{\text{s.t.}}\quad z_i = f_i(z_{\alpha(i)}) &amp;amp;\text{ for $d &amp;lt; i \le n$} \\
  \end{align*}&lt;/div&gt;
&lt;p&gt;The first set of constraint (&lt;span class="math"&gt;\(1, \ldots, d\)&lt;/span&gt;) are a little silly. They are only
there to keep our formulation tidy. The variables in the program fall into three
categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;input variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;): &lt;span class="math"&gt;\(x_1, \ldots, x_d\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;intermediate variables&lt;/strong&gt;: (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;): &lt;span class="math"&gt;\(z_i = f_i(z_{\alpha(i)})\)&lt;/span&gt; for
  &lt;span class="math"&gt;\(1 \le i \le n\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\alpha(i)\)&lt;/span&gt; is a list of indices from &lt;span class="math"&gt;\(\{1, \ldots,
  n-1\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_{\alpha(i)}\)&lt;/span&gt; is the subvector of variables needed to evaluate
  &lt;span class="math"&gt;\(f_i(\cdot)\)&lt;/span&gt;. Minor detail: take &lt;span class="math"&gt;\(f_{1:d}\)&lt;/span&gt; to be the identity function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;output variable&lt;/strong&gt; (&lt;span class="math"&gt;\(z_n\)&lt;/span&gt;): We assume that our programs has a singled scalar
  output variable, &lt;span class="math"&gt;\(z_n\)&lt;/span&gt;, which represents the quantity we'd like to maximize.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- (It is possible to generalize this story to compute Jacobians of functions
with multi-variate outputs by "scalarizing" the objective, e.g., multiply the
outputs by a vector. This Gives an efficient program for computing Jacobian
vector products that can be used to extra Jacobians.)  --&gt;

&lt;p&gt;The relation &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a
&lt;a href="https://en.wikipedia.org/wiki/Dependency_graph"&gt;dependency graph&lt;/a&gt; among
variables. Thus, &lt;span class="math"&gt;\(\alpha(i)\)&lt;/span&gt; is the list of &lt;em&gt;incoming&lt;/em&gt; edges to node &lt;span class="math"&gt;\(i\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\beta(j) = \{ i: j \in \alpha(i) \}\)&lt;/span&gt; is the set of &lt;em&gt;outgoing&lt;/em&gt; edges. For now,
we'll assume that the dependency graph given by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is ① acyclic: no &lt;span class="math"&gt;\(z_i\)&lt;/span&gt;
can transitively depend on itself.  ② single-assignment: each &lt;span class="math"&gt;\(z_i\)&lt;/span&gt; appears on
the left-hand side of &lt;em&gt;exactly one&lt;/em&gt; equation.  We'll discuss relaxing these
assumptions in &lt;a href="#lagrange-backprop-generalization"&gt;§ Generalizations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The standard way to solve a constrained optimization is to use the method
Lagrange multipliers, which converts a &lt;em&gt;constrained&lt;/em&gt; optimization problem into
an &lt;em&gt;unconstrained&lt;/em&gt; problem with a few more variables &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; (one
per &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; constraint), called Lagrange multipliers.&lt;/p&gt;
&lt;h4&gt;The Lagrangian&lt;/h4&gt;
&lt;p&gt;To handle constaints, let's dig up a tool from our calculus class,
&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;the method of Lagrange multipliers&lt;/a&gt;,
which converts a &lt;em&gt;constrained&lt;/em&gt; optimization probelm into an &lt;em&gt;unconstrainted&lt;/em&gt;
one. The unconstrained version is called "the Lagrangian" of the constrained
problem. Here is its form for our task,&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathcal{L}\left(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\lambda}\right)
= z_n - \sum_{i=1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right).
$$&lt;/div&gt;
&lt;p&gt;Optimizing the Lagrangian amounts to solving the following nonlinear system of
equations, which give necessary, but not sufficient, conditions for optimality,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla \mathcal{L}\left(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\lambda}\right) = 0.
$$&lt;/div&gt;
&lt;p&gt;Let's look a little closer at the Lagrangian conditions by breaking up the
system of equations into salient parts, corresponding to which variable types
are affected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intermediate variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;): Optimizing the
multipliers&amp;mdash;i.e., setting the gradient of Lagrangian
w.r.t. &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; to zero&amp;mdash;ensures that the constraints on
intermediate variables are satisfied.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\nabla_{\! \lambda_i} \mathcal{L}
= z_i - f_i(z_{\alpha(i)}) = 0
\quad\Leftrightarrow\quad z_i = f_i(z_{\alpha(i)})
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We can use forward propagation to satisfy these equations, which we may regard
as a block-coordinate step in the context of optimizing the &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;.&lt;/p&gt;
&lt;!--GENERALIZATION:
However, if they are cyclic dependencies we may need to
solve a nonlinear system of equations. (TODO: it's unclear what the more general
cyclic setting is. Perhaps I should having a running example of a cyclic program
and an acyclic program.)
--&gt;

&lt;p&gt;&lt;strong&gt;Lagrange multipliers&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt;, excluding &lt;span class="math"&gt;\(\lambda_n\)&lt;/span&gt;):
  Setting the gradient of the &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; w.r.t. the intermediate variables
  equal to zeros tells us what to do with the intermediate multipliers.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray*}
0 &amp;amp;=&amp;amp; \nabla_{\! z_j} \mathcal{L} \\
&amp;amp;=&amp;amp; \nabla_{\! z_j}\! \left[ z_n - \sum_{i=1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right) \right] \\
&amp;amp;=&amp;amp; - \sum_{i=1}^n \lambda_i \nabla_{\! z_j}\! \left[ \left( z_i - f_i(z_{\alpha(i)}) \right) \right] \\
&amp;amp;=&amp;amp; - \left( \sum_{i=1}^n \lambda_i \nabla_{\! z_j}\! \left[ z_i \right] \right) + \left( \sum_{i=1}^n \lambda_i \nabla_{\! z_j}\! \left[ f_i(z_{\alpha(i)}) \right] \right) \\
&amp;amp;=&amp;amp; - \lambda_j + \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} \\
&amp;amp;\Updownarrow&amp;amp; \\
\lambda_j &amp;amp;=&amp;amp; \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} \\
\end{eqnarray*}&lt;/div&gt;
&lt;p&gt;Clearly, &lt;span class="math"&gt;\(\frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(j \notin
\alpha(i)\)&lt;/span&gt;, which is why the &lt;span class="math"&gt;\(\beta(j)\)&lt;/span&gt; notation came in handy. By assumption,
the local derivatives, &lt;span class="math"&gt;\(\frac{\partial f_i(z_{\alpha(i)})}{\partial z_j}\)&lt;/span&gt; for &lt;span class="math"&gt;\(j
\in \alpha(i)\)&lt;/span&gt;, are easy to calculate&amp;mdash;we don't even need the chain rule to
compute them because they are simple function applications without
composition. Similar to the equations for &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;, solving this linear
system is another block-coordinate step.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Key observation&lt;/em&gt;: The last equation for &lt;span class="math"&gt;\(\lambda_j\)&lt;/span&gt; should look very familiar:
It is exactly the equation used in backpropagation! It says that we sum
&lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; of nodes that immediately depend on &lt;span class="math"&gt;\(j\)&lt;/span&gt; where we scaled each
&lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; by the derivative of the function that directly relates &lt;span class="math"&gt;\(i\)&lt;/span&gt; and
&lt;span class="math"&gt;\(j\)&lt;/span&gt;. You should think of the scaling as a "unit conversion" from derivatives of
type &lt;span class="math"&gt;\(i\)&lt;/span&gt; to derivatives of type &lt;span class="math"&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output mutliplier&lt;/strong&gt; (&lt;span class="math"&gt;\(\lambda_n\)&lt;/span&gt;): Here we follow the same pattern as for
  intermediate multipliers.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
0 &amp;amp;=&amp;amp; \nabla_{\! z_n}\! \left[ z_n - \sum_{i=1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right) \right] &amp;amp;=&amp;amp; 1 - \lambda_n \\
 &amp;amp;\Updownarrow&amp;amp; \\
 \lambda_n &amp;amp;=&amp;amp; 1
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Input multipliers&lt;/strong&gt; &lt;span class="math"&gt;\((\boldsymbol{\lambda}_{1:d})\)&lt;/span&gt;: Our dummy constraints
  gives us &lt;span class="math"&gt;\(\boldsymbol{\lambda}_{1:d}\)&lt;/span&gt;, which are conveniently equal to the
  gradient of the function we're optimizing:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{\!\boldsymbol{x}} f(\boldsymbol{x}) = \boldsymbol{\lambda}_{1:d}.
$$&lt;/div&gt;
&lt;p&gt;Of course, this interpretation is only precise when ① the constraints are
satisfied (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; equations) and ② the linear system on multipliers is
satisfied (&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; equations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;): Unforunately, the there is no
  closed-form solution to how to set &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. For this we resort to
  something like gradient ascent. Conveniently, &lt;span class="math"&gt;\(\nabla_{\!\boldsymbol{x}}
  f(\boldsymbol{x}) = \boldsymbol{\lambda}_{1:d}\)&lt;/span&gt;, which we can use to optimize
  &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;!&lt;/p&gt;
&lt;div id="lagrange-backprop-generalization"&gt;&lt;/div&gt;

&lt;h3&gt;Generalizations&lt;/h3&gt;
&lt;p&gt;We can think of these equations for &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; as a simple &lt;em&gt;linear&lt;/em&gt;
system of equations, which we are solving by back-substitution when we use the
backpropagation method. The reason why back-substitution is sufficient for the
linear system (i.e., we don't need a &lt;em&gt;full&lt;/em&gt; linear system solver) is that the
dependency graph induced by the &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; relation is acyclic. If we had needed a
full linear system solver, the solution would take &lt;span class="math"&gt;\(\mathcal{O}(n^3)\)&lt;/span&gt; time
instead of linear time, seriously blowing-up our nice runtime!&lt;/p&gt;
&lt;p&gt;This connection to linear systems is interesting: It tells us that we &lt;em&gt;could&lt;/em&gt;
compute gradients in cyclic graphs. All we'd need is to run a linear system
solver to stich together our gradients! That is exactly what the
&lt;a href="https://en.wikipedia.org/wiki/Implicit_function_theorem"&gt;implicit function theorem&lt;/a&gt;
says!&lt;/p&gt;
&lt;p&gt;Cyclic constraints add some expressive powerful to "constraint language" and
it's interesting that we can still efficiently compute gradients in this
setting. An example of what a general type of cyclic constraint looks like is&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \underset{\boldsymbol{x}}{\text{argmax}}\, z_n \\
&amp;amp; \text{s.t.}\quad g(\boldsymbol{z}) = \boldsymbol{0} \\
&amp;amp; \text{and}\quad \boldsymbol{z}_{1:d} = \boldsymbol{x}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(g\)&lt;/span&gt; can be an any smooth multivariate function of the intermediate
variables! Of course, allowing cyclic constraints come at the cost of a
more-difficult analogue of "the forward pass" to satisfy the &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;
equations (if we want to keep it a block-coordinate step). The
&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; equations are now a linear system that requres a linear
solver (e.g., Guassian elimination).&lt;/p&gt;
&lt;p&gt;Example use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bi-level optimization: Solving an optimization problem with another one inside
  it. For example,
  &lt;a href="http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/"&gt;gradient-based hyperparameter optimization&lt;/a&gt;
  in machine learning. The implicit function theorem manages to get gradients of
  hyperparameters without needing to store any of intermediate states of the
  optimization algorithm used in the inner optimzation! This is a &lt;em&gt;huge&lt;/em&gt; memory
  saver since direct backprop on the inner gradient decent algorithm would
  require caching all intermediate states. Yikes!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cyclic constraints are useful in many graph algorithms. For example, computing
  gradients of edge weights in a general finite-state machine or, similarly,
  computing the value function in a Markov decision process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other methods for optimization?&lt;/h3&gt;
&lt;p&gt;The connection to Lagrangians brings tons of algorithms for constrained
optimization into the mix! We can imagine using more general algorithms for
optimizing our function and other ways of enforcing the constraints. We see
immediately that we could run optimization with adjoints set to values other
than those that backprop would set them to (i.e., we can optimize them like we'd
do in other algorithms for optimizing general Langrangians).&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Backprop is does not directly fall out of the the rules for differentiation
that you learned in calculus (e.g., the chain rule).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is because it operates on a more general family of functions: &lt;em&gt;programs&lt;/em&gt;
  which have &lt;em&gt;intermediate variables&lt;/em&gt;. Supporting intermediate variables is
  crucial for implementing both functions and their gradients efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I described how we could use something we did learn from calculus 101, the
method of Lagrange multipliers, to support optimization with intermediate
variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It turned out that backprop is a &lt;em&gt;particular instantiation&lt;/em&gt; of the method of
  Lagrange multipliers, involving block-coordinate steps for solving for the
  intermediates and multipliers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I also described a neat generalization to support &lt;em&gt;cyclic&lt;/em&gt; programs and I
  hinted at ideas for doing optimization a little differently, deviating from
  the de facto block-coordinate strategy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Levels of enlightenment" src="/blog/images/backprop-brain-meme.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After working out the connection between backprop and the method of Lagrange
  multipliers, I discovered following paper, which beat me to it. I don't think
  my version is too redundant.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Yann LeCun. (1988)
&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf"&gt;A Theoretical Framework from Back-Propagation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The backpropagation algorithm can be cleanly generalized from values to
  functionals!&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Alexander Grubb and J. Andrew Bagnell. (2010)
&lt;a href="https://t.co/5OW5xBT4Y1"&gt;Boosted Backpropagation Learning for Training Deep Modular Networks&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A great blog post that uses the implicit function theorem to &lt;em&gt;derive&lt;/em&gt; the
  method of Lagrange multipliers. He also touches on the connection to
  backpropgation.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ben Recht. (2016)
&lt;a href="http://www.argmin.net/2016/05/31/mechanics-of-lagrangians/"&gt;Mechanics of Lagrangians&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;I have coded up and tested the Lagrangian perspective on automatic
differentiation that I presented in this article. The code is available in this
&lt;a href="https://gist.github.com/timvieira/8addcb81dd622b0108e0e7e06af74185"&gt;gist&lt;/a&gt;.&lt;/p&gt;
&lt;script src="https://gist.github.com/timvieira/8addcb81dd622b0108e0e7e06af74185.js"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="calculus"></category><category term="automatic-differentiation"></category></entry><entry><title>Estimating means in a finite universe</title><link href="http://timvieira.github.io/blog/post/2017/07/03/estimating-means-in-a-finite-universe/" rel="alternate"></link><published>2017-07-03T00:00:00-04:00</published><updated>2017-07-03T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2017-07-03:/blog/post/2017/07/03/estimating-means-in-a-finite-universe/</id><summary type="html">&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;: In this post, I'm going to describe some efficient approaches
to estimating the mean of a random variable that takes on only finitely many
values. Despite the ubiquity of Monte Carlo estimation, it is really inefficient
for finite domains. I'll describe some lesser-known algorithms based on sampling
without replacement that can be adapted to estimating means.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we want to estimate an expectation of a derministic function
&lt;span class="math"&gt;\(f\)&lt;/span&gt; over a (large) finite universe of &lt;span class="math"&gt;\(n\)&lt;/span&gt; elements where each element &lt;span class="math"&gt;\(i\)&lt;/span&gt; has
probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu \overset{\tiny{\text{def}}}{=} \sum_{i=1}^n p_i f(i)
$$&lt;/div&gt;
&lt;p&gt;However, &lt;span class="math"&gt;\(f\)&lt;/span&gt; is too expensive to evaluate &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. So let's say that we have
&lt;span class="math"&gt;\(m \le n\)&lt;/span&gt; evaluations to form our estimate. (Obviously, if we're happy
evaluating &lt;span class="math"&gt;\(f\)&lt;/span&gt; a total of &lt;span class="math"&gt;\(n\)&lt;/span&gt; times, then we should just compute &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; exactly
with the definition above.)&lt;/p&gt;
&lt;!--
**Why I'm writing this post**: Monte Carlo is often used in designing algorithms
as a means to cheaply approximate intermediate expectations, think of stochastic
gradient descent as a prime example. However, in many cases, we have a *finite*
universe, i.e., we *could* enumerate all elements, but it's just inefficient to
do so. In other words, sampling is merely a choice made by the algorithm
designer, not a fundamental property of the environment, as it is typically in
statistics. What can we do to improve estimation in this special setting? I
won't get into bigger questions of how to design these algorithms, instead I'll
focus on this specific type of estimation problem.
--&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo:&lt;/strong&gt; The most well-known approach to this type of problem is Monte
Carlo (MC) estimation: sample &lt;span class="math"&gt;\(x^{(1)}, \ldots, x^{(m)}
\overset{\tiny\text{i.i.d.}}{\sim} p\)&lt;/span&gt;, return &lt;span class="math"&gt;\(\widehat{\mu}_{\text{MC}} =
\frac{1}{m} \sum_{i = 1}^m f(x^{(i)})\)&lt;/span&gt;. &lt;em&gt;Remarks&lt;/em&gt;: (1) Monte Carlo can be very
inefficient because it resamples high-probability items over and over again. (2)
We can improve efficiency&amp;mdash;measured in &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluations&amp;mdash;somewhat by
caching past evaluations of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. However, this introduces a serious &lt;em&gt;runtime&lt;/em&gt;
inefficiency and requires modifying the method to account for the fact that &lt;span class="math"&gt;\(m\)&lt;/span&gt;
is not fixed ahead of time. (3) Even in our simple setting, MC never reaches
&lt;em&gt;zero&lt;/em&gt; error; it only converges in an &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-&lt;span class="math"&gt;\(\delta\)&lt;/span&gt; sense.&lt;/p&gt;
&lt;!--
Remarks

 - We saw a similar problem where we kept sampling the same individuals over and
   over again in my
   [sqrt-biased sampling post](http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/).
--&gt;

&lt;p&gt;&lt;strong&gt;Sampling without replacement:&lt;/strong&gt; We can get around the problem of resampling
the same elements multiple times by sampling &lt;span class="math"&gt;\(m\)&lt;/span&gt; distinct elements. This is
called a sampling &lt;em&gt;without replacement&lt;/em&gt; (SWOR) scheme. Note that there is no
unique sampling without replacement scheme; although, there does seem to be a
&lt;em&gt;de facto&lt;/em&gt; method (more on that later). There are lots of ways to do sampling
without replacement, e.g., any point process over the universe will do as long
as we can control the size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An alternative formulation:&lt;/strong&gt; We can also formulate our estimation problem as
seeking a sparse, unbiased approximation to a vector &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. We want
our approximation, &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; to satisfy &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}] =
\boldsymbol{x}\)&lt;/span&gt; and while &lt;span class="math"&gt;\(|| \boldsymbol{s} ||_0 \le m\)&lt;/span&gt;. This will suffice for
estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (above) when &lt;span class="math"&gt;\(\boldsymbol{x}=\boldsymbol{p}\)&lt;/span&gt;, the vector of
probabillties, because &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}^\top\! \boldsymbol{f}] =
\mathbb{E}[\boldsymbol{s}]^\top\! \boldsymbol{f} = \boldsymbol{p}^\top\!
\boldsymbol{f} = \mu\)&lt;/span&gt; where &lt;span class="math"&gt;\(\boldsymbol{f}\)&lt;/span&gt; is a vector of all &lt;span class="math"&gt;\(n\)&lt;/span&gt; values of
the function &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Obviously, we don't need to evaluate &lt;span class="math"&gt;\(f\)&lt;/span&gt; in places where
&lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; is zero so it works for our budgeted estimation task. Of
course, unbiased estimation of all probabillties is not &lt;em&gt;necessary&lt;/em&gt; for unbiased
estimation of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; alone. However, this characterization is a good model for
when we have zero knowledge of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Additionally, this formulation might be of
independent interest, since a sparse, unbiased representation of a vector might
be useful in some applications (e.g., replacing a dense vector with a sparse
vector can lead to more efficient computations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priority sampling&lt;/strong&gt;: Priority sampling (Duffield et al., 2005;
&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., 2007&lt;/a&gt;)
is a remarkably simple algorithm, which is essentially optimal for our task, if
we assume no prior knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Here is pseudocode for priority sampling
(PS), based on the &lt;em&gt;alternative formulation&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;\textbf{procedure } \textrm{PrioritySample} \\
&amp;amp;\textbf{inputs: } \text{vector } \boldsymbol{x} \in \mathbb{R}^n, \text{budget } m \in \{1, \ldots, n\}\\
&amp;amp;\textbf{output: } \text{sparse and unbiased representation of $\boldsymbol{x}$} \\
&amp;amp;\quad u_i, \ldots, u_n \overset{\tiny\text{i.i.d.}} \sim \textrm{Uniform}(0,1] \\
&amp;amp;\quad  k_i \leftarrow u_i/x_i \text{ for each $i$} \quad\color{grey}{\text{# random sort key }} \\
&amp;amp;\quad S \leftarrow \{ \text{$m$-smallest elements according to $k_i$} \} \\
&amp;amp;\quad \tau \leftarrow (m+1)^{\text{th}}\text{ smallest }k_i \\
&amp;amp;\quad  s_i \gets \begin{cases}
  \max\left( x_i, 1/\tau \right)  &amp;amp; \text{ if } i \in S \\
  0                               &amp;amp; \text{ otherwise}
\end{cases} \\
&amp;amp;\quad \textbf{return }\boldsymbol{s}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textrm{PrioritySample}\)&lt;/span&gt; can be applied to obtain a sparse and unbiased
representation of any vector in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;. We make use of such a
representation for our original problem of budgeted mean estimation (&lt;span class="math"&gt;\(\mu\)&lt;/span&gt;) as
follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \boldsymbol{s} \gets \textrm{PrioritySample}(\boldsymbol{p}, m) \\
&amp;amp; \widehat{\mu}_{\text{PS}} = \sum_{i \in S} s_i \!\cdot\! f(i)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Explanation: The definition of &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; might look a little mysterious. In the &lt;span class="math"&gt;\((i
\in S)\)&lt;/span&gt; case, it comes from &lt;span class="math"&gt;\(s_i = \frac{p_i}{p(i \in S | \tau)} =
\frac{p_i}{\min(1, x_i \cdot \tau)} = \max(x_i,\ 1/\tau)\)&lt;/span&gt;. The factor &lt;span class="math"&gt;\(p(i \in S
| \tau)\)&lt;/span&gt; is an importance-weighting correction that comes from the
&lt;a href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator"&gt;Horvitz-Thompson estimator&lt;/a&gt;
(modified slightly from its usual presentation to estimate means),
&lt;span class="math"&gt;\(\sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S]\)&lt;/span&gt;, where
&lt;span class="math"&gt;\(S\)&lt;/span&gt; is sampled according to some process with inclusion probabilities &lt;span class="math"&gt;\(q_i = p(i
\in S)\)&lt;/span&gt;. In the case of priority sampling, we have an auxiliary variable for
&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; that makes computing &lt;span class="math"&gt;\(q_i\)&lt;/span&gt; easy. Thus, for priority sampling, we can use
&lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;. This auxillary variable adds a tiny bit extra noise
in our estimator, which is tantamount to one extra sample.&lt;/p&gt;
&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#ps-unbiased');"&gt;Show proof of
unbiasedness&lt;/button&gt; &lt;div id="ps-unbiased" class="derivation"
style="display:none;"&gt; &lt;strong&gt;Proof of unbiasedness&lt;/strong&gt;. The following proof is a
little different from that in the priority sampling papers. I think it's more
straightforward. More importantly, it shows how we can extend the method to
sample from slightly different without-replacement distributions (as long as we
can compute &lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
\mathbb{E}\left[ \widehat{\mu}_{\text{PS}} \right]
&amp;amp;=&amp;amp; \mathbb{E}_{\tau, u_1, \ldots u_n}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \mathbb{E}_{u_i | \tau}\!\left[ \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S] \right] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \mathbb{E}_{u_i | \tau}\!\Big[ \boldsymbol{1}[ i \in S] \Big] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot q_i \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n p_i \cdot f(i) \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \mu \right] \\
&amp;amp;=&amp;amp; \mu
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling satisfies our task criteria: it is both unbiased and sparse
   (i.e., under the evaluation budget).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling can be straighforwardly generalized to support streaming
   &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, since the keys and threshold can be computed as we run, which means it
   can be stopped at any time, in principle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling was designed for estimating subset sums, i.e., estimating
   &lt;span class="math"&gt;\(\sum_{i \in I} x_i\)&lt;/span&gt; for some &lt;span class="math"&gt;\(I \subseteq \{1,\ldots,n\}\)&lt;/span&gt;. In this setting,
   the set of sampled items &lt;span class="math"&gt;\(S\)&lt;/span&gt; is chosen to be "representative" of the
   population, albeit much smaller. In the subset sum setting, priority sampling
   has been shown to have near-optimal variance
   &lt;a href="https://www.cs.rutgers.edu/~szegedy/PUBLICATIONS/full1.pdf"&gt;(Szegedy, 2005)&lt;/a&gt;.
   Specifically, priority sampling with &lt;span class="math"&gt;\(m\)&lt;/span&gt; samples is no worse than the best
   possible &lt;span class="math"&gt;\((m-1)\)&lt;/span&gt;-sparse estimator in terms of variance. Of course, when
   estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; some knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we can obviously be used to beat
   PS. &lt;!-- We can relate subset sums to estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; by interpreting
   &lt;span class="math"&gt;\(\boldsymbol{x} = \alpha\!\cdot\! \boldsymbol{p}\)&lt;/span&gt; for some &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, scaling
   &lt;span class="math"&gt;\(f\)&lt;/span&gt; appropriately by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, and encoding the subset via indicators in
   &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s dimensions. --&gt;
   &lt;!-- (e.g.,. via
   &lt;a href="http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/"&gt;importance sampling&lt;/a&gt;
   or by modifying PS to sample proportional to &lt;span class="math"&gt;\(x_i = p_i \!\cdot\! |f_i|\)&lt;/span&gt; (as
   well as other straightforward modifications), but presumably with a surrogate
   for &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; because we don't want to evaluate it). --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Components of the estimate &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; are uncorrelated, i.e.,
   &lt;span class="math"&gt;\(\textrm{Cov}[s_i, s_j] = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(i \ne j\)&lt;/span&gt; and &lt;span class="math"&gt;\(m \ge 2\)&lt;/span&gt;. This is surprising
   since &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; are related via the threshold &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we instead sample &lt;span class="math"&gt;\(u_1, \ldots, u_n \overset{\text{i.i.d.}}{\sim}
   -\textrm{Exponential}(1)\)&lt;/span&gt;, then &lt;span class="math"&gt;\(S\)&lt;/span&gt; will be sampled according to the &lt;em&gt;de facto&lt;/em&gt;
   sampling without replacement scheme (e.g., &lt;code&gt;numpy.random.sample(..., replace=False)&lt;/code&gt;),
   known as probability proportional to size without replacement (PPSWOR).
   To we can then adjust our estimator
   &lt;div class="math"&gt;$$
   \widehat{\mu}_{\text{PPSWOR}} = \sum_{i \in S} \frac{p_i}{q_i} f(i)
   $$&lt;/div&gt;
   where &lt;span class="math"&gt;\(q_i = p(i \in S|\tau) = p(k_i &amp;gt; \tau) = 1-\exp(-x_i \!\cdot\!
   \tau)\)&lt;/span&gt;. This estimator performs about as well as priority sampling. It
   inherits my proof of unbiasedness (above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; is an auxiliary variable that is introduced to break complex
   dependencies between keys. Computing &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;'s distribution is complicated
   because it is an order statistic of non-identically distributed random
   variates; this means we can't rely on symmetry to make summing over
   permutations efficient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
 - The one downside of this method is that sampling seems to require looking at
   all $n$ items.
--&gt;

&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;You can get the Jupyter notebook for replicating this experiment
&lt;a href="https://github.com/timvieira/blog/blob/master/content/notebook/Priority%20Sampling.ipynb"&gt;here&lt;/a&gt;.
So download the notebook and play with it!&lt;/p&gt;
&lt;p&gt;The improvement of priority sampling (PS) over Monte Carlo (MC) is pretty
nice. I've also included PPSWOR, which seems pretty indistinguishable from PS so
I won't really bother to discuss it. Check out the results!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Priority sampling vs. Monte Carlo" src="http://timvieira.github.io/blog/images/ps-mc.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The shaded region indicates the 10% and 90% percentiles over 20,000
replications, which gives a sense of the variability of each estimator. The
x-axis is the sampling budget, &lt;span class="math"&gt;\(m \le n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The plot shows a small example with &lt;span class="math"&gt;\(n=50\)&lt;/span&gt;. We see that PS's variability
actually goes to zero, unlike Monte Carlo, which is still pretty inaccurate even
at &lt;span class="math"&gt;\(m=n\)&lt;/span&gt;. (Note that MC's x-axis measures raw evaluations, not distinct ones.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt; If you liked this post, you might like my other posts
tagged with &lt;a href="http://timvieira.github.io/blog/tag/sampling.html"&gt;sampling&lt;/a&gt; and
&lt;a href="http://timvieira.github.io/blog/tag/reservoir-sampling.html"&gt;reservoir sampling&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Edith Cohen, "The Magic of Random Sampling"
   (&lt;a href="http://www.cohenwang.com/edith/Talks/MagicSampling201611.pdf"&gt;slides&lt;/a&gt;,
   &lt;a href="https://www.youtube.com/watch?v=jp83HyDs8fs"&gt;talk&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., (2007)&lt;/a&gt;
   has plenty good stuff that I didn't cover.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alex Smola's &lt;a href="http://blog.smola.org/post/1078486350/priority-sampling"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suresh Venkatasubramanian's
   &lt;a href="http://blog.geomblog.org/2005/10/priority-sampling.html"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;: In this post, I'm going to describe some efficient approaches
to estimating the mean of a random variable that takes on only finitely many
values. Despite the ubiquity of Monte Carlo estimation, it is really inefficient
for finite domains. I'll describe some lesser-known algorithms based on sampling
without replacement that can be adapted to estimating means.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we want to estimate an expectation of a derministic function
&lt;span class="math"&gt;\(f\)&lt;/span&gt; over a (large) finite universe of &lt;span class="math"&gt;\(n\)&lt;/span&gt; elements where each element &lt;span class="math"&gt;\(i\)&lt;/span&gt; has
probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu \overset{\tiny{\text{def}}}{=} \sum_{i=1}^n p_i f(i)
$$&lt;/div&gt;
&lt;p&gt;However, &lt;span class="math"&gt;\(f\)&lt;/span&gt; is too expensive to evaluate &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. So let's say that we have
&lt;span class="math"&gt;\(m \le n\)&lt;/span&gt; evaluations to form our estimate. (Obviously, if we're happy
evaluating &lt;span class="math"&gt;\(f\)&lt;/span&gt; a total of &lt;span class="math"&gt;\(n\)&lt;/span&gt; times, then we should just compute &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; exactly
with the definition above.)&lt;/p&gt;
&lt;!--
**Why I'm writing this post**: Monte Carlo is often used in designing algorithms
as a means to cheaply approximate intermediate expectations, think of stochastic
gradient descent as a prime example. However, in many cases, we have a *finite*
universe, i.e., we *could* enumerate all elements, but it's just inefficient to
do so. In other words, sampling is merely a choice made by the algorithm
designer, not a fundamental property of the environment, as it is typically in
statistics. What can we do to improve estimation in this special setting? I
won't get into bigger questions of how to design these algorithms, instead I'll
focus on this specific type of estimation problem.
--&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo:&lt;/strong&gt; The most well-known approach to this type of problem is Monte
Carlo (MC) estimation: sample &lt;span class="math"&gt;\(x^{(1)}, \ldots, x^{(m)}
\overset{\tiny\text{i.i.d.}}{\sim} p\)&lt;/span&gt;, return &lt;span class="math"&gt;\(\widehat{\mu}_{\text{MC}} =
\frac{1}{m} \sum_{i = 1}^m f(x^{(i)})\)&lt;/span&gt;. &lt;em&gt;Remarks&lt;/em&gt;: (1) Monte Carlo can be very
inefficient because it resamples high-probability items over and over again. (2)
We can improve efficiency&amp;mdash;measured in &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluations&amp;mdash;somewhat by
caching past evaluations of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. However, this introduces a serious &lt;em&gt;runtime&lt;/em&gt;
inefficiency and requires modifying the method to account for the fact that &lt;span class="math"&gt;\(m\)&lt;/span&gt;
is not fixed ahead of time. (3) Even in our simple setting, MC never reaches
&lt;em&gt;zero&lt;/em&gt; error; it only converges in an &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-&lt;span class="math"&gt;\(\delta\)&lt;/span&gt; sense.&lt;/p&gt;
&lt;!--
Remarks

 - We saw a similar problem where we kept sampling the same individuals over and
   over again in my
   [sqrt-biased sampling post](http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/).
--&gt;

&lt;p&gt;&lt;strong&gt;Sampling without replacement:&lt;/strong&gt; We can get around the problem of resampling
the same elements multiple times by sampling &lt;span class="math"&gt;\(m\)&lt;/span&gt; distinct elements. This is
called a sampling &lt;em&gt;without replacement&lt;/em&gt; (SWOR) scheme. Note that there is no
unique sampling without replacement scheme; although, there does seem to be a
&lt;em&gt;de facto&lt;/em&gt; method (more on that later). There are lots of ways to do sampling
without replacement, e.g., any point process over the universe will do as long
as we can control the size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An alternative formulation:&lt;/strong&gt; We can also formulate our estimation problem as
seeking a sparse, unbiased approximation to a vector &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. We want
our approximation, &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; to satisfy &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}] =
\boldsymbol{x}\)&lt;/span&gt; and while &lt;span class="math"&gt;\(|| \boldsymbol{s} ||_0 \le m\)&lt;/span&gt;. This will suffice for
estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (above) when &lt;span class="math"&gt;\(\boldsymbol{x}=\boldsymbol{p}\)&lt;/span&gt;, the vector of
probabillties, because &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}^\top\! \boldsymbol{f}] =
\mathbb{E}[\boldsymbol{s}]^\top\! \boldsymbol{f} = \boldsymbol{p}^\top\!
\boldsymbol{f} = \mu\)&lt;/span&gt; where &lt;span class="math"&gt;\(\boldsymbol{f}\)&lt;/span&gt; is a vector of all &lt;span class="math"&gt;\(n\)&lt;/span&gt; values of
the function &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Obviously, we don't need to evaluate &lt;span class="math"&gt;\(f\)&lt;/span&gt; in places where
&lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; is zero so it works for our budgeted estimation task. Of
course, unbiased estimation of all probabillties is not &lt;em&gt;necessary&lt;/em&gt; for unbiased
estimation of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; alone. However, this characterization is a good model for
when we have zero knowledge of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Additionally, this formulation might be of
independent interest, since a sparse, unbiased representation of a vector might
be useful in some applications (e.g., replacing a dense vector with a sparse
vector can lead to more efficient computations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priority sampling&lt;/strong&gt;: Priority sampling (Duffield et al., 2005;
&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., 2007&lt;/a&gt;)
is a remarkably simple algorithm, which is essentially optimal for our task, if
we assume no prior knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Here is pseudocode for priority sampling
(PS), based on the &lt;em&gt;alternative formulation&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;\textbf{procedure } \textrm{PrioritySample} \\
&amp;amp;\textbf{inputs: } \text{vector } \boldsymbol{x} \in \mathbb{R}^n, \text{budget } m \in \{1, \ldots, n\}\\
&amp;amp;\textbf{output: } \text{sparse and unbiased representation of $\boldsymbol{x}$} \\
&amp;amp;\quad u_i, \ldots, u_n \overset{\tiny\text{i.i.d.}} \sim \textrm{Uniform}(0,1] \\
&amp;amp;\quad  k_i \leftarrow u_i/x_i \text{ for each $i$} \quad\color{grey}{\text{# random sort key }} \\
&amp;amp;\quad S \leftarrow \{ \text{$m$-smallest elements according to $k_i$} \} \\
&amp;amp;\quad \tau \leftarrow (m+1)^{\text{th}}\text{ smallest }k_i \\
&amp;amp;\quad  s_i \gets \begin{cases}
  \max\left( x_i, 1/\tau \right)  &amp;amp; \text{ if } i \in S \\
  0                               &amp;amp; \text{ otherwise}
\end{cases} \\
&amp;amp;\quad \textbf{return }\boldsymbol{s}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textrm{PrioritySample}\)&lt;/span&gt; can be applied to obtain a sparse and unbiased
representation of any vector in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;. We make use of such a
representation for our original problem of budgeted mean estimation (&lt;span class="math"&gt;\(\mu\)&lt;/span&gt;) as
follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \boldsymbol{s} \gets \textrm{PrioritySample}(\boldsymbol{p}, m) \\
&amp;amp; \widehat{\mu}_{\text{PS}} = \sum_{i \in S} s_i \!\cdot\! f(i)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Explanation: The definition of &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; might look a little mysterious. In the &lt;span class="math"&gt;\((i
\in S)\)&lt;/span&gt; case, it comes from &lt;span class="math"&gt;\(s_i = \frac{p_i}{p(i \in S | \tau)} =
\frac{p_i}{\min(1, x_i \cdot \tau)} = \max(x_i,\ 1/\tau)\)&lt;/span&gt;. The factor &lt;span class="math"&gt;\(p(i \in S
| \tau)\)&lt;/span&gt; is an importance-weighting correction that comes from the
&lt;a href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator"&gt;Horvitz-Thompson estimator&lt;/a&gt;
(modified slightly from its usual presentation to estimate means),
&lt;span class="math"&gt;\(\sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S]\)&lt;/span&gt;, where
&lt;span class="math"&gt;\(S\)&lt;/span&gt; is sampled according to some process with inclusion probabilities &lt;span class="math"&gt;\(q_i = p(i
\in S)\)&lt;/span&gt;. In the case of priority sampling, we have an auxiliary variable for
&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; that makes computing &lt;span class="math"&gt;\(q_i\)&lt;/span&gt; easy. Thus, for priority sampling, we can use
&lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;. This auxillary variable adds a tiny bit extra noise
in our estimator, which is tantamount to one extra sample.&lt;/p&gt;
&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#ps-unbiased');"&gt;Show proof of
unbiasedness&lt;/button&gt; &lt;div id="ps-unbiased" class="derivation"
style="display:none;"&gt; &lt;strong&gt;Proof of unbiasedness&lt;/strong&gt;. The following proof is a
little different from that in the priority sampling papers. I think it's more
straightforward. More importantly, it shows how we can extend the method to
sample from slightly different without-replacement distributions (as long as we
can compute &lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
\mathbb{E}\left[ \widehat{\mu}_{\text{PS}} \right]
&amp;amp;=&amp;amp; \mathbb{E}_{\tau, u_1, \ldots u_n}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \mathbb{E}_{u_i | \tau}\!\left[ \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S] \right] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \mathbb{E}_{u_i | \tau}\!\Big[ \boldsymbol{1}[ i \in S] \Big] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot q_i \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n p_i \cdot f(i) \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \mu \right] \\
&amp;amp;=&amp;amp; \mu
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling satisfies our task criteria: it is both unbiased and sparse
   (i.e., under the evaluation budget).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling can be straighforwardly generalized to support streaming
   &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, since the keys and threshold can be computed as we run, which means it
   can be stopped at any time, in principle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling was designed for estimating subset sums, i.e., estimating
   &lt;span class="math"&gt;\(\sum_{i \in I} x_i\)&lt;/span&gt; for some &lt;span class="math"&gt;\(I \subseteq \{1,\ldots,n\}\)&lt;/span&gt;. In this setting,
   the set of sampled items &lt;span class="math"&gt;\(S\)&lt;/span&gt; is chosen to be "representative" of the
   population, albeit much smaller. In the subset sum setting, priority sampling
   has been shown to have near-optimal variance
   &lt;a href="https://www.cs.rutgers.edu/~szegedy/PUBLICATIONS/full1.pdf"&gt;(Szegedy, 2005)&lt;/a&gt;.
   Specifically, priority sampling with &lt;span class="math"&gt;\(m\)&lt;/span&gt; samples is no worse than the best
   possible &lt;span class="math"&gt;\((m-1)\)&lt;/span&gt;-sparse estimator in terms of variance. Of course, when
   estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; some knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we can obviously be used to beat
   PS. &lt;!-- We can relate subset sums to estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; by interpreting
   &lt;span class="math"&gt;\(\boldsymbol{x} = \alpha\!\cdot\! \boldsymbol{p}\)&lt;/span&gt; for some &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, scaling
   &lt;span class="math"&gt;\(f\)&lt;/span&gt; appropriately by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, and encoding the subset via indicators in
   &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s dimensions. --&gt;
   &lt;!-- (e.g.,. via
   &lt;a href="http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/"&gt;importance sampling&lt;/a&gt;
   or by modifying PS to sample proportional to &lt;span class="math"&gt;\(x_i = p_i \!\cdot\! |f_i|\)&lt;/span&gt; (as
   well as other straightforward modifications), but presumably with a surrogate
   for &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; because we don't want to evaluate it). --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Components of the estimate &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; are uncorrelated, i.e.,
   &lt;span class="math"&gt;\(\textrm{Cov}[s_i, s_j] = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(i \ne j\)&lt;/span&gt; and &lt;span class="math"&gt;\(m \ge 2\)&lt;/span&gt;. This is surprising
   since &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; are related via the threshold &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we instead sample &lt;span class="math"&gt;\(u_1, \ldots, u_n \overset{\text{i.i.d.}}{\sim}
   -\textrm{Exponential}(1)\)&lt;/span&gt;, then &lt;span class="math"&gt;\(S\)&lt;/span&gt; will be sampled according to the &lt;em&gt;de facto&lt;/em&gt;
   sampling without replacement scheme (e.g., &lt;code&gt;numpy.random.sample(..., replace=False)&lt;/code&gt;),
   known as probability proportional to size without replacement (PPSWOR).
   To we can then adjust our estimator
   &lt;div class="math"&gt;$$
   \widehat{\mu}_{\text{PPSWOR}} = \sum_{i \in S} \frac{p_i}{q_i} f(i)
   $$&lt;/div&gt;
   where &lt;span class="math"&gt;\(q_i = p(i \in S|\tau) = p(k_i &amp;gt; \tau) = 1-\exp(-x_i \!\cdot\!
   \tau)\)&lt;/span&gt;. This estimator performs about as well as priority sampling. It
   inherits my proof of unbiasedness (above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; is an auxiliary variable that is introduced to break complex
   dependencies between keys. Computing &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;'s distribution is complicated
   because it is an order statistic of non-identically distributed random
   variates; this means we can't rely on symmetry to make summing over
   permutations efficient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
 - The one downside of this method is that sampling seems to require looking at
   all $n$ items.
--&gt;

&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;You can get the Jupyter notebook for replicating this experiment
&lt;a href="https://github.com/timvieira/blog/blob/master/content/notebook/Priority%20Sampling.ipynb"&gt;here&lt;/a&gt;.
So download the notebook and play with it!&lt;/p&gt;
&lt;p&gt;The improvement of priority sampling (PS) over Monte Carlo (MC) is pretty
nice. I've also included PPSWOR, which seems pretty indistinguishable from PS so
I won't really bother to discuss it. Check out the results!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Priority sampling vs. Monte Carlo" src="http://timvieira.github.io/blog/images/ps-mc.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The shaded region indicates the 10% and 90% percentiles over 20,000
replications, which gives a sense of the variability of each estimator. The
x-axis is the sampling budget, &lt;span class="math"&gt;\(m \le n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The plot shows a small example with &lt;span class="math"&gt;\(n=50\)&lt;/span&gt;. We see that PS's variability
actually goes to zero, unlike Monte Carlo, which is still pretty inaccurate even
at &lt;span class="math"&gt;\(m=n\)&lt;/span&gt;. (Note that MC's x-axis measures raw evaluations, not distinct ones.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt; If you liked this post, you might like my other posts
tagged with &lt;a href="http://timvieira.github.io/blog/tag/sampling.html"&gt;sampling&lt;/a&gt; and
&lt;a href="http://timvieira.github.io/blog/tag/reservoir-sampling.html"&gt;reservoir sampling&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Edith Cohen, "The Magic of Random Sampling"
   (&lt;a href="http://www.cohenwang.com/edith/Talks/MagicSampling201611.pdf"&gt;slides&lt;/a&gt;,
   &lt;a href="https://www.youtube.com/watch?v=jp83HyDs8fs"&gt;talk&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., (2007)&lt;/a&gt;
   has plenty good stuff that I didn't cover.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alex Smola's &lt;a href="http://blog.smola.org/post/1078486350/priority-sampling"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suresh Venkatasubramanian's
   &lt;a href="http://blog.geomblog.org/2005/10/priority-sampling.html"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="sampling"></category><category term="statistics"></category><category term="reservoir-sampling"></category></entry><entry><title>How to test gradient implementations</title><link href="http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/" rel="alternate"></link><published>2017-04-21T00:00:00-04:00</published><updated>2017-04-21T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2017-04-21:/blog/post/2017/04/21/how-to-test-gradient-implementations/</id><summary type="html">&lt;!--
**Who should read this?** Nowadays, you're probably just using automatic
differentiation to compute the gradient of whatever function you're using. If
that's you and you trust your software package wholeheartedly, then you probably
don't need to read this. If you're rolling your own module/Op to put in to an
auto-diff library, then you should read this. I find that none of the available
libraries are any good for differentiating dynamic programs, so I still find
this stuff useful. You'll probably have to write gradient code without the aid
of an autodiff library someday... So either way, knowing this stuff is good for
you. To answer the question, everyone.
--&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we have a function, &lt;span class="math"&gt;\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)&lt;/span&gt;,
and we want to test code that computes &lt;span class="math"&gt;\(\nabla f\)&lt;/span&gt;. (Note that these techniques
also apply when &lt;span class="math"&gt;\(f\)&lt;/span&gt; has multivariate output.)&lt;/p&gt;
&lt;h2&gt;Finite-difference approximation&lt;/h2&gt;
&lt;p&gt;The main way that people test gradient computation is by comparing it against a
finite-difference (FD) approximation to the gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{d}^\top\! \nabla f(\boldsymbol{x}) \approx \frac{1}{2 \varepsilon}(f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d}) - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d}))
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;
where &lt;span class="math"&gt;\(\boldsymbol{d} \in \mathbb{R}^n\)&lt;/span&gt; is an arbitrary "direction" in parameter
space. We will look at many directions when we test. Generally, people take the
&lt;span class="math"&gt;\(n\)&lt;/span&gt; elementary vectors as the directions, but random directions are just as good
(and you can catch bugs in all dimensions with less than &lt;span class="math"&gt;\(n\)&lt;/span&gt; of them).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Always use the two-sided difference formula&lt;/strong&gt;. There is a version which
doesn't add &lt;em&gt;and&lt;/em&gt; subtract, just does one or the other. Do not use it ever.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Make sure you test multiple inputs&lt;/strong&gt; (values of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;) or any thing
else the function depends on (e.g., the minibatch).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What directions to use&lt;/strong&gt;: When debugging, I tend to use elementary directions
because they tell me something about which dimensions that are wrong... this
doesn't always help though. The random directions are best when you want the
test cases to run really quickly. In that case, you can switch to check a few
random directions using a
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/util.py"&gt;spherical&lt;/a&gt;
distribution&amp;mdash;do &lt;em&gt;not&lt;/em&gt; sample them from a multivariate uniform!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Always test your implementation of &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/strong&gt; It's very easy to &lt;em&gt;correctly&lt;/em&gt;
  compute the gradient of the &lt;em&gt;wrong&lt;/em&gt; function. The FD approximation is a
  "self-consistency" test, it does not validate &lt;span class="math"&gt;\(f\)&lt;/span&gt; only the relationship
  between &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(\nabla\! f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Obviously, how you test &lt;span class="math"&gt;\(f\)&lt;/span&gt; depends strongly on what it's supposed to compute.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: For a conditional random field (CRF), you can also test that your
   implementation of a dynamic program for computing &lt;span class="math"&gt;\(\log Z_\theta(x)\)&lt;/span&gt; is
   correctly by comparing against brute-force enumeration of &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; on
   small examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, you can directly test the gradient code if you know a different way
to compute it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: In a CRF, we know that the &lt;span class="math"&gt;\(\nabla \log Z_\theta(x)\)&lt;/span&gt; is a feature
   expectation, which you can also test against a brute-force enumeration on
   small examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Why not just use the FD approximation as your gradient?&lt;/h3&gt;
&lt;p&gt;For low-dimensional functions, you can straight-up use the finite-difference
approximation instead of rolling code to compute the gradient. (Take &lt;span class="math"&gt;\(n\)&lt;/span&gt;
axis-aligned unit vectors for &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;.) The FD approximation is very
accurate. Of course, specialized code is probably a little more accurate, but
that's not &lt;em&gt;really&lt;/em&gt; why we bother to do it! The reason why we write specialized
gradient code is &lt;em&gt;not&lt;/em&gt; to improve numerical accuracy, it's to improve
&lt;em&gt;efficiency&lt;/em&gt;. As I've
&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;ranted&lt;/a&gt;
before, automatic differentiation techniques guarantee that evaluating &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; gradient should be as efficient as computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; (with the caveat that
&lt;em&gt;space&lt;/em&gt; complexity may increase substantially - i.e., space-time tradeoffs
exists). FD is &lt;span class="math"&gt;\(\mathcal{O}(n \cdot \textrm{runtime } f(x))\)&lt;/span&gt;, where as autodiff
is &lt;span class="math"&gt;\(\mathcal{O}(\textrm{runtime } f(x))\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;How to compare vectors&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Absolute difference is the devil.&lt;/strong&gt; You should never compare vectors in
absolute difference (this is Lecture 1 of any numerical methods course). In this
case, the problem is that gradients depend strongly on the scale of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. If &lt;span class="math"&gt;\(f\)&lt;/span&gt;
takes tiny values then it's easy for differences to be lower than a tiny
threshold.&lt;/p&gt;
&lt;p&gt;Most people use &lt;strong&gt;relative error&lt;/strong&gt; &lt;span class="math"&gt;\(= \frac{|\textbf{want} -
\textbf{got}|}{|\textbf{want}|}\)&lt;/span&gt;, to get a scale-free error measure, but
unfortunately relative error chokes when &lt;span class="math"&gt;\(\textbf{want}\)&lt;/span&gt; is zero.&lt;/p&gt;
&lt;p&gt;I compute several error measures with a script that you can import from my
github
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/checkgrad.py"&gt;arsenal.math.checkgrad.{fdcheck}&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I use two metrics to test gradients:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Relative error (skipping zeros): If relative error hits a zero, I skip
   it. I'll rely on the other measure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pearson correlation: Checks the &lt;em&gt;direction&lt;/em&gt; of the gradient, but allows a
   scale and shift transformation. This measure doesn't have trouble with zeros,
   but allows scale and shift problems to pass by. &lt;em&gt;Make sure you fix those
   errors!&lt;/em&gt; (e.g. In the CRF example, you might have forgotten to divide by
   &lt;span class="math"&gt;\(Z(x)\)&lt;/span&gt;, which not really a constant... I've made this exact mistake a few
   times.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I also look at some diagnostics, which help me debug stuff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Accuracy of predicting the sign {+,-,0} of each dimension (or dot random product).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Absolute error (just as a diagnostic)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scatter plot: When debugging, I like to scatter plot the elements of FD vs. my
  implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these measurements (and the scatter plot) can be computed with
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/compare.py"&gt;arsenal.math.compare.{compare}&lt;/a&gt;,
which I find super useful when debugging absolutely anything numerical.&lt;/p&gt;
&lt;h2&gt;Bonus tests&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Testing modules&lt;/strong&gt;: You can test the different modules of your code as well
(assuming you have a composable module-based setup). E.g., I test my DP
algorithm independent of how the features and downstream loss are computed. You
can also test feature and downstream loss modules independent of one
another. Note that autodiff (implicitly) computes Jacobian-vector products
because modules are multivariate in general. We can reduce to the scalar case by
taking a dot product of the outputs with a (fixed) random vector.&lt;/p&gt;
&lt;p&gt;Something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spherical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# fixed random vector |output|=|m|&lt;/span&gt;
&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# scalar function for use in fd&lt;/span&gt;

&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# propagate&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adjoint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="c1"&gt;# set output adjoint to r, usually we set adjoint of scalar output=1&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bprop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adjoint&lt;/span&gt; &lt;span class="c1"&gt;# grab the gradient&lt;/span&gt;
&lt;span class="n"&gt;fd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fdgrad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ad&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Integration tests&lt;/strong&gt;: Test that running a gradient-based optimization algorithm
is successful with your gradient implementation. Use smaller versions of your
problem if possible. A related test for machine learning applications is to make
sure that your model and learning procedure can (over)fit small datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test that batch = minibatch&lt;/strong&gt; (if applicable). It's very easy to get this bit
wrong. Broadcasting rules (in numpy, for example) make it easy to hide matrix
conformability mishaps. So make sure you get the same results as manual
minibatching (Of course, you should only do minibatching if are get a speed-up
from vectorization or parallelism. You should probably test that it's actually
faster.)&lt;/p&gt;
&lt;!--
Other common sources of bugs

* Really look over your test cases. I often find that my errors are actually in
  the test case themselves because either (1) I wrote it really quickly with
  less care than the difficult function/gradient, or (2) there is a gap between
  "what I want it to do" and "what I told it to do".

* Random search in the space of programs can result in overfitting! This is a
  general problem with test-driven development that always applies. If you are
  hamfistedly twiddling bits of your code without thinking about why things
  work, you can trick almost any test.
--&gt;

&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I've written about gradient approximations before, you might like these
  articles:
  &lt;a href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/"&gt;Gradient-vector products&lt;/a&gt;,
  &lt;a href="http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/"&gt;Complex-step method&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The foundations of backprop:
  &lt;a href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/"&gt;Backprop is not just the chain rule&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;I strongly recommend&lt;/a&gt;
  learning how automatic differentiation works, I learned it from
  &lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's course notes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2017/04/22/you-deserve-better-than-two-sided-finite-differences/"&gt;Justin Domke post&lt;/a&gt;:
  Explains why we need bespoke finite-difference stencils (i.e., more than
  two-sided differences) to prevent numerical demons from destroying our results!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;!--
**Who should read this?** Nowadays, you're probably just using automatic
differentiation to compute the gradient of whatever function you're using. If
that's you and you trust your software package wholeheartedly, then you probably
don't need to read this. If you're rolling your own module/Op to put in to an
auto-diff library, then you should read this. I find that none of the available
libraries are any good for differentiating dynamic programs, so I still find
this stuff useful. You'll probably have to write gradient code without the aid
of an autodiff library someday... So either way, knowing this stuff is good for
you. To answer the question, everyone.
--&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we have a function, &lt;span class="math"&gt;\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)&lt;/span&gt;,
and we want to test code that computes &lt;span class="math"&gt;\(\nabla f\)&lt;/span&gt;. (Note that these techniques
also apply when &lt;span class="math"&gt;\(f\)&lt;/span&gt; has multivariate output.)&lt;/p&gt;
&lt;h2&gt;Finite-difference approximation&lt;/h2&gt;
&lt;p&gt;The main way that people test gradient computation is by comparing it against a
finite-difference (FD) approximation to the gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{d}^\top\! \nabla f(\boldsymbol{x}) \approx \frac{1}{2 \varepsilon}(f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d}) - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d}))
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;
where &lt;span class="math"&gt;\(\boldsymbol{d} \in \mathbb{R}^n\)&lt;/span&gt; is an arbitrary "direction" in parameter
space. We will look at many directions when we test. Generally, people take the
&lt;span class="math"&gt;\(n\)&lt;/span&gt; elementary vectors as the directions, but random directions are just as good
(and you can catch bugs in all dimensions with less than &lt;span class="math"&gt;\(n\)&lt;/span&gt; of them).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Always use the two-sided difference formula&lt;/strong&gt;. There is a version which
doesn't add &lt;em&gt;and&lt;/em&gt; subtract, just does one or the other. Do not use it ever.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Make sure you test multiple inputs&lt;/strong&gt; (values of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;) or any thing
else the function depends on (e.g., the minibatch).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What directions to use&lt;/strong&gt;: When debugging, I tend to use elementary directions
because they tell me something about which dimensions that are wrong... this
doesn't always help though. The random directions are best when you want the
test cases to run really quickly. In that case, you can switch to check a few
random directions using a
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/util.py"&gt;spherical&lt;/a&gt;
distribution&amp;mdash;do &lt;em&gt;not&lt;/em&gt; sample them from a multivariate uniform!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Always test your implementation of &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/strong&gt; It's very easy to &lt;em&gt;correctly&lt;/em&gt;
  compute the gradient of the &lt;em&gt;wrong&lt;/em&gt; function. The FD approximation is a
  "self-consistency" test, it does not validate &lt;span class="math"&gt;\(f\)&lt;/span&gt; only the relationship
  between &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(\nabla\! f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Obviously, how you test &lt;span class="math"&gt;\(f\)&lt;/span&gt; depends strongly on what it's supposed to compute.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: For a conditional random field (CRF), you can also test that your
   implementation of a dynamic program for computing &lt;span class="math"&gt;\(\log Z_\theta(x)\)&lt;/span&gt; is
   correctly by comparing against brute-force enumeration of &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; on
   small examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, you can directly test the gradient code if you know a different way
to compute it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: In a CRF, we know that the &lt;span class="math"&gt;\(\nabla \log Z_\theta(x)\)&lt;/span&gt; is a feature
   expectation, which you can also test against a brute-force enumeration on
   small examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Why not just use the FD approximation as your gradient?&lt;/h3&gt;
&lt;p&gt;For low-dimensional functions, you can straight-up use the finite-difference
approximation instead of rolling code to compute the gradient. (Take &lt;span class="math"&gt;\(n\)&lt;/span&gt;
axis-aligned unit vectors for &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;.) The FD approximation is very
accurate. Of course, specialized code is probably a little more accurate, but
that's not &lt;em&gt;really&lt;/em&gt; why we bother to do it! The reason why we write specialized
gradient code is &lt;em&gt;not&lt;/em&gt; to improve numerical accuracy, it's to improve
&lt;em&gt;efficiency&lt;/em&gt;. As I've
&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;ranted&lt;/a&gt;
before, automatic differentiation techniques guarantee that evaluating &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; gradient should be as efficient as computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; (with the caveat that
&lt;em&gt;space&lt;/em&gt; complexity may increase substantially - i.e., space-time tradeoffs
exists). FD is &lt;span class="math"&gt;\(\mathcal{O}(n \cdot \textrm{runtime } f(x))\)&lt;/span&gt;, where as autodiff
is &lt;span class="math"&gt;\(\mathcal{O}(\textrm{runtime } f(x))\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;How to compare vectors&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Absolute difference is the devil.&lt;/strong&gt; You should never compare vectors in
absolute difference (this is Lecture 1 of any numerical methods course). In this
case, the problem is that gradients depend strongly on the scale of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. If &lt;span class="math"&gt;\(f\)&lt;/span&gt;
takes tiny values then it's easy for differences to be lower than a tiny
threshold.&lt;/p&gt;
&lt;p&gt;Most people use &lt;strong&gt;relative error&lt;/strong&gt; &lt;span class="math"&gt;\(= \frac{|\textbf{want} -
\textbf{got}|}{|\textbf{want}|}\)&lt;/span&gt;, to get a scale-free error measure, but
unfortunately relative error chokes when &lt;span class="math"&gt;\(\textbf{want}\)&lt;/span&gt; is zero.&lt;/p&gt;
&lt;p&gt;I compute several error measures with a script that you can import from my
github
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/checkgrad.py"&gt;arsenal.math.checkgrad.{fdcheck}&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I use two metrics to test gradients:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Relative error (skipping zeros): If relative error hits a zero, I skip
   it. I'll rely on the other measure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pearson correlation: Checks the &lt;em&gt;direction&lt;/em&gt; of the gradient, but allows a
   scale and shift transformation. This measure doesn't have trouble with zeros,
   but allows scale and shift problems to pass by. &lt;em&gt;Make sure you fix those
   errors!&lt;/em&gt; (e.g. In the CRF example, you might have forgotten to divide by
   &lt;span class="math"&gt;\(Z(x)\)&lt;/span&gt;, which not really a constant... I've made this exact mistake a few
   times.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I also look at some diagnostics, which help me debug stuff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Accuracy of predicting the sign {+,-,0} of each dimension (or dot random product).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Absolute error (just as a diagnostic)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scatter plot: When debugging, I like to scatter plot the elements of FD vs. my
  implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these measurements (and the scatter plot) can be computed with
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/compare.py"&gt;arsenal.math.compare.{compare}&lt;/a&gt;,
which I find super useful when debugging absolutely anything numerical.&lt;/p&gt;
&lt;h2&gt;Bonus tests&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Testing modules&lt;/strong&gt;: You can test the different modules of your code as well
(assuming you have a composable module-based setup). E.g., I test my DP
algorithm independent of how the features and downstream loss are computed. You
can also test feature and downstream loss modules independent of one
another. Note that autodiff (implicitly) computes Jacobian-vector products
because modules are multivariate in general. We can reduce to the scalar case by
taking a dot product of the outputs with a (fixed) random vector.&lt;/p&gt;
&lt;p&gt;Something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spherical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# fixed random vector |output|=|m|&lt;/span&gt;
&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# scalar function for use in fd&lt;/span&gt;

&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# propagate&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adjoint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="c1"&gt;# set output adjoint to r, usually we set adjoint of scalar output=1&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bprop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adjoint&lt;/span&gt; &lt;span class="c1"&gt;# grab the gradient&lt;/span&gt;
&lt;span class="n"&gt;fd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fdgrad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ad&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Integration tests&lt;/strong&gt;: Test that running a gradient-based optimization algorithm
is successful with your gradient implementation. Use smaller versions of your
problem if possible. A related test for machine learning applications is to make
sure that your model and learning procedure can (over)fit small datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test that batch = minibatch&lt;/strong&gt; (if applicable). It's very easy to get this bit
wrong. Broadcasting rules (in numpy, for example) make it easy to hide matrix
conformability mishaps. So make sure you get the same results as manual
minibatching (Of course, you should only do minibatching if are get a speed-up
from vectorization or parallelism. You should probably test that it's actually
faster.)&lt;/p&gt;
&lt;!--
Other common sources of bugs

* Really look over your test cases. I often find that my errors are actually in
  the test case themselves because either (1) I wrote it really quickly with
  less care than the difficult function/gradient, or (2) there is a gap between
  "what I want it to do" and "what I told it to do".

* Random search in the space of programs can result in overfitting! This is a
  general problem with test-driven development that always applies. If you are
  hamfistedly twiddling bits of your code without thinking about why things
  work, you can trick almost any test.
--&gt;

&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I've written about gradient approximations before, you might like these
  articles:
  &lt;a href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/"&gt;Gradient-vector products&lt;/a&gt;,
  &lt;a href="http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/"&gt;Complex-step method&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The foundations of backprop:
  &lt;a href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/"&gt;Backprop is not just the chain rule&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;I strongly recommend&lt;/a&gt;
  learning how automatic differentiation works, I learned it from
  &lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's course notes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2017/04/22/you-deserve-better-than-two-sided-finite-differences/"&gt;Justin Domke post&lt;/a&gt;:
  Explains why we need bespoke finite-difference stencils (i.e., more than
  two-sided differences) to prevent numerical demons from destroying our results!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="testing"></category><category term="calculus"></category></entry><entry><title>Counterfactual reasoning and learning from logged data</title><link href="http://timvieira.github.io/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/" rel="alternate"></link><published>2016-12-19T00:00:00-05:00</published><updated>2016-12-19T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-12-19:/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/</id><summary type="html">&lt;style&gt; .toggle-button { background-color: #555555; border: none; color: white;
padding: 10px 15px; border-radius: 6px; text-align: center; text-decoration:
none; display: inline-block; font-size: 16px; cursor: pointer; } .derivation {
background-color: #f2f2f2; border: thin solid #ddd; padding: 10px;
margin-bottom: 10px; } &lt;/style&gt;

&lt;script&gt;
/* workaround for when markdown/mathjax gets confused by the javascript dollar function. */
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;Counterfactual reasoning is &lt;em&gt;reasoning about data that we did not observe&lt;/em&gt;. For
example, reasoning about the expected reward of new policy given data collected
from a older one.&lt;/p&gt;
&lt;p&gt;In this post, I'll discuss some basic techniques for learning from logged
data. For the large part, this post is based on things I learned from
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Peshkin &amp;amp; Shelton (2002)&lt;/a&gt;
and &lt;a href="https://arxiv.org/abs/1209.2355"&gt;Bottou et al. (2013)&lt;/a&gt; (two of all-time
favorite papers).&lt;/p&gt;
&lt;p&gt;After reading, have a look at the
&lt;a href="https://gist.github.com/timvieira/788c2c25c94663c49abada60f2e107e9"&gt;Jupyter notebook&lt;/a&gt;
accompanying this post!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; (&lt;em&gt;off-line off-policy optimization&lt;/em&gt;): We're trying to optimize a
function of the form,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \underset{p_\theta}{\mathbb{E}} \left[ r(x) \right] = \sum_{x \in \mathcal{X}} p_\theta(x) r(x).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; But! We only have a &lt;em&gt;fixed&lt;/em&gt; sample of size &lt;span class="math"&gt;\(m\)&lt;/span&gt; from a data collection
policy &lt;span class="math"&gt;\(q\)&lt;/span&gt;, &lt;span class="math"&gt;\(\{ (r^{(j)}, x^{(j)} ) \}_{j=1}^m \overset{\text{i.i.d.}} \sim q.\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Although, it's not &lt;em&gt;necessarily&lt;/em&gt; the case, you can think of &lt;span class="math"&gt;\(q = p_{\theta'}\)&lt;/span&gt;
  for a &lt;em&gt;fixed&lt;/em&gt; value &lt;span class="math"&gt;\(\theta'.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; is an arbitrary multivariate space, which permits a mix of
  continuous and discrete components, with appropriate densities, &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt;
  and &lt;span class="math"&gt;\(q\)&lt;/span&gt; defined over it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(r: \mathcal{X} \mapsto \mathbb{R}\)&lt;/span&gt; is a black box that outputs a scalar
  score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I've used the notation &lt;span class="math"&gt;\(r^{(j)}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(r(x^{(j)})\)&lt;/span&gt; to emphasize that we
  can't evaluate &lt;span class="math"&gt;\(r\)&lt;/span&gt; at &lt;span class="math"&gt;\(x\)&lt;/span&gt; values other than those in the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We'll assume that &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns positive probability everywhere, &lt;span class="math"&gt;\(q(x) &amp;gt; 0\)&lt;/span&gt; for
  all &lt;span class="math"&gt;\(x \in \mathcal{X}\)&lt;/span&gt;. This means is that the data collection process must
  be randomized and eventually sample all possible configurations. Later, I
  discuss relaxing this assumption.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each distribution is a product of one or more factors of the following types:
&lt;strong&gt;policy factors&lt;/strong&gt; (at least one), which directly depend on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, and
&lt;strong&gt;environment factors&lt;/strong&gt; (possibly none), which do not depend directly on
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Note that environment factors are &lt;em&gt;only&lt;/em&gt; accessible via sampling
(i.e., we don't know the &lt;em&gt;value&lt;/em&gt; they assign to a sample). For example, a
&lt;em&gt;contextual bandit problem&lt;/em&gt;, where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a context-action pair, &lt;span class="math"&gt;\(x =
(s,a)\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(q(x) = q(a|s) p(s)\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{\theta}(x) = p_{\theta}(a|s)
p(s)\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share the environment factor &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;, the
distribution over contexts, and only differ in the action-given-context
factor. For now, assume that we can evaluate all environment factors; later,
I'll discuss how we cleverly work around it.&lt;/p&gt;
&lt;!--
(We can
even extend it to a full-blown MDP or POMDP by taking $x$ to be a sequence of
state-action pairs, often called "trajectories".)
--&gt;

&lt;p&gt;&lt;strong&gt;The main challenge&lt;/strong&gt; of this setting is that we don't have controlled
experiments to learn from because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is not (completely) in our control. This
manifests itself as high variance ("noise") in estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;. Consider
the contextual bandit setting, we receive a context &lt;span class="math"&gt;\(s\)&lt;/span&gt; and execute single
action; we never get to rollback to that precise context and try an alternative
action (to get a paired sample #yolo) because we do not control &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;. This is
an important paradigm for many 'real world' problems, e.g., predicting medical
treatments or ad selection.&lt;/p&gt;
&lt;!--
This is the crucial difference that makes counterfactual learning
more difficult than (fully) supervised learning.
--&gt;

&lt;p&gt;&lt;strong&gt;Estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;&lt;/strong&gt; [V1]: We obtain an unbiased estimator of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;
with
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta)
\approx \hat{J}_{\!\text{IS}}(\theta)
= \frac{1}{m} \sum_{j=1}^m r^{(j)} \!\cdot\! w^{(j)}_{\theta}
\quad \text{ where } w^{(j)}_{\theta} = \frac{p_{\theta}(x^{(j)}) }{ q(x^{(j)}) }.
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; This estimator is remarkable: it uses importance sampling as a function
approximator! We have an &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt; for any value of
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that we like. &lt;em&gt;The catch&lt;/em&gt; is that we have to pick &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; a priori,
i.e., with no knowledge of the sample.&lt;/p&gt;
&lt;!--
We also require that the usual 'support
conditions' for importance sampling conditions ($p_{\theta}(x)&gt;0 \Rightarrow
q(x)&gt;0$ for all $x \in \mathcal{X}$, which is why we made assumption A1.
--&gt;

&lt;p&gt;After we've collected a (large) sample it's possible to optimize
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; using any optimization algorithm (e.g., L-BFGS). Of
course, we risk overfitting to the sample if we evaluate
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt;. Actually, it's a bit worse: this objective tends to
favor regions of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, which are not well represented in the sample because
the importance sampling estimator has high variance in these regions resulting
from large importance weights (when &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; is small and &lt;span class="math"&gt;\(p_{\theta}(x)\)&lt;/span&gt; is
large, &lt;span class="math"&gt;\(w(x)\)&lt;/span&gt; is large and consequently so is &lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; regardless
of whether &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; is high!). Thus, we want some type of "regularization" to keep
the optimizer in regions which are sufficiently well-represented by the sample.&lt;/p&gt;
&lt;!--
**Visual example**: We can visualize this phenomena in a simple example. Let $q
= \mathcal{N}(0, \sigma=5)$, $r(x) = 1 \text{ if } x \in [2, 3], 0.2 \text{
otherwise},$ and $p_\theta = \mathcal{N}(\theta, \sigma=1)$.  This example is
nice because it let's us plot $x$ and $\theta$ in the same space. This is
generally not the case, because $\mathcal{X}$ may have no connection to $\theta$
space, e.g., $\mathcal{X}$ may be discrete.

**TODO** add plot
--&gt;

&lt;p&gt;&lt;strong&gt;Better surrogate&lt;/strong&gt; [V2]: There are many ways to improve the variance of the
estimator and &lt;em&gt;confidently&lt;/em&gt; obtain improvements to the system. One of my
favorites is Bottou et al.'s lower bound on &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;, which we get by
clipping importance weights, replace &lt;span class="math"&gt;\(w^{(j)}_{\theta}\)&lt;/span&gt; with &lt;span class="math"&gt;\(\min(R,
w^{(j)}_{\theta})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Confidence intervals&lt;/strong&gt; [V3]: We can augment the V2 lower bound with confidence
intervals derived from the empirical Bernstein bound (EBB). We'll require that
&lt;span class="math"&gt;\(r\)&lt;/span&gt; is bounded and that we know its max/min values. The EBB &lt;em&gt;penalizes&lt;/em&gt;
hypotheses (values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;) which have higher sample variance. (Note: a
Hoeffding bound wouldn't change the &lt;em&gt;shape&lt;/em&gt; of the objective, but EBB does
thanks to the sample variance penalty. EBB tends to be tighter.). The EBB
introduces an additional "confidence" hyperparameter, &lt;span class="math"&gt;\((1-\delta)\)&lt;/span&gt;. Bottou et
al. recommend maximizing the lower bound as it provides safer improvements. See
the original paper for the derivation.&lt;/p&gt;
&lt;!--
An important benefit of having upper *and* lower is that the bounds tell
us whether or not we should collect more data
--&gt;

&lt;p&gt;Both V2 and V3 are &lt;em&gt;biased&lt;/em&gt; (as they are lower bounds), but we can mitigate the
bias by &lt;em&gt;tuning&lt;/em&gt; the hyperparameter &lt;span class="math"&gt;\(R\)&lt;/span&gt; on a heldout sample (we can even tune
&lt;span class="math"&gt;\(\delta\)&lt;/span&gt;, if desired). Additionally, V2 and V3 are 'valid' when &lt;span class="math"&gt;\(q\)&lt;/span&gt; has limited
support since they prevent the importance weights from exploding (of course, the
bias can be arbitrarily bad, but probably unavoidable given the
learning-from-only-logged data setup).&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;!--
**Be warned**: This may be considered an idealized setting. Much of the research
in counterfactual and causal reasoning targets (often subtle) deviations from
these assumptions (and some different questions, of course). Some extensions and
discussion appear towards the end of the post.
--&gt;

&lt;p&gt;&lt;strong&gt;Unknown environment factors&lt;/strong&gt;: Consider the contextual bandit setting
(mentioned above). Here &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share an &lt;em&gt;unknown&lt;/em&gt; environment factor: the
distribution of contexts. Luckily, we do not need to know the value of this
factor in order to apply any of our estimators because they are all based on
likelihood &lt;em&gt;ratios&lt;/em&gt;, thus the shared unknown factors cancel out!  Some specific
examples are given below. Of course, these factors do influence the estimators
because they are crucial in &lt;em&gt;sampling&lt;/em&gt;, they just aren't necessary in
&lt;em&gt;evaluation&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In contextual bandit example, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a state-action pair, &lt;span class="math"&gt;\(w_{\theta}(x) =
    \frac{p_{\theta}(x)}{q(x)} = \frac{ p_{\theta}(s,a) }{ q(s,a) } =
    \frac{p_{\theta}(a|s) p(s)}{q(a|s) p(s)} = \frac{p_{\theta}(a|s) }{ q(a|s)
    }\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In a Markov decision process, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a sequence of state-action pairs,
    &lt;span class="math"&gt;\(w_{\theta}(x) = \frac{p_{\theta}(x)}{q(x)} = \frac{ p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) p_\theta(a_t|s_t) } { p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) q(a_t|s_t) } = \frac{\prod_{t=0}^T \pi_\theta(a_t|s_t)}
    {\prod_{t=0}^T q(a_t|s_t)}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: These estimators can all be improved with variance
reduction techniques. Probably the most effective technique is using
&lt;a href="https://en.wikipedia.org/wiki/Control_variates"&gt;control variates&lt;/a&gt; (of which
baseline functions are a special case). These are random variables correlated
with &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; for which we know their expectations (or at least they are estimated
separately). A great example is how ad clicks depend strongly on time-of-day
(fewer people are online late at night so we get fewer clicks), thus the
time-of-day covariate explains a large part of the variation in &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimation instead of optimization&lt;/strong&gt;: You can use this general setup for
estimation instead of optimization, in which case it's fine to let &lt;span class="math"&gt;\(r\)&lt;/span&gt; have
real-valued multivariate output. The confidence intervals are probably useful in
that setting too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: Often &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an existing complex system, which does not record
its probabilities. It is possible to use regression to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; from the
samples, which is called the &lt;strong&gt;propensity score&lt;/strong&gt; (PS). PS attempts to account
for &lt;strong&gt;confounding variables&lt;/strong&gt;, which are hidden causes that control variation in
the data. Failing to account for confounding variables may lead to
&lt;a href="https://en.wikipedia.org/wiki/Simpson's_paradox"&gt;incorrect conclusions&lt;/a&gt;. Unfortunately,
PS results in a biased estimator because we're using a 'ratio of expectations'
(we'll divide by the PS estimate) instead of an 'expectation of ratios'. PS is
only statistically consistent in the (unlikely) event that the density estimate
is correctly specified (i.e., we can eventually get &lt;span class="math"&gt;\(q\)&lt;/span&gt; correct). In the unknown
&lt;span class="math"&gt;\(q\)&lt;/span&gt; setting, it's often better to use the &lt;strong&gt;doubly-robust estimator&lt;/strong&gt; (DR) which
combines &lt;em&gt;two&lt;/em&gt; estimators: a density estimator for &lt;span class="math"&gt;\(q\)&lt;/span&gt; and a function
approximation for &lt;span class="math"&gt;\(r\)&lt;/span&gt;. A great explanation for the bandit case is in
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Dudík et al. (2011)&lt;/a&gt;. The DR estimator is also
biased, but it has a better bias-variance tradeoff than PS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; doesn't have support everywhere?&lt;/strong&gt; This is an especially important
setting because it is often the case that data collection policies abide by some
&lt;strong&gt;safety regulations&lt;/strong&gt;, which prevent known bad configurations. In many
situations, evaluating &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; corresponds to executing an action &lt;span class="math"&gt;\(x\)&lt;/span&gt; in the real
world so terrible outcomes could occur, such as, breaking a system, giving a
patient a bad treatment, or losing money. V1 is ok to use as long as we satisfy
the importance sampling support conditions, which might mean rejecting certain
values for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; (might be non-trivial to enforce) and consequently finding a
less-optimal policy. V2 and V3 are ok to use without an explicit constraint, but
additional care may be needed to ensure specific safety constraints are
satisfied by the learned policy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; is deterministic?&lt;/strong&gt; This is related to the point above. This is a
hard problem. Essentially, this trying to learn without any exploration /
experimentation! In general, we need exploration to learn. Randomization isn't
the only way to perform exploration, there are many systematic types of
experimentation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are some cases of systematic experimentation that are ok. For example,
    enumerating all elements of &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; (almost certainly
    infeasible). Another example is a contextual bandit where &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns
    actions to contexts deterministically via a hash function (this setting is
    fine because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is essentially a uniform distribution over actions, which
    is independent of the state). In other special cases, we &lt;em&gt;may&lt;/em&gt; be able to
    characterize systematic exploration as
    &lt;a href="https://en.wikipedia.org/wiki/Stratified_sampling"&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A generic solution might be to apply the doubly-robust estimator, which
    "smooths out" deterministic components (by pretending they are random) and
    accounting for confounds (by explicitly modeling them in the propensity
    score).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What if we control data collection (&lt;span class="math"&gt;\(q\)&lt;/span&gt;)?&lt;/strong&gt; This is an interesting
setting. Essentially, it asks "how do we explore/experiment optimally (and
safely)?". In general, this is an open question and depends on many
considerations, such as, how much control, exploration cost (safety constraints)
and prior knowledge (of &lt;span class="math"&gt;\(r\)&lt;/span&gt; and unknown factors in the environment). I've seen
some papers cleverly design &lt;span class="math"&gt;\(q\)&lt;/span&gt;. The first that comes to mind is
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Levine &amp;amp; Koltun (2013)&lt;/a&gt;. Another
setting is &lt;em&gt;online&lt;/em&gt; contextual bandits, in which algorithms like
&lt;a href="http://jmlr.org/proceedings/papers/v15/beygelzimer11a/beygelzimer11a.pdf"&gt;EXP4&lt;/a&gt;
and
&lt;a href="http://www.research.rutgers.edu/~lihong/pub/Chapelle12Empirical.pdf"&gt;Thompson sampling&lt;/a&gt;,
prescribe certain types of exploration and work interactively (i.e., they don't
have a fixed training sample). Lastly, I'll mention that there are many
techniques for variance reduction by importance sampling, which may apply.&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max
Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson.
&lt;a href="https://arxiv.org/abs/1209.2355"&gt;Counterfactual reasoning in learning systems&lt;/a&gt;.
JMLR 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The source for the majority of this post. It includes many other interesting
ideas and goes more in depth into some of the details.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Miroslav Dudík, John Langford, Lihong Li.
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Doubly robust policy evaluation and learning&lt;/a&gt;.
ICML 2011.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discussed in extensions section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Philip S. Thomas.
&lt;a href="http://psthomas.com/papers/Thomas2015c.pdf"&gt;Safe reinforcement learning&lt;/a&gt;.
PhD Thesis 2015.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Covers confidence intervals for policy evaluation similar to Bottou et al., as
well as learning algorithms for RL with safety guarantees (e.g., so we don't
break the robot).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Peshkin and Shelton.
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Learning from scarce experience&lt;/a&gt;.
ICML 2002.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An older RL paper, which covers learning from logged data. This is one of the
earliest papers on learning from logged data that I could find.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Levine and Koltun.
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Guided policy search&lt;/a&gt;.
ICML 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses clever choices for &lt;span class="math"&gt;\(q\)&lt;/span&gt; to better-guide learning in the RL setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corinna Cortes, Yishay Mansour, Mehryar Mohri.
&lt;a href="https://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;Learning bounds for importance weighting&lt;/a&gt;.
NIPS 2010.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses &lt;em&gt;generalization bounds&lt;/em&gt; for the counterfactual objective. Includes an
alternative weighting scheme to keep importance weights from exploding.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;style&gt; .toggle-button { background-color: #555555; border: none; color: white;
padding: 10px 15px; border-radius: 6px; text-align: center; text-decoration:
none; display: inline-block; font-size: 16px; cursor: pointer; } .derivation {
background-color: #f2f2f2; border: thin solid #ddd; padding: 10px;
margin-bottom: 10px; } &lt;/style&gt;

&lt;script&gt;
/* workaround for when markdown/mathjax gets confused by the javascript dollar function. */
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;Counterfactual reasoning is &lt;em&gt;reasoning about data that we did not observe&lt;/em&gt;. For
example, reasoning about the expected reward of new policy given data collected
from a older one.&lt;/p&gt;
&lt;p&gt;In this post, I'll discuss some basic techniques for learning from logged
data. For the large part, this post is based on things I learned from
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Peshkin &amp;amp; Shelton (2002)&lt;/a&gt;
and &lt;a href="https://arxiv.org/abs/1209.2355"&gt;Bottou et al. (2013)&lt;/a&gt; (two of all-time
favorite papers).&lt;/p&gt;
&lt;p&gt;After reading, have a look at the
&lt;a href="https://gist.github.com/timvieira/788c2c25c94663c49abada60f2e107e9"&gt;Jupyter notebook&lt;/a&gt;
accompanying this post!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; (&lt;em&gt;off-line off-policy optimization&lt;/em&gt;): We're trying to optimize a
function of the form,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \underset{p_\theta}{\mathbb{E}} \left[ r(x) \right] = \sum_{x \in \mathcal{X}} p_\theta(x) r(x).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; But! We only have a &lt;em&gt;fixed&lt;/em&gt; sample of size &lt;span class="math"&gt;\(m\)&lt;/span&gt; from a data collection
policy &lt;span class="math"&gt;\(q\)&lt;/span&gt;, &lt;span class="math"&gt;\(\{ (r^{(j)}, x^{(j)} ) \}_{j=1}^m \overset{\text{i.i.d.}} \sim q.\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Although, it's not &lt;em&gt;necessarily&lt;/em&gt; the case, you can think of &lt;span class="math"&gt;\(q = p_{\theta'}\)&lt;/span&gt;
  for a &lt;em&gt;fixed&lt;/em&gt; value &lt;span class="math"&gt;\(\theta'.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; is an arbitrary multivariate space, which permits a mix of
  continuous and discrete components, with appropriate densities, &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt;
  and &lt;span class="math"&gt;\(q\)&lt;/span&gt; defined over it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(r: \mathcal{X} \mapsto \mathbb{R}\)&lt;/span&gt; is a black box that outputs a scalar
  score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I've used the notation &lt;span class="math"&gt;\(r^{(j)}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(r(x^{(j)})\)&lt;/span&gt; to emphasize that we
  can't evaluate &lt;span class="math"&gt;\(r\)&lt;/span&gt; at &lt;span class="math"&gt;\(x\)&lt;/span&gt; values other than those in the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We'll assume that &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns positive probability everywhere, &lt;span class="math"&gt;\(q(x) &amp;gt; 0\)&lt;/span&gt; for
  all &lt;span class="math"&gt;\(x \in \mathcal{X}\)&lt;/span&gt;. This means is that the data collection process must
  be randomized and eventually sample all possible configurations. Later, I
  discuss relaxing this assumption.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each distribution is a product of one or more factors of the following types:
&lt;strong&gt;policy factors&lt;/strong&gt; (at least one), which directly depend on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, and
&lt;strong&gt;environment factors&lt;/strong&gt; (possibly none), which do not depend directly on
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Note that environment factors are &lt;em&gt;only&lt;/em&gt; accessible via sampling
(i.e., we don't know the &lt;em&gt;value&lt;/em&gt; they assign to a sample). For example, a
&lt;em&gt;contextual bandit problem&lt;/em&gt;, where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a context-action pair, &lt;span class="math"&gt;\(x =
(s,a)\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(q(x) = q(a|s) p(s)\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{\theta}(x) = p_{\theta}(a|s)
p(s)\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share the environment factor &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;, the
distribution over contexts, and only differ in the action-given-context
factor. For now, assume that we can evaluate all environment factors; later,
I'll discuss how we cleverly work around it.&lt;/p&gt;
&lt;!--
(We can
even extend it to a full-blown MDP or POMDP by taking $x$ to be a sequence of
state-action pairs, often called "trajectories".)
--&gt;

&lt;p&gt;&lt;strong&gt;The main challenge&lt;/strong&gt; of this setting is that we don't have controlled
experiments to learn from because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is not (completely) in our control. This
manifests itself as high variance ("noise") in estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;. Consider
the contextual bandit setting, we receive a context &lt;span class="math"&gt;\(s\)&lt;/span&gt; and execute single
action; we never get to rollback to that precise context and try an alternative
action (to get a paired sample #yolo) because we do not control &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;. This is
an important paradigm for many 'real world' problems, e.g., predicting medical
treatments or ad selection.&lt;/p&gt;
&lt;!--
This is the crucial difference that makes counterfactual learning
more difficult than (fully) supervised learning.
--&gt;

&lt;p&gt;&lt;strong&gt;Estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;&lt;/strong&gt; [V1]: We obtain an unbiased estimator of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;
with
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta)
\approx \hat{J}_{\!\text{IS}}(\theta)
= \frac{1}{m} \sum_{j=1}^m r^{(j)} \!\cdot\! w^{(j)}_{\theta}
\quad \text{ where } w^{(j)}_{\theta} = \frac{p_{\theta}(x^{(j)}) }{ q(x^{(j)}) }.
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; This estimator is remarkable: it uses importance sampling as a function
approximator! We have an &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt; for any value of
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that we like. &lt;em&gt;The catch&lt;/em&gt; is that we have to pick &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; a priori,
i.e., with no knowledge of the sample.&lt;/p&gt;
&lt;!--
We also require that the usual 'support
conditions' for importance sampling conditions ($p_{\theta}(x)&gt;0 \Rightarrow
q(x)&gt;0$ for all $x \in \mathcal{X}$, which is why we made assumption A1.
--&gt;

&lt;p&gt;After we've collected a (large) sample it's possible to optimize
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; using any optimization algorithm (e.g., L-BFGS). Of
course, we risk overfitting to the sample if we evaluate
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt;. Actually, it's a bit worse: this objective tends to
favor regions of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, which are not well represented in the sample because
the importance sampling estimator has high variance in these regions resulting
from large importance weights (when &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; is small and &lt;span class="math"&gt;\(p_{\theta}(x)\)&lt;/span&gt; is
large, &lt;span class="math"&gt;\(w(x)\)&lt;/span&gt; is large and consequently so is &lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; regardless
of whether &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; is high!). Thus, we want some type of "regularization" to keep
the optimizer in regions which are sufficiently well-represented by the sample.&lt;/p&gt;
&lt;!--
**Visual example**: We can visualize this phenomena in a simple example. Let $q
= \mathcal{N}(0, \sigma=5)$, $r(x) = 1 \text{ if } x \in [2, 3], 0.2 \text{
otherwise},$ and $p_\theta = \mathcal{N}(\theta, \sigma=1)$.  This example is
nice because it let's us plot $x$ and $\theta$ in the same space. This is
generally not the case, because $\mathcal{X}$ may have no connection to $\theta$
space, e.g., $\mathcal{X}$ may be discrete.

**TODO** add plot
--&gt;

&lt;p&gt;&lt;strong&gt;Better surrogate&lt;/strong&gt; [V2]: There are many ways to improve the variance of the
estimator and &lt;em&gt;confidently&lt;/em&gt; obtain improvements to the system. One of my
favorites is Bottou et al.'s lower bound on &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;, which we get by
clipping importance weights, replace &lt;span class="math"&gt;\(w^{(j)}_{\theta}\)&lt;/span&gt; with &lt;span class="math"&gt;\(\min(R,
w^{(j)}_{\theta})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Confidence intervals&lt;/strong&gt; [V3]: We can augment the V2 lower bound with confidence
intervals derived from the empirical Bernstein bound (EBB). We'll require that
&lt;span class="math"&gt;\(r\)&lt;/span&gt; is bounded and that we know its max/min values. The EBB &lt;em&gt;penalizes&lt;/em&gt;
hypotheses (values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;) which have higher sample variance. (Note: a
Hoeffding bound wouldn't change the &lt;em&gt;shape&lt;/em&gt; of the objective, but EBB does
thanks to the sample variance penalty. EBB tends to be tighter.). The EBB
introduces an additional "confidence" hyperparameter, &lt;span class="math"&gt;\((1-\delta)\)&lt;/span&gt;. Bottou et
al. recommend maximizing the lower bound as it provides safer improvements. See
the original paper for the derivation.&lt;/p&gt;
&lt;!--
An important benefit of having upper *and* lower is that the bounds tell
us whether or not we should collect more data
--&gt;

&lt;p&gt;Both V2 and V3 are &lt;em&gt;biased&lt;/em&gt; (as they are lower bounds), but we can mitigate the
bias by &lt;em&gt;tuning&lt;/em&gt; the hyperparameter &lt;span class="math"&gt;\(R\)&lt;/span&gt; on a heldout sample (we can even tune
&lt;span class="math"&gt;\(\delta\)&lt;/span&gt;, if desired). Additionally, V2 and V3 are 'valid' when &lt;span class="math"&gt;\(q\)&lt;/span&gt; has limited
support since they prevent the importance weights from exploding (of course, the
bias can be arbitrarily bad, but probably unavoidable given the
learning-from-only-logged data setup).&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;!--
**Be warned**: This may be considered an idealized setting. Much of the research
in counterfactual and causal reasoning targets (often subtle) deviations from
these assumptions (and some different questions, of course). Some extensions and
discussion appear towards the end of the post.
--&gt;

&lt;p&gt;&lt;strong&gt;Unknown environment factors&lt;/strong&gt;: Consider the contextual bandit setting
(mentioned above). Here &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share an &lt;em&gt;unknown&lt;/em&gt; environment factor: the
distribution of contexts. Luckily, we do not need to know the value of this
factor in order to apply any of our estimators because they are all based on
likelihood &lt;em&gt;ratios&lt;/em&gt;, thus the shared unknown factors cancel out!  Some specific
examples are given below. Of course, these factors do influence the estimators
because they are crucial in &lt;em&gt;sampling&lt;/em&gt;, they just aren't necessary in
&lt;em&gt;evaluation&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In contextual bandit example, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a state-action pair, &lt;span class="math"&gt;\(w_{\theta}(x) =
    \frac{p_{\theta}(x)}{q(x)} = \frac{ p_{\theta}(s,a) }{ q(s,a) } =
    \frac{p_{\theta}(a|s) p(s)}{q(a|s) p(s)} = \frac{p_{\theta}(a|s) }{ q(a|s)
    }\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In a Markov decision process, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a sequence of state-action pairs,
    &lt;span class="math"&gt;\(w_{\theta}(x) = \frac{p_{\theta}(x)}{q(x)} = \frac{ p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) p_\theta(a_t|s_t) } { p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) q(a_t|s_t) } = \frac{\prod_{t=0}^T \pi_\theta(a_t|s_t)}
    {\prod_{t=0}^T q(a_t|s_t)}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: These estimators can all be improved with variance
reduction techniques. Probably the most effective technique is using
&lt;a href="https://en.wikipedia.org/wiki/Control_variates"&gt;control variates&lt;/a&gt; (of which
baseline functions are a special case). These are random variables correlated
with &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; for which we know their expectations (or at least they are estimated
separately). A great example is how ad clicks depend strongly on time-of-day
(fewer people are online late at night so we get fewer clicks), thus the
time-of-day covariate explains a large part of the variation in &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimation instead of optimization&lt;/strong&gt;: You can use this general setup for
estimation instead of optimization, in which case it's fine to let &lt;span class="math"&gt;\(r\)&lt;/span&gt; have
real-valued multivariate output. The confidence intervals are probably useful in
that setting too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: Often &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an existing complex system, which does not record
its probabilities. It is possible to use regression to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; from the
samples, which is called the &lt;strong&gt;propensity score&lt;/strong&gt; (PS). PS attempts to account
for &lt;strong&gt;confounding variables&lt;/strong&gt;, which are hidden causes that control variation in
the data. Failing to account for confounding variables may lead to
&lt;a href="https://en.wikipedia.org/wiki/Simpson's_paradox"&gt;incorrect conclusions&lt;/a&gt;. Unfortunately,
PS results in a biased estimator because we're using a 'ratio of expectations'
(we'll divide by the PS estimate) instead of an 'expectation of ratios'. PS is
only statistically consistent in the (unlikely) event that the density estimate
is correctly specified (i.e., we can eventually get &lt;span class="math"&gt;\(q\)&lt;/span&gt; correct). In the unknown
&lt;span class="math"&gt;\(q\)&lt;/span&gt; setting, it's often better to use the &lt;strong&gt;doubly-robust estimator&lt;/strong&gt; (DR) which
combines &lt;em&gt;two&lt;/em&gt; estimators: a density estimator for &lt;span class="math"&gt;\(q\)&lt;/span&gt; and a function
approximation for &lt;span class="math"&gt;\(r\)&lt;/span&gt;. A great explanation for the bandit case is in
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Dudík et al. (2011)&lt;/a&gt;. The DR estimator is also
biased, but it has a better bias-variance tradeoff than PS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; doesn't have support everywhere?&lt;/strong&gt; This is an especially important
setting because it is often the case that data collection policies abide by some
&lt;strong&gt;safety regulations&lt;/strong&gt;, which prevent known bad configurations. In many
situations, evaluating &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; corresponds to executing an action &lt;span class="math"&gt;\(x\)&lt;/span&gt; in the real
world so terrible outcomes could occur, such as, breaking a system, giving a
patient a bad treatment, or losing money. V1 is ok to use as long as we satisfy
the importance sampling support conditions, which might mean rejecting certain
values for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; (might be non-trivial to enforce) and consequently finding a
less-optimal policy. V2 and V3 are ok to use without an explicit constraint, but
additional care may be needed to ensure specific safety constraints are
satisfied by the learned policy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; is deterministic?&lt;/strong&gt; This is related to the point above. This is a
hard problem. Essentially, this trying to learn without any exploration /
experimentation! In general, we need exploration to learn. Randomization isn't
the only way to perform exploration, there are many systematic types of
experimentation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are some cases of systematic experimentation that are ok. For example,
    enumerating all elements of &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; (almost certainly
    infeasible). Another example is a contextual bandit where &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns
    actions to contexts deterministically via a hash function (this setting is
    fine because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is essentially a uniform distribution over actions, which
    is independent of the state). In other special cases, we &lt;em&gt;may&lt;/em&gt; be able to
    characterize systematic exploration as
    &lt;a href="https://en.wikipedia.org/wiki/Stratified_sampling"&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A generic solution might be to apply the doubly-robust estimator, which
    "smooths out" deterministic components (by pretending they are random) and
    accounting for confounds (by explicitly modeling them in the propensity
    score).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What if we control data collection (&lt;span class="math"&gt;\(q\)&lt;/span&gt;)?&lt;/strong&gt; This is an interesting
setting. Essentially, it asks "how do we explore/experiment optimally (and
safely)?". In general, this is an open question and depends on many
considerations, such as, how much control, exploration cost (safety constraints)
and prior knowledge (of &lt;span class="math"&gt;\(r\)&lt;/span&gt; and unknown factors in the environment). I've seen
some papers cleverly design &lt;span class="math"&gt;\(q\)&lt;/span&gt;. The first that comes to mind is
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Levine &amp;amp; Koltun (2013)&lt;/a&gt;. Another
setting is &lt;em&gt;online&lt;/em&gt; contextual bandits, in which algorithms like
&lt;a href="http://jmlr.org/proceedings/papers/v15/beygelzimer11a/beygelzimer11a.pdf"&gt;EXP4&lt;/a&gt;
and
&lt;a href="http://www.research.rutgers.edu/~lihong/pub/Chapelle12Empirical.pdf"&gt;Thompson sampling&lt;/a&gt;,
prescribe certain types of exploration and work interactively (i.e., they don't
have a fixed training sample). Lastly, I'll mention that there are many
techniques for variance reduction by importance sampling, which may apply.&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max
Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson.
&lt;a href="https://arxiv.org/abs/1209.2355"&gt;Counterfactual reasoning in learning systems&lt;/a&gt;.
JMLR 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The source for the majority of this post. It includes many other interesting
ideas and goes more in depth into some of the details.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Miroslav Dudík, John Langford, Lihong Li.
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Doubly robust policy evaluation and learning&lt;/a&gt;.
ICML 2011.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discussed in extensions section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Philip S. Thomas.
&lt;a href="http://psthomas.com/papers/Thomas2015c.pdf"&gt;Safe reinforcement learning&lt;/a&gt;.
PhD Thesis 2015.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Covers confidence intervals for policy evaluation similar to Bottou et al., as
well as learning algorithms for RL with safety guarantees (e.g., so we don't
break the robot).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Peshkin and Shelton.
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Learning from scarce experience&lt;/a&gt;.
ICML 2002.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An older RL paper, which covers learning from logged data. This is one of the
earliest papers on learning from logged data that I could find.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Levine and Koltun.
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Guided policy search&lt;/a&gt;.
ICML 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses clever choices for &lt;span class="math"&gt;\(q\)&lt;/span&gt; to better-guide learning in the RL setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corinna Cortes, Yishay Mansour, Mehryar Mohri.
&lt;a href="https://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;Learning bounds for importance weighting&lt;/a&gt;.
NIPS 2010.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses &lt;em&gt;generalization bounds&lt;/em&gt; for the counterfactual objective. Includes an
alternative weighting scheme to keep importance weights from exploding.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="counterfactual-reasoning"></category><category term="importance-sampling"></category><category term="machine-learning"></category></entry><entry><title>Heaps for incremental computation</title><link href="http://timvieira.github.io/blog/post/2016/11/21/heaps-for-incremental-computation/" rel="alternate"></link><published>2016-11-21T00:00:00-05:00</published><updated>2016-11-21T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-11-21:/blog/post/2016/11/21/heaps-for-incremental-computation/</id><summary type="html">&lt;p&gt;In this post, I'll describe a neat trick for maintaining a summary quantity
(e.g., sum, product, max, log-sum-exp, concatenation, cross-product) under
changes to its inputs. The trick and it's implementation are inspired by the
well-known max-heap datastructure. I'll also describe a really elegant
application to fast sampling under an evolving categorical distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we'd like to efficiently compute a summary quantity under
changes to its &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional input vector &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;. The particular
form of the quantity we're going to compute is &lt;span class="math"&gt;\(z = \bigoplus_{i=1}^n w_i\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; is some associative binary operator with identity element
&lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#operator-mathy');"&gt;more formally...&lt;/button&gt;
&lt;div id="operator-mathy" class="derivation" style="display:none"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w} \in \boldsymbol{K}^n\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\oplus: \boldsymbol{K} \times \boldsymbol{K} \mapsto \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associative: &lt;span class="math"&gt;\((a \oplus b) \oplus c = a \oplus (b \oplus c)\)&lt;/span&gt; for all &lt;span class="math"&gt;\(a,b,c
  \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identity element: &lt;span class="math"&gt;\(\boldsymbol{0} \in \boldsymbol{K}\)&lt;/span&gt; such that &lt;span class="math"&gt;\(k \oplus
  \boldsymbol{0} = \boldsymbol{0} \oplus k = k\)&lt;/span&gt;, for all &lt;span class="math"&gt;\(k \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;The trick&lt;/strong&gt;: Essentially, the trick boils down to &lt;em&gt;parenthesis placement&lt;/em&gt; in
the expression which computes &lt;span class="math"&gt;\(z\)&lt;/span&gt;. A freedom we assumed via the associative
property.&lt;/p&gt;
&lt;p&gt;I'll demonstrate by example with &lt;span class="math"&gt;\(n=8\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Linear structure: We generally compute something like &lt;span class="math"&gt;\(z\)&lt;/span&gt; with a simple
loop. This looks like a right-branching binary tree when we think about the
order of operations,&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((((((w_1 \oplus w_2) \oplus w_3) \oplus w_4) \oplus w_5) \oplus w_6) \oplus w_7) \oplus w_8).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; Heap structure: Here the parentheses form a balanced tree, which looks
much more like a recursive implementation that computes the left and right
halves and &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;s the results (divide-and-conquer style),&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((w_1 \oplus w_2) \oplus (w_3 \oplus w_4)) \oplus ((w_5 \oplus w_6) \oplus (w_7 \oplus w_8))).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;
The benefit of the heap structure is that there are &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;
intermediate quantities that depend on any input, whereas the linear structure
has &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt;. The intermediate quantities correspond to the values of each of the
parenthesized expressions.&lt;/p&gt;
&lt;p&gt;Since fewer intermediate quantities depend on a given input, fewer intermediates
need to be adjusted upon a change to the input. Therefore, we get faster
algorithms for &lt;em&gt;maintaining&lt;/em&gt; the output quantity &lt;span class="math"&gt;\(z\)&lt;/span&gt; as the inputs change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Heap datastructure&lt;/strong&gt; (aka
&lt;a href="https://en.wikipedia.org/wiki/Fenwick_tree"&gt;binary index tree or Fenwick tree&lt;/a&gt;):
We're going to store the values of the intermediates quantities and inputs in a
heap datastructure, which is a &lt;em&gt;complete&lt;/em&gt; binary tree. In our case, the tree has
depth &lt;span class="math"&gt;\(1 + \lceil \log_2 n \rceil\)&lt;/span&gt;, with the values of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at it's
leaves (aligned left) and padding with &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt; for remaining
leaves. Thus, the array's length is &lt;span class="math"&gt;\(&amp;lt; 4 n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This structure makes our implementation really nice and efficient because we
don't need pointers to find the parent or children of a node (i.e., no need to
wrap elements into a "node" class like in a general tree data structure). So, we
can pack everything into an array, which means our implementation has great
memory/cache locality and low storage overhead.&lt;/p&gt;
&lt;p&gt;Traversing the tree is pretty simple: Let &lt;span class="math"&gt;\(d\)&lt;/span&gt; be the number of internal nodes,
nodes &lt;span class="math"&gt;\(1 \le i \le d\)&lt;/span&gt; are interal. For node &lt;span class="math"&gt;\(i\)&lt;/span&gt;, left child &lt;span class="math"&gt;\(\rightarrow {2
\cdot i},\)&lt;/span&gt; right child &lt;span class="math"&gt;\(\rightarrow {2 \cdot i + 1},\)&lt;/span&gt; parent &lt;span class="math"&gt;\(\rightarrow
\lfloor i / 2 \rfloor.\)&lt;/span&gt; (Note that these operations assume the array's indices
start at &lt;span class="math"&gt;\(1\)&lt;/span&gt;. We generally fake this by adding a dummy node at position &lt;span class="math"&gt;\(0\)&lt;/span&gt;,
which makes implementation simpler.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initializing the heap&lt;/strong&gt;: Here's code that initializes the heap structure we
  just described.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sumheap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Create sumheap from weights `w` in O(n) time.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  &lt;span class="c1"&gt;# number of intermediates&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                &lt;span class="c1"&gt;# intermediates + leaves&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;                     &lt;span class="c1"&gt;# store `w` at leaves.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Updating &lt;span class="math"&gt;\(w_k\)&lt;/span&gt;&lt;/strong&gt; boils down to fixing intermediate sums that (transitively)
  depend on &lt;span class="math"&gt;\(w_k.\)&lt;/span&gt; I won't go into all of the details here, instead I'll give
  code (below). I'd like to quickly point out that the term "parents" is not
  great for our purposes because they are actually the &lt;em&gt;dependents&lt;/em&gt;: when an
  input changes the value the parents, grand parents, great grand parents, etc,
  become stale and need to be recomputed bottom up (from the leaves). The code
  below implements the update method for changing the value of &lt;span class="math"&gt;\(w_k\)&lt;/span&gt; and runs in
  &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Update w[k] = v` in time O(log n).&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# fix parents in the tree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;//=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical stability&lt;/strong&gt;: If the operations are noisy (e.g., floating point
   operator), then the heap version may be better behaved. For example, if
   operations have an independent, additive noise rate &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; then noise
   of &lt;span class="math"&gt;\(z_{\text{heap}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot \log n)\)&lt;/span&gt;, whereas
   &lt;span class="math"&gt;\(z_{\text{linear}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot n)\)&lt;/span&gt;. (Without further
   assumptions about the underlying operator, I don't believe you can do better
   than that.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relationship to max-heap&lt;/strong&gt;: In the case of a max or min heap, we can avoid
   allocating extra space for intermediate quantities because all intermediates
   values are equal to exactly one element of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change propagation&lt;/strong&gt;: The general idea of &lt;em&gt;adjusting&lt;/em&gt; cached intermediate
   quantities is a neat idea. In fact, we encounter it each time we type
   &lt;code&gt;make&lt;/code&gt; at the command line! The general technique goes by many
   names&amp;mdash;including change propagation, incremental maintenance, and
   functional reactive programming&amp;mdash;and applies to basically &lt;em&gt;any&lt;/em&gt;
   side-effect-free computation. However, it's most effective when the
   dependency structure of the computation is sparse and requires little
   overhead to find and refresh stale values. In our example of computing &lt;span class="math"&gt;\(z\)&lt;/span&gt;,
   these considerations manifest themselves as the heap vs linear structures and
   our fast array implementation instead of a generic tree datastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Generalizations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No zero? No problem. We don't &lt;em&gt;actually&lt;/em&gt; require a zero element. So, it's
   fair to augment &lt;span class="math"&gt;\(\boldsymbol{K} \cup \{ \textsf{null} \}\)&lt;/span&gt; where
   &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; is distinguished value (i.e., &lt;span class="math"&gt;\(\textsf{null} \notin
   \boldsymbol{K}\)&lt;/span&gt;) that &lt;em&gt;acts&lt;/em&gt; just like a zero after we overload &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; to
   satisfy the definition of a zero (e.g., by adding an if-statement).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to an arbitrary maps instead of fixed vectors is possible with
   a "locator" map, which a bijective map from elements to indices in a dense
   array.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for growing and shrinking: We support &lt;strong&gt;growing&lt;/strong&gt; by maintaining an
   underlying array that is always slightly larger than we need&amp;mdash;which
   we're &lt;em&gt;already&lt;/em&gt; doing in the heap datastructure. Doubling the size of the
   underlying array (i.e., rounding up to the next power of two) has the added
   benefit of allowing us to grow &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at no asymptotic cost!  This
   is because the resize operation, which requires an &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt; time to
   allocate a new array and copying old values, happens so infrequently that
   they can be completely amortized. We get of effect of &lt;strong&gt;shrinking&lt;/strong&gt; by
   replacing the old value with &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; (or &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;). We can
   shrink the underlying array when the fraction of nonzeros dips below
   &lt;span class="math"&gt;\(25\%\)&lt;/span&gt;. This prevents "thrashing" between shrinking and growing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Application&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sampling from an evolving distribution&lt;/strong&gt;: Suppose that &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;
corresponds to a categorical distributions over &lt;span class="math"&gt;\(\{1, \ldots, n\}\)&lt;/span&gt; and that we'd
like to sample elements from in proportion to this (unnormalized) distribution.&lt;/p&gt;
&lt;p&gt;Other methods like the &lt;a href="http://www.keithschwarz.com/darts-dice-coins/"&gt;alias&lt;/a&gt; or
inverse CDF methods are efficient after a somewhat costly initialization
step. But! they are not as efficient as the heap sampler when the distribution
is being updated. (I'm not sure about whether variants of alias that support
updates exist.)&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Init&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;alias&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;td&gt;O(n)?&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;i-CDF&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heap&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Use cases include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Gibbs_sampling"&gt;Gibbs sampling&lt;/a&gt;, where
  distributions are constantly modified and sampled from (changes may not be
  sparse so YMMV). The heap sampler is used in
  &lt;a href="https://arxiv.org/abs/1412.4986"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/"&gt;EXP3&lt;/a&gt;
  (&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;mutli-armed bandit algorithm&lt;/a&gt;)
  is an excellent example of an algorithm that samples and modifies a single
  weight in the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Stochastic priority queues&lt;/em&gt; where we sample proportional to priority and the
  weights on items in the queue may change, elements are possibly removed after
  they are sampled (i.e., sampling without replacement), and elements are added.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, I won't spell out all of the details of these algorithms. Instead, I'll
just give the code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inverse CDF sampling&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Ordinary sampling method, O(n) init, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;            &lt;span class="c1"&gt;# build cdf, O(n)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;     &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# binary search, O(log n)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Heap sampling&lt;/strong&gt; is essentially the same, except the cdf is stored as heap,
which is perfect for binary search!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hsample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Sample from sumheap, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="c1"&gt;# number of internal nodes.&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="c1"&gt;# Use binary search to find the index of the largest CDF (represented as a&lt;/span&gt;
    &lt;span class="c1"&gt;# heap) value that is less than a random probe.&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Determine if the value is in the left or right subtree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;         &lt;span class="c1"&gt;# Point at left child&lt;/span&gt;
        &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="c1"&gt;# Probability mass under left subtree.&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# Value is in right subtree.&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;  &lt;span class="c1"&gt;# Subtract mass from left subtree&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;     &lt;span class="c1"&gt;# Point at right child&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Complete code and test cases for heap sampling are available in this
&lt;a href="https://gist.github.com/timvieira/da31b56436045a3122f5adf5aafec515"&gt;gist&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;In this post, I'll describe a neat trick for maintaining a summary quantity
(e.g., sum, product, max, log-sum-exp, concatenation, cross-product) under
changes to its inputs. The trick and it's implementation are inspired by the
well-known max-heap datastructure. I'll also describe a really elegant
application to fast sampling under an evolving categorical distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we'd like to efficiently compute a summary quantity under
changes to its &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional input vector &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;. The particular
form of the quantity we're going to compute is &lt;span class="math"&gt;\(z = \bigoplus_{i=1}^n w_i\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; is some associative binary operator with identity element
&lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#operator-mathy');"&gt;more formally...&lt;/button&gt;
&lt;div id="operator-mathy" class="derivation" style="display:none"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w} \in \boldsymbol{K}^n\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\oplus: \boldsymbol{K} \times \boldsymbol{K} \mapsto \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associative: &lt;span class="math"&gt;\((a \oplus b) \oplus c = a \oplus (b \oplus c)\)&lt;/span&gt; for all &lt;span class="math"&gt;\(a,b,c
  \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identity element: &lt;span class="math"&gt;\(\boldsymbol{0} \in \boldsymbol{K}\)&lt;/span&gt; such that &lt;span class="math"&gt;\(k \oplus
  \boldsymbol{0} = \boldsymbol{0} \oplus k = k\)&lt;/span&gt;, for all &lt;span class="math"&gt;\(k \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;The trick&lt;/strong&gt;: Essentially, the trick boils down to &lt;em&gt;parenthesis placement&lt;/em&gt; in
the expression which computes &lt;span class="math"&gt;\(z\)&lt;/span&gt;. A freedom we assumed via the associative
property.&lt;/p&gt;
&lt;p&gt;I'll demonstrate by example with &lt;span class="math"&gt;\(n=8\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Linear structure: We generally compute something like &lt;span class="math"&gt;\(z\)&lt;/span&gt; with a simple
loop. This looks like a right-branching binary tree when we think about the
order of operations,&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((((((w_1 \oplus w_2) \oplus w_3) \oplus w_4) \oplus w_5) \oplus w_6) \oplus w_7) \oplus w_8).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; Heap structure: Here the parentheses form a balanced tree, which looks
much more like a recursive implementation that computes the left and right
halves and &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;s the results (divide-and-conquer style),&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((w_1 \oplus w_2) \oplus (w_3 \oplus w_4)) \oplus ((w_5 \oplus w_6) \oplus (w_7 \oplus w_8))).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;
The benefit of the heap structure is that there are &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;
intermediate quantities that depend on any input, whereas the linear structure
has &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt;. The intermediate quantities correspond to the values of each of the
parenthesized expressions.&lt;/p&gt;
&lt;p&gt;Since fewer intermediate quantities depend on a given input, fewer intermediates
need to be adjusted upon a change to the input. Therefore, we get faster
algorithms for &lt;em&gt;maintaining&lt;/em&gt; the output quantity &lt;span class="math"&gt;\(z\)&lt;/span&gt; as the inputs change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Heap datastructure&lt;/strong&gt; (aka
&lt;a href="https://en.wikipedia.org/wiki/Fenwick_tree"&gt;binary index tree or Fenwick tree&lt;/a&gt;):
We're going to store the values of the intermediates quantities and inputs in a
heap datastructure, which is a &lt;em&gt;complete&lt;/em&gt; binary tree. In our case, the tree has
depth &lt;span class="math"&gt;\(1 + \lceil \log_2 n \rceil\)&lt;/span&gt;, with the values of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at it's
leaves (aligned left) and padding with &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt; for remaining
leaves. Thus, the array's length is &lt;span class="math"&gt;\(&amp;lt; 4 n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This structure makes our implementation really nice and efficient because we
don't need pointers to find the parent or children of a node (i.e., no need to
wrap elements into a "node" class like in a general tree data structure). So, we
can pack everything into an array, which means our implementation has great
memory/cache locality and low storage overhead.&lt;/p&gt;
&lt;p&gt;Traversing the tree is pretty simple: Let &lt;span class="math"&gt;\(d\)&lt;/span&gt; be the number of internal nodes,
nodes &lt;span class="math"&gt;\(1 \le i \le d\)&lt;/span&gt; are interal. For node &lt;span class="math"&gt;\(i\)&lt;/span&gt;, left child &lt;span class="math"&gt;\(\rightarrow {2
\cdot i},\)&lt;/span&gt; right child &lt;span class="math"&gt;\(\rightarrow {2 \cdot i + 1},\)&lt;/span&gt; parent &lt;span class="math"&gt;\(\rightarrow
\lfloor i / 2 \rfloor.\)&lt;/span&gt; (Note that these operations assume the array's indices
start at &lt;span class="math"&gt;\(1\)&lt;/span&gt;. We generally fake this by adding a dummy node at position &lt;span class="math"&gt;\(0\)&lt;/span&gt;,
which makes implementation simpler.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initializing the heap&lt;/strong&gt;: Here's code that initializes the heap structure we
  just described.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sumheap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Create sumheap from weights `w` in O(n) time.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  &lt;span class="c1"&gt;# number of intermediates&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                &lt;span class="c1"&gt;# intermediates + leaves&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;                     &lt;span class="c1"&gt;# store `w` at leaves.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Updating &lt;span class="math"&gt;\(w_k\)&lt;/span&gt;&lt;/strong&gt; boils down to fixing intermediate sums that (transitively)
  depend on &lt;span class="math"&gt;\(w_k.\)&lt;/span&gt; I won't go into all of the details here, instead I'll give
  code (below). I'd like to quickly point out that the term "parents" is not
  great for our purposes because they are actually the &lt;em&gt;dependents&lt;/em&gt;: when an
  input changes the value the parents, grand parents, great grand parents, etc,
  become stale and need to be recomputed bottom up (from the leaves). The code
  below implements the update method for changing the value of &lt;span class="math"&gt;\(w_k\)&lt;/span&gt; and runs in
  &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Update w[k] = v` in time O(log n).&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# fix parents in the tree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;//=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical stability&lt;/strong&gt;: If the operations are noisy (e.g., floating point
   operator), then the heap version may be better behaved. For example, if
   operations have an independent, additive noise rate &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; then noise
   of &lt;span class="math"&gt;\(z_{\text{heap}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot \log n)\)&lt;/span&gt;, whereas
   &lt;span class="math"&gt;\(z_{\text{linear}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot n)\)&lt;/span&gt;. (Without further
   assumptions about the underlying operator, I don't believe you can do better
   than that.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relationship to max-heap&lt;/strong&gt;: In the case of a max or min heap, we can avoid
   allocating extra space for intermediate quantities because all intermediates
   values are equal to exactly one element of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change propagation&lt;/strong&gt;: The general idea of &lt;em&gt;adjusting&lt;/em&gt; cached intermediate
   quantities is a neat idea. In fact, we encounter it each time we type
   &lt;code&gt;make&lt;/code&gt; at the command line! The general technique goes by many
   names&amp;mdash;including change propagation, incremental maintenance, and
   functional reactive programming&amp;mdash;and applies to basically &lt;em&gt;any&lt;/em&gt;
   side-effect-free computation. However, it's most effective when the
   dependency structure of the computation is sparse and requires little
   overhead to find and refresh stale values. In our example of computing &lt;span class="math"&gt;\(z\)&lt;/span&gt;,
   these considerations manifest themselves as the heap vs linear structures and
   our fast array implementation instead of a generic tree datastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Generalizations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No zero? No problem. We don't &lt;em&gt;actually&lt;/em&gt; require a zero element. So, it's
   fair to augment &lt;span class="math"&gt;\(\boldsymbol{K} \cup \{ \textsf{null} \}\)&lt;/span&gt; where
   &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; is distinguished value (i.e., &lt;span class="math"&gt;\(\textsf{null} \notin
   \boldsymbol{K}\)&lt;/span&gt;) that &lt;em&gt;acts&lt;/em&gt; just like a zero after we overload &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; to
   satisfy the definition of a zero (e.g., by adding an if-statement).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to an arbitrary maps instead of fixed vectors is possible with
   a "locator" map, which a bijective map from elements to indices in a dense
   array.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for growing and shrinking: We support &lt;strong&gt;growing&lt;/strong&gt; by maintaining an
   underlying array that is always slightly larger than we need&amp;mdash;which
   we're &lt;em&gt;already&lt;/em&gt; doing in the heap datastructure. Doubling the size of the
   underlying array (i.e., rounding up to the next power of two) has the added
   benefit of allowing us to grow &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at no asymptotic cost!  This
   is because the resize operation, which requires an &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt; time to
   allocate a new array and copying old values, happens so infrequently that
   they can be completely amortized. We get of effect of &lt;strong&gt;shrinking&lt;/strong&gt; by
   replacing the old value with &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; (or &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;). We can
   shrink the underlying array when the fraction of nonzeros dips below
   &lt;span class="math"&gt;\(25\%\)&lt;/span&gt;. This prevents "thrashing" between shrinking and growing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Application&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sampling from an evolving distribution&lt;/strong&gt;: Suppose that &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;
corresponds to a categorical distributions over &lt;span class="math"&gt;\(\{1, \ldots, n\}\)&lt;/span&gt; and that we'd
like to sample elements from in proportion to this (unnormalized) distribution.&lt;/p&gt;
&lt;p&gt;Other methods like the &lt;a href="http://www.keithschwarz.com/darts-dice-coins/"&gt;alias&lt;/a&gt; or
inverse CDF methods are efficient after a somewhat costly initialization
step. But! they are not as efficient as the heap sampler when the distribution
is being updated. (I'm not sure about whether variants of alias that support
updates exist.)&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Init&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;alias&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;td&gt;O(n)?&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;i-CDF&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heap&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Use cases include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Gibbs_sampling"&gt;Gibbs sampling&lt;/a&gt;, where
  distributions are constantly modified and sampled from (changes may not be
  sparse so YMMV). The heap sampler is used in
  &lt;a href="https://arxiv.org/abs/1412.4986"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/"&gt;EXP3&lt;/a&gt;
  (&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;mutli-armed bandit algorithm&lt;/a&gt;)
  is an excellent example of an algorithm that samples and modifies a single
  weight in the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Stochastic priority queues&lt;/em&gt; where we sample proportional to priority and the
  weights on items in the queue may change, elements are possibly removed after
  they are sampled (i.e., sampling without replacement), and elements are added.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, I won't spell out all of the details of these algorithms. Instead, I'll
just give the code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inverse CDF sampling&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Ordinary sampling method, O(n) init, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;            &lt;span class="c1"&gt;# build cdf, O(n)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;     &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# binary search, O(log n)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Heap sampling&lt;/strong&gt; is essentially the same, except the cdf is stored as heap,
which is perfect for binary search!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hsample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Sample from sumheap, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="c1"&gt;# number of internal nodes.&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="c1"&gt;# Use binary search to find the index of the largest CDF (represented as a&lt;/span&gt;
    &lt;span class="c1"&gt;# heap) value that is less than a random probe.&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Determine if the value is in the left or right subtree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;         &lt;span class="c1"&gt;# Point at left child&lt;/span&gt;
        &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="c1"&gt;# Probability mass under left subtree.&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# Value is in right subtree.&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;  &lt;span class="c1"&gt;# Subtract mass from left subtree&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;     &lt;span class="c1"&gt;# Point at right child&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Complete code and test cases for heap sampling are available in this
&lt;a href="https://gist.github.com/timvieira/da31b56436045a3122f5adf5aafec515"&gt;gist&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="sampling"></category><category term="datastructures"></category></entry><entry><title>Reversing a sequence with sublinear space</title><link href="http://timvieira.github.io/blog/post/2016/10/01/reversing-a-sequence-with-sublinear-space/" rel="alternate"></link><published>2016-10-01T00:00:00-04:00</published><updated>2016-10-01T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-10-01:/blog/post/2016/10/01/reversing-a-sequence-with-sublinear-space/</id><summary type="html">&lt;p&gt;Suppose we have a computation which generates sequence of states &lt;span class="math"&gt;\(s_1 \ldots
s_n\)&lt;/span&gt; according to &lt;span class="math"&gt;\(s_{t} = f(s_{t-1})\)&lt;/span&gt; where &lt;span class="math"&gt;\(s_0\)&lt;/span&gt; is given.&lt;/p&gt;
&lt;p&gt;We'd like to devise an algorithm, which can reconstruct each point in the
sequence efficiently as we traverse it backwards. You can think of this as
"hitting undo" from the end of the sequence or reversing a singly-liked list.&lt;/p&gt;
&lt;p&gt;Obviously, we &lt;em&gt;could&lt;/em&gt; just record the entire sequence, but if &lt;span class="math"&gt;\(n\)&lt;/span&gt; is large &lt;em&gt;or&lt;/em&gt;
the size of each state is large, this will be infeasible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 0&lt;/strong&gt;: Rerun the forward pass &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. Runtime &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt;, space
  &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 1&lt;/strong&gt;: Suppose we save &lt;span class="math"&gt;\(0 &amp;lt; k \le n\)&lt;/span&gt; evenly spaced "checkpoint" states.
Clearly, this gives us &lt;span class="math"&gt;\(\mathcal{O}(k)\)&lt;/span&gt; space, but what does it do to the
runtime?  Well, if we are at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; the we have to "replay" computation from
the last recorded checkpoint to get &lt;span class="math"&gt;\(s_t\)&lt;/span&gt;, which takes &lt;span class="math"&gt;\(O(n/k)\)&lt;/span&gt; time. Thus, the
overall runtimes becomes &lt;span class="math"&gt;\(O(n^2/k)\)&lt;/span&gt;. This runtime is not ideal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: &lt;em&gt;Idea 1&lt;/em&gt; did something kind of silly, within a chunk of size &lt;span class="math"&gt;\(n/k\)&lt;/span&gt;,
it does each computation multiple times! Suppose we increase the memory
requirement &lt;em&gt;just a little bit&lt;/em&gt; to remember the current chunk we're working on,
making it now &lt;span class="math"&gt;\(\mathcal{O}(k + n/k)\)&lt;/span&gt;. Now, we compute each state at most &lt;span class="math"&gt;\(2\)&lt;/span&gt;
times: once in the initial sequence and once in the reverse. This implies a
&lt;em&gt;linear&lt;/em&gt; runtime.  Now, the question: how should we set &lt;span class="math"&gt;\(k\)&lt;/span&gt; so that we minimize
extra space? Easy! Solve the following little optimization problem:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{k}{\textrm{argmin}}\ k+n/k = \sqrt{n}
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-optimal-space')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-optimal-space" style="display:none;" class="derivation"&gt;
To get the minimum, we solve for &lt;span class="math"&gt;\(k\)&lt;/span&gt; that sets the derivative to zero.
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
    0 &amp;amp;=&amp;amp; \frac{\partial}{\partial k} \left[ k+n/k \right] \\
      &amp;amp;=&amp;amp; 1-n/k^2 \\
n/k^2 &amp;amp;=&amp;amp; 1 \\
  k^2 &amp;amp;=&amp;amp; n \\
    k &amp;amp;=&amp;amp; \sqrt{n}
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Since it's safe to assume that &lt;span class="math"&gt;\(n,k \ge 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial^2}{\partial k\,
\partial k} = 2 n / k^3 &amp;gt; 0\)&lt;/span&gt; this is indeed a minimum. It's also global minimum
because &lt;span class="math"&gt;\(k+n/k\)&lt;/span&gt; is convex in &lt;span class="math"&gt;\(k\)&lt;/span&gt; when &lt;span class="math"&gt;\(n,k &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;That's nuts! We get away with &lt;em&gt;sublinear&lt;/em&gt; space &lt;span class="math"&gt;\(\mathcal{O}(\sqrt{n})\)&lt;/span&gt; and we
only blow up our runtime by a factor of 2. Also, I really love the "introduce a
parameter then optimize it out" trick.&lt;/p&gt;
&lt;p&gt;&lt;button onclick="toggle('#code-sqrt-space')"&gt;Code&lt;/button&gt;
&lt;div id="code-sqrt-space" style="display:none;"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sqrt_space&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# last chunk may be shorter than k.&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
            &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Take `k` steps from state `s`, save path. Cost: O(k) space, O(k) time.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Idea 3&lt;/strong&gt;: What if we apply "the remember &lt;span class="math"&gt;\(k\)&lt;/span&gt; states" trick &lt;em&gt;recursively&lt;/em&gt;? I'm
going to work this out for &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; (and then claim that the value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; doesn't
matter).&lt;/p&gt;
&lt;p&gt;Run forward to get the midpoint at &lt;span class="math"&gt;\(s_{m}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(m=b + \lfloor n/2
\rfloor\)&lt;/span&gt;. Next, recurse on the left and right chunks &lt;span class="math"&gt;\([b,m)\)&lt;/span&gt; and &lt;span class="math"&gt;\([m,e)\)&lt;/span&gt;.
We hit the base case when the width of the interval is
one.&lt;/p&gt;
&lt;p&gt;Note that we implicitly store midpoints as we recurse (thanks to the stack
frame).  The max depth of the recursion is &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;, which gives us
a &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; space bound.&lt;/p&gt;
&lt;p&gt;We can characterize runtime with the following recurrence relation, &lt;span class="math"&gt;\(T(n) = 2
\cdot T(n/2) + \mathcal{O}(n)\)&lt;/span&gt;. Since we recognize this as the recurrence for
mergesort, we know that it flattens to &lt;span class="math"&gt;\(\mathcal{O}(n \log n)\)&lt;/span&gt; time. Also, just
like in the case of sorting, the branching factor doesn't matter so we're happy
with or initial assumption that &lt;span class="math"&gt;\(k=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;button onclick="toggle('#code-recursive')"&gt;Code&lt;/button&gt;
&lt;div id="code-recursive" style="display:none;"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# do O(n/2) work to find the midpoint with O(1) space.&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;The algorithms describe in this post are generic algorithmic tricks, which has
been used in a number of place, including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The classic computer science interview problem of reversing a singly-linked list
  under a tight budget on &lt;em&gt;additional&lt;/em&gt; memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backpropagation for computing gradients in sequence models, including HMMs (&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2000/01/icslp00_logspace.pdf"&gt;Zweig &amp;amp; Padmanabhan, 2000&lt;/a&gt;)
  and RNNs (&lt;a href="https://arxiv.org/abs/1604.06174v2"&gt;Chen et al., 2016&lt;/a&gt;). I have
  sample code that illustrates the basic idea below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory-efficient &lt;a href="https://arxiv.org/pdf/cs/0310016v1"&gt;omniscient debugging&lt;/a&gt;,
  which allows a user to inspect program state while moving forward &lt;em&gt;and
  backward&lt;/em&gt; in time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sample code&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/timvieira/d2ac72ec3af7972d2471035011cbf1e2"&gt;The basics&lt;/a&gt;:
  Simple implementation complete with test cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/timvieira/aceb64047aed1b13bf4e4da3b9a4c0ea"&gt;Memory-efficient backprop in an RNN&lt;/a&gt;:
  A simple application with test cases, of course.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Suppose we have a computation which generates sequence of states &lt;span class="math"&gt;\(s_1 \ldots
s_n\)&lt;/span&gt; according to &lt;span class="math"&gt;\(s_{t} = f(s_{t-1})\)&lt;/span&gt; where &lt;span class="math"&gt;\(s_0\)&lt;/span&gt; is given.&lt;/p&gt;
&lt;p&gt;We'd like to devise an algorithm, which can reconstruct each point in the
sequence efficiently as we traverse it backwards. You can think of this as
"hitting undo" from the end of the sequence or reversing a singly-liked list.&lt;/p&gt;
&lt;p&gt;Obviously, we &lt;em&gt;could&lt;/em&gt; just record the entire sequence, but if &lt;span class="math"&gt;\(n\)&lt;/span&gt; is large &lt;em&gt;or&lt;/em&gt;
the size of each state is large, this will be infeasible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 0&lt;/strong&gt;: Rerun the forward pass &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. Runtime &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt;, space
  &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 1&lt;/strong&gt;: Suppose we save &lt;span class="math"&gt;\(0 &amp;lt; k \le n\)&lt;/span&gt; evenly spaced "checkpoint" states.
Clearly, this gives us &lt;span class="math"&gt;\(\mathcal{O}(k)\)&lt;/span&gt; space, but what does it do to the
runtime?  Well, if we are at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; the we have to "replay" computation from
the last recorded checkpoint to get &lt;span class="math"&gt;\(s_t\)&lt;/span&gt;, which takes &lt;span class="math"&gt;\(O(n/k)\)&lt;/span&gt; time. Thus, the
overall runtimes becomes &lt;span class="math"&gt;\(O(n^2/k)\)&lt;/span&gt;. This runtime is not ideal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: &lt;em&gt;Idea 1&lt;/em&gt; did something kind of silly, within a chunk of size &lt;span class="math"&gt;\(n/k\)&lt;/span&gt;,
it does each computation multiple times! Suppose we increase the memory
requirement &lt;em&gt;just a little bit&lt;/em&gt; to remember the current chunk we're working on,
making it now &lt;span class="math"&gt;\(\mathcal{O}(k + n/k)\)&lt;/span&gt;. Now, we compute each state at most &lt;span class="math"&gt;\(2\)&lt;/span&gt;
times: once in the initial sequence and once in the reverse. This implies a
&lt;em&gt;linear&lt;/em&gt; runtime.  Now, the question: how should we set &lt;span class="math"&gt;\(k\)&lt;/span&gt; so that we minimize
extra space? Easy! Solve the following little optimization problem:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{k}{\textrm{argmin}}\ k+n/k = \sqrt{n}
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-optimal-space')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-optimal-space" style="display:none;" class="derivation"&gt;
To get the minimum, we solve for &lt;span class="math"&gt;\(k\)&lt;/span&gt; that sets the derivative to zero.
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
    0 &amp;amp;=&amp;amp; \frac{\partial}{\partial k} \left[ k+n/k \right] \\
      &amp;amp;=&amp;amp; 1-n/k^2 \\
n/k^2 &amp;amp;=&amp;amp; 1 \\
  k^2 &amp;amp;=&amp;amp; n \\
    k &amp;amp;=&amp;amp; \sqrt{n}
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Since it's safe to assume that &lt;span class="math"&gt;\(n,k \ge 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial^2}{\partial k\,
\partial k} = 2 n / k^3 &amp;gt; 0\)&lt;/span&gt; this is indeed a minimum. It's also global minimum
because &lt;span class="math"&gt;\(k+n/k\)&lt;/span&gt; is convex in &lt;span class="math"&gt;\(k\)&lt;/span&gt; when &lt;span class="math"&gt;\(n,k &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;That's nuts! We get away with &lt;em&gt;sublinear&lt;/em&gt; space &lt;span class="math"&gt;\(\mathcal{O}(\sqrt{n})\)&lt;/span&gt; and we
only blow up our runtime by a factor of 2. Also, I really love the "introduce a
parameter then optimize it out" trick.&lt;/p&gt;
&lt;p&gt;&lt;button onclick="toggle('#code-sqrt-space')"&gt;Code&lt;/button&gt;
&lt;div id="code-sqrt-space" style="display:none;"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sqrt_space&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# last chunk may be shorter than k.&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
            &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Take `k` steps from state `s`, save path. Cost: O(k) space, O(k) time.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Idea 3&lt;/strong&gt;: What if we apply "the remember &lt;span class="math"&gt;\(k\)&lt;/span&gt; states" trick &lt;em&gt;recursively&lt;/em&gt;? I'm
going to work this out for &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; (and then claim that the value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; doesn't
matter).&lt;/p&gt;
&lt;p&gt;Run forward to get the midpoint at &lt;span class="math"&gt;\(s_{m}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(m=b + \lfloor n/2
\rfloor\)&lt;/span&gt;. Next, recurse on the left and right chunks &lt;span class="math"&gt;\([b,m)\)&lt;/span&gt; and &lt;span class="math"&gt;\([m,e)\)&lt;/span&gt;.
We hit the base case when the width of the interval is
one.&lt;/p&gt;
&lt;p&gt;Note that we implicitly store midpoints as we recurse (thanks to the stack
frame).  The max depth of the recursion is &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;, which gives us
a &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; space bound.&lt;/p&gt;
&lt;p&gt;We can characterize runtime with the following recurrence relation, &lt;span class="math"&gt;\(T(n) = 2
\cdot T(n/2) + \mathcal{O}(n)\)&lt;/span&gt;. Since we recognize this as the recurrence for
mergesort, we know that it flattens to &lt;span class="math"&gt;\(\mathcal{O}(n \log n)\)&lt;/span&gt; time. Also, just
like in the case of sorting, the branching factor doesn't matter so we're happy
with or initial assumption that &lt;span class="math"&gt;\(k=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;button onclick="toggle('#code-recursive')"&gt;Code&lt;/button&gt;
&lt;div id="code-recursive" style="display:none;"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# do O(n/2) work to find the midpoint with O(1) space.&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;The algorithms describe in this post are generic algorithmic tricks, which has
been used in a number of place, including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The classic computer science interview problem of reversing a singly-linked list
  under a tight budget on &lt;em&gt;additional&lt;/em&gt; memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backpropagation for computing gradients in sequence models, including HMMs (&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2000/01/icslp00_logspace.pdf"&gt;Zweig &amp;amp; Padmanabhan, 2000&lt;/a&gt;)
  and RNNs (&lt;a href="https://arxiv.org/abs/1604.06174v2"&gt;Chen et al., 2016&lt;/a&gt;). I have
  sample code that illustrates the basic idea below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory-efficient &lt;a href="https://arxiv.org/pdf/cs/0310016v1"&gt;omniscient debugging&lt;/a&gt;,
  which allows a user to inspect program state while moving forward &lt;em&gt;and
  backward&lt;/em&gt; in time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sample code&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/timvieira/d2ac72ec3af7972d2471035011cbf1e2"&gt;The basics&lt;/a&gt;:
  Simple implementation complete with test cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/timvieira/aceb64047aed1b13bf4e4da3b9a4c0ea"&gt;Memory-efficient backprop in an RNN&lt;/a&gt;:
  A simple application with test cases, of course.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="algorithms"></category></entry><entry><title>Evaluating ∇f(x) is as fast as f(x)</title><link href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/" rel="alternate"></link><published>2016-09-25T00:00:00-04:00</published><updated>2016-09-25T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-09-25:/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/</id><summary type="html">&lt;p&gt;Automatic differentiation ('autodiff' or 'backprop') is great&amp;mdash;not just
because it makes it easy to rapidly prototype deep networks with plenty of
doodads and geegaws, but because it means that evaluating the gradient &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; is as fast of computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;. In fact, the gradient provably requires at
most a &lt;em&gt;small&lt;/em&gt; constant factor more arithmetic operations than the function
itself.  Furthermore, autodiff tells us how to derive and implement the gradient
efficiently. This is a fascinating result that is perhaps not emphasized enough
in machine learning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The gradient should never be asymptotically slower than the function.&lt;/strong&gt; In my
recent &lt;a href="/doc/2016-emnlp-vocrf.pdf"&gt;EMNLP'16 paper&lt;/a&gt;, my coauthors and I found a
line of work on variable-order CRFs
(&lt;a href="https://papers.nips.cc/paper/3815-conditional-random-fields-with-high-order-features-for-sequence-labeling.pdf"&gt;Ye+'09&lt;/a&gt;;
&lt;a href="http://www.jmlr.org/papers/volume15/cuong14a/cuong14a.pdf"&gt;Cuong+'14&lt;/a&gt;), which
had an unnecessarily slow and complicated algorithm for computing gradients,
which was asymptotically (and practically) slower than their forward
algorithm. Without breaking a sweat, we derived a simpler and more efficient
gradient algorithm by simply applying backprop to the forward algorithm (and
made some other contributions).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Many algorithms are just backprop.&lt;/strong&gt; For example, forward-backward and
inside-outside, are actually just instances of automatic differentiation
(&lt;a href="https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf"&gt;Eisner,'16&lt;/a&gt;) (i.e.,
outside is just backprop on inside). This shouldn't be a surprise because these
algorithms are used to compute gradients. Basically, if you know backprop and
the inside algorithm, then you can derive the outside algorithm by applying the
backprop transform manually. I find it easier to understand the outside
algorithm via its connection to backprop, then via
&lt;a href="https://www.cs.jhu.edu/~jason/465/iobasics.pdf"&gt;the usual presentation&lt;/a&gt;. Note
that inside-outside and forward-backward pre-date backpropagation and have
additional uses beyond computing gradients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once you've grokked backprop, the world is your oyster!&lt;/strong&gt; You can backprop
through many approximate inference algorithms, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v15/stoyanov11a/stoyanov11a.pdf"&gt;Stoyanov+'11&lt;/a&gt;
and many of Justin Domke's papers, to avoid issues I've mentioned
&lt;a href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/"&gt;before&lt;/a&gt;. You
can even backprop through optimization algorithms to get gradients of dev loss wrt
hyperparameters, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf"&gt;Domke'12&lt;/a&gt; and
&lt;a href="https://arxiv.org/abs/1502.03492"&gt;Maclaurin+'15&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's at least one catch!&lt;/strong&gt; Although the &lt;em&gt;time&lt;/em&gt; complexity of computing the
gradient is as good as the function, the &lt;em&gt;space&lt;/em&gt; complexity may be much larger
because the autodiff recipe (at least the default reverse-mode one) requires memoizing
all intermediate quantities (e.g., the quantities you overwrite in a
loop). There are generic methods for balancing the time-space tradeoff in
autodiff, since you can (at least in theory) reconstruct the intermediate
quantities by playing the forward computation again from intermediate
checkpoints (at a cost to runtime, of course). A recent example is
&lt;a href="https://arxiv.org/abs/1606.03401"&gt;Gruslys+'16&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A final remark&lt;/strong&gt;. Despite the name "automatic" differentiation, there is no
need to rely on software to "automatically" give you gradient routines. Applying
the backprop transformation is generally easy to do manually and sometimes more
efficient than using a library. Many autodiff libraries lack good support for
dynamic computation graph, i.e., when the structure depends on quantities that
vary with the input (e.g., sentence length).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Automatic differentiation ('autodiff' or 'backprop') is great&amp;mdash;not just
because it makes it easy to rapidly prototype deep networks with plenty of
doodads and geegaws, but because it means that evaluating the gradient &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; is as fast of computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;. In fact, the gradient provably requires at
most a &lt;em&gt;small&lt;/em&gt; constant factor more arithmetic operations than the function
itself.  Furthermore, autodiff tells us how to derive and implement the gradient
efficiently. This is a fascinating result that is perhaps not emphasized enough
in machine learning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The gradient should never be asymptotically slower than the function.&lt;/strong&gt; In my
recent &lt;a href="/doc/2016-emnlp-vocrf.pdf"&gt;EMNLP'16 paper&lt;/a&gt;, my coauthors and I found a
line of work on variable-order CRFs
(&lt;a href="https://papers.nips.cc/paper/3815-conditional-random-fields-with-high-order-features-for-sequence-labeling.pdf"&gt;Ye+'09&lt;/a&gt;;
&lt;a href="http://www.jmlr.org/papers/volume15/cuong14a/cuong14a.pdf"&gt;Cuong+'14&lt;/a&gt;), which
had an unnecessarily slow and complicated algorithm for computing gradients,
which was asymptotically (and practically) slower than their forward
algorithm. Without breaking a sweat, we derived a simpler and more efficient
gradient algorithm by simply applying backprop to the forward algorithm (and
made some other contributions).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Many algorithms are just backprop.&lt;/strong&gt; For example, forward-backward and
inside-outside, are actually just instances of automatic differentiation
(&lt;a href="https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf"&gt;Eisner,'16&lt;/a&gt;) (i.e.,
outside is just backprop on inside). This shouldn't be a surprise because these
algorithms are used to compute gradients. Basically, if you know backprop and
the inside algorithm, then you can derive the outside algorithm by applying the
backprop transform manually. I find it easier to understand the outside
algorithm via its connection to backprop, then via
&lt;a href="https://www.cs.jhu.edu/~jason/465/iobasics.pdf"&gt;the usual presentation&lt;/a&gt;. Note
that inside-outside and forward-backward pre-date backpropagation and have
additional uses beyond computing gradients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once you've grokked backprop, the world is your oyster!&lt;/strong&gt; You can backprop
through many approximate inference algorithms, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v15/stoyanov11a/stoyanov11a.pdf"&gt;Stoyanov+'11&lt;/a&gt;
and many of Justin Domke's papers, to avoid issues I've mentioned
&lt;a href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/"&gt;before&lt;/a&gt;. You
can even backprop through optimization algorithms to get gradients of dev loss wrt
hyperparameters, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf"&gt;Domke'12&lt;/a&gt; and
&lt;a href="https://arxiv.org/abs/1502.03492"&gt;Maclaurin+'15&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's at least one catch!&lt;/strong&gt; Although the &lt;em&gt;time&lt;/em&gt; complexity of computing the
gradient is as good as the function, the &lt;em&gt;space&lt;/em&gt; complexity may be much larger
because the autodiff recipe (at least the default reverse-mode one) requires memoizing
all intermediate quantities (e.g., the quantities you overwrite in a
loop). There are generic methods for balancing the time-space tradeoff in
autodiff, since you can (at least in theory) reconstruct the intermediate
quantities by playing the forward computation again from intermediate
checkpoints (at a cost to runtime, of course). A recent example is
&lt;a href="https://arxiv.org/abs/1606.03401"&gt;Gruslys+'16&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A final remark&lt;/strong&gt;. Despite the name "automatic" differentiation, there is no
need to rely on software to "automatically" give you gradient routines. Applying
the backprop transformation is generally easy to do manually and sometimes more
efficient than using a library. Many autodiff libraries lack good support for
dynamic computation graph, i.e., when the structure depends on quantities that
vary with the input (e.g., sentence length).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="calculus"></category><category term="automatic-differentiation"></category><category term="rant"></category></entry><entry><title>Fast sigmoid sampling</title><link href="http://timvieira.github.io/blog/post/2016/07/04/fast-sigmoid-sampling/" rel="alternate"></link><published>2016-07-04T00:00:00-04:00</published><updated>2016-07-04T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-07-04:/blog/post/2016/07/04/fast-sigmoid-sampling/</id><summary type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;
&lt;style type="text/css"&gt;
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
&lt;/style&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we describe a simple trick for efficiently sampling a Bernoulli random variable $Y$ from a sigmoid-defined distribution, $p(Y = 1) = (1 + \exp(-x))^{-1}$, where $x \in \mathbb{R}$ is the only parameter of the distribution ($x$ is often defined as the dot product of features and weights).&lt;/p&gt;
&lt;p&gt;The "slow" method for sampling from a sigmoid,&lt;/p&gt;
&lt;p&gt;$$
u \sim \textrm{Uniform}(0,1)
$$
$$
Y = \textrm{sigmoid}(x) &gt; u
$$&lt;/p&gt;
&lt;p&gt;This method is slow because it calls the sigmoid function for every value of $x$. It is slow because $\exp$ is 2-3x slower than basic arithmetic operations.&lt;/p&gt;
&lt;p&gt;In this post, I'll describe a simple trick, which is well-suited to vectorized computations (e.g., numpy, matlab). The way it works is by &lt;em&gt;precomputing&lt;/em&gt; the expensive stuff (i.e., calls to expensive functions like $\exp$).&lt;/p&gt;
&lt;p&gt;$$
\textrm{sigmoid}(x) &gt; u \Leftrightarrow \textrm{logit}(\textrm{sigmoid}(x)) &gt; \textrm{logit}(u) \Leftrightarrow x &gt; \textrm{logit}(u).
$$&lt;/p&gt;
&lt;p&gt;Some details worth mentioning: (a) &lt;a href="https://en.wikipedia.org/wiki/Logit"&gt;logit&lt;/a&gt; is the inverse of sigmoid—sometimes it's called &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html"&gt;expit&lt;/a&gt; to highlight this connection—and (b) logit is strictly monotonic increasing you can apply it both sides of an inequality and preserve the ordering (there's a plot in the appendix).&lt;/p&gt;
&lt;p&gt;The "fast" method derives it's advantage by leveraging the fact that expensive computation can be done independently of the data (i.e., specific values of $x$). The fast method is also interesting as just cute math. In the bonus section of this post, we'll make a connection to the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel-max trick&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How fast is it in practice?&lt;/strong&gt; Below, we run a quick experiment to test that the method is correct and how fast it is.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.special&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;expit&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.timer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;    &lt;span class="c1"&gt;# https://github.com/timvieira/arsenal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# These are the sigmoid parameters we're going to sample from.&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# number of runs to average over.&lt;/span&gt;
&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="c1"&gt;# Used for plotting average p(Y=1)&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Temporary array for saving on memory allocation, cf. method slow-2.&lt;/span&gt;
&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# Let's use the same random variables for all methods. This allows &lt;/span&gt;
    &lt;span class="c1"&gt;# for a lower variance comparsion and equivalence testing.&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# used in fast method: precompute expensive stuff.&lt;/span&gt;

    &lt;span class="c1"&gt;# Requires computing sigmoid for each x.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'slow1'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;           
        
    &lt;span class="c1"&gt;# Avoid memory allocation in slow-1 by using the out option to sigmoid&lt;/span&gt;
    &lt;span class="c1"&gt;# function. It's a little bit faster than slow-1.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'slow2'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;           
        &lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;

    &lt;span class="c1"&gt;# Rolling our sigmoid is a bit slower than using the library function.&lt;/span&gt;
    &lt;span class="c1"&gt;# Not to mention this implementation isn't as numerically stable.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'slow3'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;
        
    &lt;span class="c1"&gt;# The fast method.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'fast'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;
    
    &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;    
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;fast is 26.8226x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.000353234 fast: 1.31693e-05)&lt;/span&gt;
slow2 is 1.0093x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.000353234 slow2: 0.000349975)&lt;/span&gt;
slow1 is 1.0920x faster than slow3 &lt;span class="ansi-yellow-fg"&gt;(avg: slow3: 0.000385725 slow1: 0.000353234)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;It looks like our trick is about $28$x faster than the fastest competing slow method!&lt;/p&gt;
&lt;p&gt;We also see that the assert statements passed, which means that the methods tested produce precisely the same samples.&lt;/p&gt;
&lt;p&gt;The final plot demonstrates that we get the right expected value (red curve) as we sweep the distributions parameter (x-axis).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Bonus"&gt;Bonus&lt;a class="anchor-link" href="#Bonus"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We could alternatively use the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel-max trick&lt;/a&gt; to derive a similar algorithm. If we ground out the trick for a sigmoid instead of a general mutlinomal distributions, we end up with&lt;/p&gt;
&lt;p&gt;$$
Z_0 \sim \textrm{Gumbel}(0,1)
$$
$$
Z_1 \sim \textrm{Gumbel}(0,1)
$$
$$
Y = x &gt; Z_0 - Z_1
$$&lt;/p&gt;
&lt;p&gt;Much like our new trick, this one benefits from the fact that all expensive stuff is done independent of the data (i.e., the value of $x$). However, it seems silly that we "need" to generate &lt;em&gt;two&lt;/em&gt; Gumbel RVs to get one sample from the sigmoid. With a little bit of Googling, we discover that the difference of $\textrm{Gumbel}(0,1)$ RVs is a &lt;a href="https://en.wikipedia.org/wiki/Logistic_distribution"&gt;logistic&lt;/a&gt; RV (specifically $\textrm{Logistic}(0,1)$).&lt;/p&gt;
&lt;p&gt;It turns out that $\textrm{logit}(\textrm{Uniform}(0,1))$ is a $\textrm{Logistic}(0,1)$ RV.&lt;/p&gt;
&lt;p&gt;Voila! Our fast sampling trick and the Gumbel-max trick are connected!&lt;/p&gt;
&lt;h2 id="Related-tricks"&gt;Related tricks&lt;a class="anchor-link" href="#Related-tricks"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Another trick is Justin Domke's &lt;a href="https://justindomke.wordpress.com/2014/01/08/reducing-sigmoid-computations-by-at-least-88-0797077977882/"&gt;trick&lt;/a&gt; to reduce calls to $\exp$ by $\approx 88\%$. The &lt;em&gt;disadvantage&lt;/em&gt; of this approach is that it's harder to implement with vectorization. The &lt;em&gt;advantage&lt;/em&gt; is that we don't need to precompute any expensive things.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Appendix"&gt;Appendix&lt;a class="anchor-link" href="#Appendix"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logit-plot"&gt;Logit plot&lt;a class="anchor-link" href="#Logit-plot"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logistic-random-variable"&gt;Logistic random variable&lt;a class="anchor-link" href="#Logistic-random-variable"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Check that our sampling method is equivalent to sampling from a logistic distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;
&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VfWd//HXJyEBZFH2LUAAUUBkM4AIgigqi4Kd0Q62
tbWtUtuqtU6t/Lqo09pRu02XwTJoW6ujQ7WiRQFRUEDZE7bIvkPCvqNAIMn398c5CdeYkJvk5p67
vJ+PRx733LPc+85J8sm53/M932POOUREJHmkBB1ARESiS4VfRCTJqPCLiCQZFX4RkSSjwi8ikmRU
+EVEkowKv4hIklHhFxFJMir8IiJJpk7QAcrTvHlzl5mZGXQMEZG4kZOTc8g51yKcdWOy8GdmZpKd
nR10DBGRuGFmO8NdV009IiJJRoVfRCTJqPCLiCQZFX4RkSSjwi8ikmRU+EVEkkxYhd/MRprZRjPb
YmYTy1n+ZTNbY2a5ZrbIzHqHLNvhz19lZuqjKSISsEr78ZtZKjAJuBHIA5ab2XTn3LqQ1bYDw5xz
R81sFDAFGBiyfLhz7lAEc4uISDWFc8Q/ANjinNvmnDsLTAXGha7gnFvknDvqP10CZEQ2poiIREo4
hb8dsDvkeZ4/ryLfBGaFPHfAHDPLMbMJVY8oEl8yJ84o/SqVkwPDh8Nll3lf48fDnj3BhZSkFtEh
G8xsOF7hHxIye4hzLt/MWgLvmdkG59yCcradAEwA6NChQyRjiQSnqAj+8z/hZz+DwsLz8zdvhnff
hcmT4YtfDC6fJKVwjvjzgfYhzzP8eZ9hZr2A54FxzrnDJfOdc/n+4wHgDbymo89xzk1xzmU557Ja
tAhrnCGR2PfTn8Jjj3lF/8EHYf16WLkSRo6Eo0fh3/4N3nwz6JSSZMIp/MuBrmbWyczSgfHA9NAV
zKwDMA24yzm3KWR+AzNrVDIN3AR8HKnwIrHs2u0r4KmnKLIUvn774/D730O3btCnD8ycCU884a34
jW/Arl2BZpXkUmnhd84VAvcDs4H1wKvOubVmdp+Z3eev9hjQDHi2TLfNVsBHZrYaWAbMcM69E/Hv
QiTGtPjkKL99+7cA/NeQL/FBl/6fXcHM+zQwZox35H/nnXDuXABJJRmF1cbvnJsJzCwzb3LI9D3A
PeVstw3oXXa+SKL70Qd/psWpYyzs2Itnr76j/JVSUuCFF7xPAIsWee39DzwQ1ZySnHTlrkiEdTm0
m3Hr5nM2pQ4/HPUQxSmpFa/cvDn88Y/e9FNPwenT0QkpSU2FXyTCHlr4Cik4/t77JvIvbln5Brfd
Bn37wt69MGVK7QeUpKfCLxJJa9cyZsNHFKTWYdLVYXbTNDt/ovfpp+HUqVqLJwIq/CIRUXLB1vQ7
vkMKjqm9b2Zf4+bhv8Ctt8JVV8G+ffDcc7UXVAQVfpGIafbpMUZuXESRpTB54O1V29gMfvxjb3ry
ZHAu8gFFfDF5s3WRePSFte+TXlzIe5cOYG/jC1+EGDqcw46nx3gTt9wCrVvDhg1eL5/Bg2szriQx
FX6RSHCO8avfBeDvvW4ud5XPjN1Twfwdd9/ttfM//7wKv9QaNfWIREBW/jouPZLH/oZN+aBLVvVf
6Bvf8B5ffRWOH49MOJEyVPhFIqDkaP8fPW+g6EL99ivTtStcd53Xs2fq1MiEEylDTT0i1VTSRFP/
7BlyNn4EwKu9bqz5C99zD8ybB3/7G3zrWzV/PZEydMQvUkPDtudw0bkCVra5nJ1N2tb8BceNg/r1
YfFiyMur+euJlKHCL1JDozcuBGDm5RE6GduwIYwa5U1PmxaZ1xQJocIvUgN1C89y/dblAMy6/JrI
vfDt/nUAr78eudcU8anwi9TAtdtX0vDsaXJbdSHvktaRe+ExYyA9HT780LuaVySCdHJXpAZG+Sd1
Z0WqmadE48Zw883w1lvwxhtk7jx/O9LSC75EqklH/CLVlFZ0jhu3LANqofDD+eaef/wj8q8tSU2F
X6SaBuxeS+OCT9nYvAPbm7aL/BuMHQt16sD8+TQ+80nkX1+Slgq/SDVdt827w+jnbqsYKZdcAtdc
A0VFDN6xqnbeQ5KSCr9INV23LQeADzrXYIiGyoweDcDwbdmVrCgSPhV+kerYsYOuh3dzIv0ictp1
r7338fvzX7ctR0M1S8So8ItUx6xZAHyU2YfC1FrsHHflldCuHS0/PcoVB7bV3vtIUlHhF6kOv/DP
q81mHvBu0OIf9Q/zm5ZEakqFX6SqzpyBuXMBmN+5X+2/n1/4h29VO79Ehgq/SFV9+CGcOsW6lp3Y
36gK99WtrhEjOJeSSr89G9StUyJChV+kqubMAWBBZt/ovF/jxqxo151UV8ygXWui856S0FT4RarK
b+ZZ1LF31N5yUYdeAAzaqcIvNafCL1IVR47AihWQlsbyjCui9rYLM71/MoN3ro7ae0ri0iBtIlUx
b57Xn37QIE6n16vVtwq9CXtam8v4NK0eXQ/vhj17oG0EbvgiSUtH/CJV8f773uMNN0T1bc+lprGs
/RWfzSBSTSr8IlXht+9Hu/ADLCw5p1CSQaSaVPhFwpWfDxs2eLdGHDAg6m+/qGMfb+L99zV8g9SI
Cr9IuEqaWIYOhbS0qL/9+paZHKnfGHbtYth9z3/mHIBIVYRV+M1spJltNLMtZjaxnOVfNrM1ZpZr
ZovMrHe424rEjQ8+8B6HDw/k7Z2lsLjDlYC6dUrNVFr4zSwVmASMAnoAd5pZjzKrbQeGOeeuBH4O
TKnCtiLxYcEC7/G66wKLsLR9TwAG5K0NLIPEv3CO+AcAW5xz25xzZ4GpwLjQFZxzi5xzR/2nS4CM
cLcViQv5+bB1KzRqBH36BBZjqX/EP3DXx2rnl2oLpx9/O2B3yPM8YOAF1v8mMKua24rEpvnzvcfB
g73bIdaSytrtNzXvwNF6jWh38iAZx/fXWg5JbBH9DTaz4XiFf0g1tp0ATADo0KFDJGOJ1JzfzPPM
6Vb8KcCTqs5SWN7+Cm7avISBu9XcI9UTTlNPPtA+5HmGP+8zzKwX8Dwwzjl3uCrbAjjnpjjnspxz
WS1atAgnu0j0+Ef8JW3sQVrqDxUxcHduwEkkXoVT+JcDXc2sk5mlA+OB6aErmFkHYBpwl3NuU1W2
FYl5Bw54/fcvuojc1pcGnaa0nX+Ajvilmipt6nHOFZrZ/cBsIBX4i3NurZnd5y+fDDwGNAOeNTOA
Qv/ovdxta+l7EakdJb15Bg3iXGr0+++Xtb5lJ06m1yfz2F7vpHO7dkFHkjgTVhu/c24mMLPMvMkh
0/cA94S7rUhcKTmxO2wYfBpsFICilFSyM3owfFuOl+1LXwo6ksQZXbkrUpkPP/Qehw4NNkeIZSXn
GkqyiVSBCr/IhRw/DmvWeEM0BDA+T0WWldwL4KOPgg0icUmFX+RCFi/2LpTKyoL69YNOUyq3dVcK
UtPg44/h6NHKNxAJocIvciELF3qPQ6p8aUqtOlsnjdVtunpPFi0KNozEHRV+kQspaUqJscIPkJ3h
D3ul5h6pIhV+kYqcPQtLl3rT11wTbJZyLFc7v1ST7rkrUpGVK+H0aTY3a8+Nv14adJrPyWnX3ZtY
tgzOnIF6tXsPYEkcOuIXqYh/JL08IzZHEj9RryH07Ol9MsnJCTqOxBEVfpGK+IU/O0YLP3D+3IOa
e6QKVPhFyuNcaY+e7HZxUPh1IZdUgQq/SHk2b4aDB6F1a3Zd0jroNBUa8lEBAEfnLqDTo28FnEbi
hQq/SHlK+u8PHgzewIMxKa9xS/Y1bEqTMyfpfLjcEc9FPke9ekTKU1L4r7kGDgQb5YLMyGnXnTEb
F3JV/vrP3MFrx9NjAgwmsUxH/CLlCT3ij3E5/jmIrPx1ASeReKHCL1LW4cPejVfq1YO+fYNOU6ns
DK8//1X56wNOIvFChV+krMWLvcf+/SE9PdgsYVjXsjOn69Sly5F8mpw6HnQciQMq/CJlxVEzD0Bh
ap3SAduuyt8QcBqJByr8ImXFWeGH8xeZqZ1fwqHCLxLq7FlYvtybHjQo2CxVkO2P23NVntr5pXIq
/CKhVq70Bjzr1g2aNQs6TdhW+IW/177NpBeeCziNxDoVfpFQJTc1iaNmHvAGbNvUrAN1i85xxf6t
QceRGKfCLxLKL/yP7GlA5sQZn7kgKtbltOsGwFVq55dKqPCLlAgZmK2k6SSe5PgneNWzRyqjwi9S
YudO2LsXmjZla9OMoNNUWcmNWbLy13n/xEQqoMIv4nvwwUkAzGnSJaYHZqvI9iZtOVy/MS0+PUb7
4/uDjiMxTIO0ifiy/K6Q8djMA4AZK9p158YtS8nKW6cB26RCOuIX8ZWMdRPTd9yqhMbtkXCo8IsA
nDxJt4M7OJeSyprWlwadptpK2vlV+OVCVPhFAJYuJdUVs7ZVZ86k1Qs6TbXltrqUsyl1uPzgThoV
fBp0HIlRKvwiUNqNMyeW768bhoK0unzcugspOPqqW6dUQIVfBEJurB6nJ3ZDlNwcXs09UhEVfpGi
IliyBDjfRh7PcjJC+vOLlEOFXyQ3F06eZPfFrTjQKH4GZqtIyT+vPns2kVpcFHAaiUVhFX4zG2lm
G81si5lNLGd5NzNbbGYFZvaDMst2mFmuma0ys+xIBReJmARq5gE41KAJOy5pQ4NzZ+h+YHvQcSQG
VVr4zSwVmASMAnoAd5pZ2TNgR4AHgV9X8DLDnXN9nHNZNQkrUitKTuzGcf/9skqbe/LU3COfF84R
/wBgi3Num3PuLDAVGBe6gnPugHNuOaCBwCWuZE6cQd6MuUDiHPGDTvDKhYVT+NsBu0Oe5/nzwuWA
OWaWY2YTKlrJzCaYWbaZZR88eLAKLy9Sfa1OHiLjxAFOpF/EpuYdgo4TMdm6kEsuIBond4c45/rg
NRV918yGlreSc26Kcy7LOZfVokWLKMQSOT8+z8p23ShOSQ04TeRsad6e43Ub0PbkIdqeOBB0HIkx
4RT+fKB9yPMMf15YnHP5/uMB4A28piORmFDS5TGRmnkAnKWcH6ZZ7fxSRjiFfznQ1cw6mVk6MB6Y
Hs6Lm1kDM2tUMg3cBHxc3bAikdbfL4rZGVcEnCTySgab66/CL2VUOiyzc67QzO4HZgOpwF+cc2vN
7D5/+WQzaw1kA42BYjN7CK8HUHPgDfPGNq8DvOKce6d2vhWRKjpxgu4HtnMuJZWVbS8LOk3ELfcL
v474paywxuN3zs0EZpaZNzlkeh9eE1BZJ4DeNQkoUmuWLCHVFbOm9eVxPTBbRda0uYyCVG/ANo4e
hSZNgo4kMUJX7kry+ugj4PyRcaIpqJPOmtaXkYKDxYuDjiMxRIVfkpdf+OP5xiuVKfneJv38BTIn
zvjMXbkkeanwS3I6e7Z0YLZELvzLS0/wrg04icQS3XNXkk7mxBn02bORN0+fZkvTDI5cdHHQkWpN
yT+13ns3UbfwLAV10gNOJLFAR/ySlLL8I+BEbd8vcaJeQzY070jdokJ67tsSdByJESr8kpQG+F0c
E2lgtoqUHPUP3K1LaMSjwi9Jx1wx/Xd7R/xL2vcMOE3tW+Z/j2rnlxIq/JJ0uh7aRZMzJ9nTqDl5
F7cKOk6tW+ZflZyVt44U3ZhFUOGXJDTAP9pf1v4K8K4qT2j7Gjdn5yWtaXT2tG7MIoAKvyShkrbu
ZUnQzFNiWYb3vQ7creYeUeGXZONcaeFfmpFEhb+919wzIE8neEWFX5LNli20/PQohy66mK3Nyhte
KjEt9T/dDNi9FpwLOI0ETYVfksuCBYB/wjMJ2vdL7LqkNfsaNqXp6ROwXnflSnYq/JJcSgp/ErXv
A2B2/nv294EkLxV+SS7z5wOwvH3i3XilMqWFf968QHNI8FT4JXls3w47d3KsXkPWtewUdJqoW9z+
Sm9i3jy18yc5FX5JHh98AHgnOp0l36/+1mYZHGjQBPbvhw0bgo4jAUq+335JXn7hX9yhV8BBAmLG
kg7+Ub+/LyQ5qfBLcnCutG17ccckLfyE/NNT4U9qKvySHLZuhbw8aNaMTc07BJ0mMKVH/PPmQXFx
oFkkOCr8khxKjnCvuy4p2/dLbG/SFtq2hUOHYK2Gb0hWyfsXIMmlpPAPHx5sjqCZnd8Hau5JWir8
kvhC2veTvvCDCr+o8EsS2LAB9u6FVq2ge/eg0wTv+uu9x3nzoEjj8ycjFX5JfO+95z2OGJFU4/NU
qFMn6NwZjh2DFSuCTiMBUOGXxDdnDgA/ONqCzIkzAg4TI0aM8B79fSPJRYVfEtu5c6Xt+ws79g42
SywpKfwln4YkqdQJOoBIbcmcOIN+eeuZdvIkW5tmsLdxi6AjxY7rrwczChZ8SO9/f50zafXY8fSY
oFNJlOiIXxLatTtWAvBhZp+Ak8SYZs2gXz/qFhXSP29d0GkkylT4JaEN3rkKgIUq/J/nN/cM3rEq
4CASbWrqkYTVoOAUffdspMhSzg9VIKUnuAfvaMTLwJCdq4MNJFGnI35JWFfvziWtuIjVbbpysm6D
oOPEnOx23TlTJ52e+7fS7NNjQceRKAqr8JvZSDPbaGZbzGxiOcu7mdliMyswsx9UZVuR2nLdthwA
5ne6KuAksakgrW7pTdiHbld//mRSaeE3s1RgEjAK6AHcaWY9yqx2BHgQ+HU1thWJPOfOF/7OKvwV
mefvm2HbcwJOItEUzhH/AGCLc26bc+4sMBUYF7qCc+6Ac245cK6q24rUik2baH98P0fqN2ZN60uD
ThOzSj4NDd2+UsM3JJFwCn87YHfI8zx/XjjC3tbMJphZtpllHzx4MMyXF6nAO+8A8GFmX4pTUgMO
E7u2NW3Hrotb0fT0CcjRUX+yiJmTu865Kc65LOdcVosWutBGasgv/PM79ws4SIwzO98U5u8zSXzh
FP58oH3I8wx/Xjhqsq1I9Zw+XTpMw4JOKvyVKT35PWtWsEEkasIp/MuBrmbWyczSgfHA9DBfvybb
ilTP/Plw5gy5rbpwqEGToNPEvEUde3E2pQ4sWwaHDwcdR6Kg0sLvnCsE7gdmA+uBV51za83sPjO7
D8DMWptZHvAw8BMzyzOzxhVtW1vfjAgAb78NwLzOWQEHiQ+n0uuzrP0VUFzM9+5+SiOYJoGwrtx1
zs0EZpaZNzlkeh9eM05Y24rUGufgrbcAmHvpgIDDxI+5lw5gyM7VjNiyjH9eobuUJbqYObkrEhG5
ubBrF7Rqxeo2XYNOEzfmXDoQgGHbckgrKtsrWxKNCr8kFv9on1tuwZl+vcO1+5LWbGrWgcZnT9F/
t1pjE53+MiSxTPf7Dtx6a7A54tCcrl7T2IgtywJOIrVNhV8Sx759sGwZZ+qk031+YdBp4s6cLl5z
z4gtS71zJZKwVPglcczweqMs7Nib0+n1Ag4Tf1a1vYzD9RvT4fh+WKebsyQyFX5JHG++Cag3T3UV
p6Tyfhd/3/n7UhKTCr8khhMn4N13KcZ479Krg04Tt2ZfNsibeP31YINIrVLhl7iXOXEGD37lSTh7
luUZPTjYUFfrVteHnfrySXp9WLkStm0LOo7UEhV+SQgjNy4E4J3Lrwk4SXwrqJPOByVXPE+bFmwY
qTUq/BL36p07w3X+jUTeuUyFv6ZmXT7Ym1BzT8JS4Ze4N2zbCi46V8CqNpext7GG9K6peZ2vgnr1
YMkSyMsLOo7UAhV+iXuj/WaeWWrmiYhT6fVh5EjviZp7EpIKv8S3U6e8C46AmZcPCThMArnjDu/x
738PNofUChV+iW9vvUWDc2dY2eZydl/SOug0CaNHdjqn69SFRYtg+/ag40iEqfBLfHvlFQD+2WNY
wEESy6n0+rzX1RvC4Zd3P0HmxBkapz+BqPBL/Dp6FGbNoshSmNHt2qDTJJySf6a3rl8QcBKJNBV+
iV+vvw7nzrGoQy9dtFULFnTqx7F6Del+cAeXH9wRdByJIBV+iUuZE2ew6Od/BGC6mnlqxbnUtNIT
5mPXzQ84jUSSCr/EpbYnDnD1rlwKUtPOjy8jEVfS3HPb2nmkFBcFnEYiRYVf4tK/5s4lBce7Xa/m
RL2GQcdJWMvaX8Hui1vR7uRBrtm5Jug4EiEq/BJ/iou5/eO5ALza68aAwyQ2Zym8duUIAO7InRNw
GokUFX6JPwsW0PHYPvY0as7Cjr2DTpPwXu95A8UYN29eDMeOBR1HIkCFX+LPX/8KwD963kBxSmrA
YRJf/sUtWdixN/UKz8LUqUHHkQhQ4Zf4cuIE/OMfAPzDb4KQ2vdaL39f/+UvwQaRiFDhl/jy0ktw
6hRL2vdkV5M2QadJGrO7DuJ43QawfDlj7v69ruKNcyr8Ej+cg2efBeClvmMCDpNcCtLqlp7kvWvl
zIDTSE2p8Ev8WLAA1q2D1q159zLdVzfaXu47GoBx6+bT+MwnAaeRmlDhl/jhH+1z772cS00LNksS
2t60HQsy+1K/sEBdO+OcCr/Eh717vZuCpKbChAlBp0laL/Xzmti+snIGnR59S6N2xikVfokPzz4L
hYUwdixkZASdJmm936U/+Y1a0OnoXq7blhN0HKkmFX6JfZ9+er6Z5+GHg82S5IpSUnnhqlsBmLBM
t2WMVyr8Evv++lc4cgQGDoTBg4NOk/Sm9rmZk+n1GbQrlyv3bg46jlRDWIXfzEaa2UYz22JmE8tZ
bmb2B3/5GjPrF7Jsh5nlmtkqM8uOZHhJAkVF8NvfetOPPAJmweYRTtZtwCt9RgFw7/I3Ak4j1VFp
4TezVGASMAroAdxpZj3KrDYK6Op/TQD+VGb5cOdcH+dcVs0jS1KZNs2752vnznDbbUGnEd8LV93K
uZRURm/4iIzj+4OOI1UUzhH/AGCLc26bc+4sMBUYV2adccCLzrMEuMTMdFml1ExxMfziF970ww97
PXokJuxt3ILp3YdSxxXz7SWvBR1Hqiicwt8O2B3yPM+fF+46DphjZjlmpn54Er7p02H1amjbFr75
zaDTSBnPDvoixRh3rJkDu3YFHUeqIBond4c45/rgNQd918yGlreSmU0ws2wzyz548GAUYklMcw7+
4z+86YkToV69YPPI52xt1p7pPYaSXlwITz0VdBypgnAKfz7QPuR5hj8vrHWccyWPB4A38JqOPsc5
N8U5l+Wcy2rRokV46SVx/fOfsGoVtGkD994bdBqpwB8HjacYgz//WUf9cSScwr8c6GpmncwsHRgP
TC+zznTgq37vnquB4865vWbWwMwaAZhZA+Am4OMI5pdEVFQEjz0GwONXjCXzibkBB5KKbG3enre6
D4Vz585/QpOYV2nhd84VAvcDs4H1wKvOubVmdp+Z3eevNhPYBmwBngO+489vBXxkZquBZcAM59w7
Ef4eJNG8+CLk5pLXuCVTe98cdBqpxO+GfAnq1IEXXoCPdVwXD+qEs5JzbiZecQ+dNzlk2gHfLWe7
bYDujSfhO3UKfvITAH419C4K6qQHHEgqs71pO/jWt2DSJHj0UZihsXtiXViFXyRq/uu/YM8e6NeP
6T2Glc7WQGAx7vHHvU9qM2fC++/D9dcHnUguQEM2SOzIz4enn/amf/UrnOnXM15k/mYZv+z7Be/J
Qw95A+pJzNJflsSOhx+GTz7xrtDVEWPc+XPWOHZd3Apyc+G//zvoOHIBKvwSG959F159FS66CH73
u6DTSDUUpNXliRHf8p489pjXZCcxSYVfAlFyA4/MiTPgzBl44AFvwU9/Ch07BhtOqu39SwfAuHFw
8iR8//tBx5EKqPBL4CYP+zJs2sSWphkabz8BDG4zjlNpdb1PcNM0Zn8sUq8eCVS//PXcu/xNiiyF
R0Y/xMrH3gs6ktRQ/sUteXrY3fxszv/AfffBtdeCrsaPKTril8DUPVfAr2b+jlRXzJQB/8LKdt2C
jiQR8lK/MSzucCUcPAjf/a439pLEDBV+Cczjc6fQ5Ug+m5p18K7+lIThLIVHRn0PGjSA117z7qIm
MUOFXwIxdt08vrR6NgWpaXxv7A90hW4CyruktXc1L8D998PatcEGklIq/BJ9Gzfyn7O9gvCzG+5l
fcvOAQeSWvO1r3lfp0/DHXd412lI4FT4JbqOHoWxY2l49jRvd7uWl/17t0oCmzQJuneH9evhq1/1
7qwmgVKvHomewkIYPx42bWJ9i0weHfmAbp6e4ErGWOo8+Hu8v+dReOMNePxxMs9dXbrOjqfHBBUv
aanwS60qHVzNOXZ8Msu7QrdFC+7915/yad2Lgg0nUbOtWYbXr3/0aHjySf5lzPeZ1vOGoGMlLTX1
SFR8/6NX4NlnoW5dmDaNvItbBR1Jou2mm0qH4/jlzN9zw5alAQdKXir8Uuu+nv1Pvrfo/yAlBaZO
hSFDgo4kQbn/fvjRj6jjipn0z2e4eteaoBMlJTX1SMSFjp3/zeVv8tP3nwfgkZsf4LUlabBEY+sn
tSef5OWZK/jyqnf462v/Ad/oDyNGBJ0qqajwS+1wju8seY0fLngRgJ/c9B1e63VjwKEkSKEHBCk3
fpu0okK+mDsHbrnFG9Nn9OgA0yUXNfVIxKUWF/Gz9ybzwwUvUozxyKgH+d+++qOW84pTUnl01IO8
1Hc0FBTA2LHw3HNBx0oaKvwSWSdOMGXak3x15QwKUtN4YOwPea3XTUGnkhjkLIWf3vht+NGPoKgI
JkyAiRO9aalVKvwSOWvXQv/+3LB1OUfrNeLL459kRvdrg04lscwMfvELmDIFUlPhmWe8Jp9Dh4JO
ltBU+KXmnIM//xkGDCi9OOu2r/6G7Iwrgk4m8eLee2H2bGje3LvWo18/mD8/6FQJS4VfambfPu8e
uffcA6dOwVe+whfu+jU7m7QNOpnEmxtugBUrYOBA2L0bhg+HRx7xxvmRiFLhl+opLoY//Qm6dYPp
0+Hii+F//xdefJEzafWCTifxqn17+PBD7569KSnw61/DlVd6nwIkYtSdU6pu3jz4wQ8gJweADzpf
xfB5b3h/tCJVFNrN87wB7Fi40Psk+fHHcPPN3ifLZ56Byy6LesZEoyN+Cd+yZTBmjPcRPCeHfQ2b
8u1xE/n67U+o6EvkDRzoNf088wxcdBG8+Sb06AHf+hbs3Bl0urhmLgZviZaVleWys7ODjiHgnbid
Mwd+8xvGtDRTAAAI/0lEQVTv5BtAw4bw6KN0P9Kd0+lq1pHaETpq54Dvvsj3P3qZL+bOIdUVQ506
cNdd8NBD0KtXgCljh5nlOOeywlpXhV9KhH7k3vHI1fDyy/A//wPr1gHwSXp9Xuo7huf738bhBpcE
FVOSWJfDu5l7ZiG88krpuP6LO1zJ//UeyR9e/RnUS94DkaoUfrXxS6l6585w/dZsbl2/gIJfL6Nu
USEA+xo25cV+t/BKn5Ecq9844JSSzLY2aw9PvwSPPw5/+AOfTn6OQbtyGbQrF9o+D7ff7t3zYehQ
71OBlEt7Jtnt2AHvvQdvvcWqWbOpV3gWgGKMeZ2uYmrvm5h76QDOpaYFm1Mk1KWXwh/+wNWpQxi3
bj7jV8+m5/6t3rAPzz3HkfqN+aBLFu937s/CzN6s+v2dQSeOKWrqSSbOwebN/Pv3J9M/by0Dd+fS
6ejez6yyss3lvNV9KDO6DWZ/o+YBBRWpussO7uDW9R8yZsOHdD66p3R+MUZKv77ep4Brr2XA3E84
0KgZkFh3/1Ibv3gXU23Y4HWFW7MGVq70vo4e/ex6jRt7F87cfDMD1tQv/YMQiVvO0eVwHjduWcq1
O1aQlbeutNmyxJ5GzVnbqjM3fmmkd51Az57QpQukxe8n24gXfjMbCfweSAWed849XWa5+ctHA6eA
u51zK8LZtjwq/GE6dMgr7rt2ed3btm+Hbdtg0ybvysfytG7Nu40yWZbRg2Xte7K2VReKUlKjm1sk
iuqfPUPfPRsYuPtj+uVvoPfeTTQ+e+rzK6amQufO0LWr99ipE3ToAB07et1IGzSIfvgqiOjJXTNL
BSYBNwJ5wHIzm+6cWxey2iigq/81EPgTMDDMbaW6pkyBH/+4/GVpaV47aM+e0LMn96wuZG2rzuxt
1Fw3OJekcjq9Hosy+7Aosw8A5orpfCSfHvu30ePAdroe2snlh3bR7vgBUjZvhs2bP/8ic+fC9ddH
OXntCefk7gBgi3NuG4CZTQXGAaHFexzwovM+Piwxs0vMrA2QGca2SeMz3SUraFssexXjBdsgu3eH
gQN5+1gaexq3YNclrXny4bHeR9ZOncj8id/v/hTev2QRwVkKW5u1Z2uz9rzVY1jp/LrnCsg8tpeO
R/fS4dhe2p04SLsTB7mpwRnv6J+q/w2HrlPV+bWp0qYeM7sdGOmcu8d/fhcw0Dl3f8g6bwNPO+c+
8p/PBR7FK/wX3LY8NWnqqepOLP9y8Yp/MFVdR0QkXDUp/HHZj9/MJgAT/KefmNlGf7o5UK3Bue2Z
GuQJY9sLrFPtzAGLx9zKHB3KHAX2TI0ydwx3xXAKfz4QOhBLhj8vnHXSwtgWAOfcFGBK2flmlh3u
f7FYEY+ZIT5zK3N0KHN0RCtzOIO0LQe6mlknM0sHxgPTy6wzHfiqea4Gjjvn9oa5rYiIRFGlR/zO
uUIzux+Yjdcl8y/OubVmdp+/fDIwE68r5xa8U4lfv9C2tfKdiIhIWMJq43fOzcQr7qHzJodMO+C7
4W5bRZ9r/okD8ZgZ4jO3MkeHMkdHVDLH5JW7IiJSe3QjFhGRJBMThd/M7jCztWZWbGZZZZb9PzPb
YmYbzezmCrZvambvmdlm/7FJdJKXvv/fzWyV/7XDzFZVsN4OM8v11wt8TAoze8LM8kOyj65gvZH+
/t9iZhOjnbNMll+Z2QYzW2Nmb5hZuTcGiIV9Xdl+8ztD/MFfvsbM+gWRMyRPezP7wMzW+X+P3ytn
nevM7HjI78xjQWQtk+mCP+sY3M+Xh+y/VWZ2wsweKrNO7e5n51zgX0B34HJgHpAVMr8HsBqoC3QC
tgKp5Wz/S2CiPz0ReCbA7+U3wGMVLNsBNA96f4fkeQL4QSXrpPr7vTOQ7v88egSY+Sagjj/9TEU/
66D3dTj7Da9DxCzAgKuBpQH/PrQB+vnTjYBN5WS+Dng7yJxV/VnH2n4u5/dkH9Axmvs5Jo74nXPr
nXMby1k0DpjqnCtwzm3H6zU0oIL1/uZP/w24rXaSXpg/WN0Xgf8L4v1rSemQHc65s0DJsBuBcM69
65wrGWpxCd61IbEonP1WOtSJc24JUDLUSSCcc3udP7iic+4ksB5oF1SeCIqp/VzGDcBW51xUbyIc
E4X/AtoBocNM5lH+L2Ir5103AN5/z1a1HawC1wL7nXPljPIEgAPmmFmOf6VyLHjA//j7lwqayML9
GQThG3hHcuUJel+Hs99idt+aWSbQF1hazuJr/N+ZWWZ2RVSDla+yn3XM7me8a5sqOlCstf0ctSEb
zGwO0LqcRT92zv0zUu/jnHNmFvGuSmHmv5MLH+0Pcc7lm1lL4D0z2+CcWxDprKEulBtvFNWf4/3h
/ByvmeobtZknHOHsazP7MVAIvFzBy0R9XycKM2sIvA485Jw7UWbxCqCDc+4T/5zQmwQ/BGBc/qz9
i1rHAv+vnMW1up+jVvidcyOqsVk4w0UA7DezNs65vf5HuAPVyXghleU3szrAvwBXXeA18v3HA2b2
Bl5zQK3+goa7383sOeDtchaF+zOImDD29d3ALcANzm8QLec1or6vy6jJUCeBMbM0vKL/snNuWtnl
of8InHMzzexZM2vunAtsTJwwftYxt599o4AVzrn9ZRfU9n6O9aae6cB4M6trZp3w/uMtq2C9r/nT
XwMi9gmiCkYAG5xzeeUtNLMGZtaoZBrvJOXHUcxXXqbQds4vUH6emBp2w7wb+/wQGOucK+duGjGz
r2sy1Ekg/HNUfwbWO+d+W8E6rf31MLMBeDXkcPRSfi5POD/rmNrPISpsIaj1/Rz0WW3/gO0LeO1u
BcB+YHbIsh/j9Y7YCIwKmf88fg8goBkwF9gMzAGaBvA9vADcV2ZeW2CmP90Zr2fHamAtXrNF0Pv9
JSAXWIP3x9GmbG7/+Wi8Hh5bg86Nd4J/N7DK/5ocq/u6vP0G3Ffye4LXy2SSvzyXkB5tAeUdgtfs
tyZk/44uk/l+f5+uxju5fk3Amcv9WcfyfvYzNcAr5BeHzIvaftaVuyIiSSbWm3pERCTCVPhFRJKM
Cr+ISJJR4RcRSTIq/CIiSUaFX0Qkyajwi4gkGRV+EZEk8/8BqAuC7RfrrmsAAAAASUVORK5CYII=
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;
&lt;style type="text/css"&gt;
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
&lt;/style&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we describe a simple trick for efficiently sampling a Bernoulli random variable $Y$ from a sigmoid-defined distribution, $p(Y = 1) = (1 + \exp(-x))^{-1}$, where $x \in \mathbb{R}$ is the only parameter of the distribution ($x$ is often defined as the dot product of features and weights).&lt;/p&gt;
&lt;p&gt;The "slow" method for sampling from a sigmoid,&lt;/p&gt;
&lt;p&gt;$$
u \sim \textrm{Uniform}(0,1)
$$
$$
Y = \textrm{sigmoid}(x) &gt; u
$$&lt;/p&gt;
&lt;p&gt;This method is slow because it calls the sigmoid function for every value of $x$. It is slow because $\exp$ is 2-3x slower than basic arithmetic operations.&lt;/p&gt;
&lt;p&gt;In this post, I'll describe a simple trick, which is well-suited to vectorized computations (e.g., numpy, matlab). The way it works is by &lt;em&gt;precomputing&lt;/em&gt; the expensive stuff (i.e., calls to expensive functions like $\exp$).&lt;/p&gt;
&lt;p&gt;$$
\textrm{sigmoid}(x) &gt; u \Leftrightarrow \textrm{logit}(\textrm{sigmoid}(x)) &gt; \textrm{logit}(u) \Leftrightarrow x &gt; \textrm{logit}(u).
$$&lt;/p&gt;
&lt;p&gt;Some details worth mentioning: (a) &lt;a href="https://en.wikipedia.org/wiki/Logit"&gt;logit&lt;/a&gt; is the inverse of sigmoid—sometimes it's called &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html"&gt;expit&lt;/a&gt; to highlight this connection—and (b) logit is strictly monotonic increasing you can apply it both sides of an inequality and preserve the ordering (there's a plot in the appendix).&lt;/p&gt;
&lt;p&gt;The "fast" method derives it's advantage by leveraging the fact that expensive computation can be done independently of the data (i.e., specific values of $x$). The fast method is also interesting as just cute math. In the bonus section of this post, we'll make a connection to the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel-max trick&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How fast is it in practice?&lt;/strong&gt; Below, we run a quick experiment to test that the method is correct and how fast it is.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.special&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;expit&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.timer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;    &lt;span class="c1"&gt;# https://github.com/timvieira/arsenal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# These are the sigmoid parameters we're going to sample from.&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# number of runs to average over.&lt;/span&gt;
&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="c1"&gt;# Used for plotting average p(Y=1)&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Temporary array for saving on memory allocation, cf. method slow-2.&lt;/span&gt;
&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# Let's use the same random variables for all methods. This allows &lt;/span&gt;
    &lt;span class="c1"&gt;# for a lower variance comparsion and equivalence testing.&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# used in fast method: precompute expensive stuff.&lt;/span&gt;

    &lt;span class="c1"&gt;# Requires computing sigmoid for each x.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'slow1'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;           
        
    &lt;span class="c1"&gt;# Avoid memory allocation in slow-1 by using the out option to sigmoid&lt;/span&gt;
    &lt;span class="c1"&gt;# function. It's a little bit faster than slow-1.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'slow2'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;           
        &lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;

    &lt;span class="c1"&gt;# Rolling our sigmoid is a bit slower than using the library function.&lt;/span&gt;
    &lt;span class="c1"&gt;# Not to mention this implementation isn't as numerically stable.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'slow3'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;
        
    &lt;span class="c1"&gt;# The fast method.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'fast'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;&gt;&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;
    
    &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;    
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;fast is 26.8226x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.000353234 fast: 1.31693e-05)&lt;/span&gt;
slow2 is 1.0093x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.000353234 slow2: 0.000349975)&lt;/span&gt;
slow1 is 1.0920x faster than slow3 &lt;span class="ansi-yellow-fg"&gt;(avg: slow3: 0.000385725 slow1: 0.000353234)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;It looks like our trick is about $28$x faster than the fastest competing slow method!&lt;/p&gt;
&lt;p&gt;We also see that the assert statements passed, which means that the methods tested produce precisely the same samples.&lt;/p&gt;
&lt;p&gt;The final plot demonstrates that we get the right expected value (red curve) as we sweep the distributions parameter (x-axis).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Bonus"&gt;Bonus&lt;a class="anchor-link" href="#Bonus"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We could alternatively use the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel-max trick&lt;/a&gt; to derive a similar algorithm. If we ground out the trick for a sigmoid instead of a general mutlinomal distributions, we end up with&lt;/p&gt;
&lt;p&gt;$$
Z_0 \sim \textrm{Gumbel}(0,1)
$$
$$
Z_1 \sim \textrm{Gumbel}(0,1)
$$
$$
Y = x &gt; Z_0 - Z_1
$$&lt;/p&gt;
&lt;p&gt;Much like our new trick, this one benefits from the fact that all expensive stuff is done independent of the data (i.e., the value of $x$). However, it seems silly that we "need" to generate &lt;em&gt;two&lt;/em&gt; Gumbel RVs to get one sample from the sigmoid. With a little bit of Googling, we discover that the difference of $\textrm{Gumbel}(0,1)$ RVs is a &lt;a href="https://en.wikipedia.org/wiki/Logistic_distribution"&gt;logistic&lt;/a&gt; RV (specifically $\textrm{Logistic}(0,1)$).&lt;/p&gt;
&lt;p&gt;It turns out that $\textrm{logit}(\textrm{Uniform}(0,1))$ is a $\textrm{Logistic}(0,1)$ RV.&lt;/p&gt;
&lt;p&gt;Voila! Our fast sampling trick and the Gumbel-max trick are connected!&lt;/p&gt;
&lt;h2 id="Related-tricks"&gt;Related tricks&lt;a class="anchor-link" href="#Related-tricks"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Another trick is Justin Domke's &lt;a href="https://justindomke.wordpress.com/2014/01/08/reducing-sigmoid-computations-by-at-least-88-0797077977882/"&gt;trick&lt;/a&gt; to reduce calls to $\exp$ by $\approx 88\%$. The &lt;em&gt;disadvantage&lt;/em&gt; of this approach is that it's harder to implement with vectorization. The &lt;em&gt;advantage&lt;/em&gt; is that we don't need to precompute any expensive things.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Appendix"&gt;Appendix&lt;a class="anchor-link" href="#Appendix"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logit-plot"&gt;Logit plot&lt;a class="anchor-link" href="#Logit-plot"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logistic-random-variable"&gt;Logistic random variable&lt;a class="anchor-link" href="#Logistic-random-variable"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Check that our sampling method is equivalent to sampling from a logistic distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;
&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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=
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="sampling"></category><category term="Gumbel"></category></entry><entry><title>Sqrt-biased sampling</title><link href="http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/" rel="alternate"></link><published>2016-06-28T00:00:00-04:00</published><updated>2016-06-28T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-06-28:/blog/post/2016/06/28/sqrt-biased-sampling/</id><summary type="html">&lt;p&gt;The following post is about instance of "sampling in proportion to &lt;span class="math"&gt;\(p\)&lt;/span&gt; is not
optimal, but you probably think it is." It's surprising how few people seem to
know this trick. Myself included! It was brought to my attention recently by
&lt;a href="http://lowrank.net/nikos/"&gt;Nikos Karampatziakis&lt;/a&gt;. (Thanks, Nikos!)&lt;/p&gt;
&lt;p&gt;The paper credited for this trick is
&lt;a href="http://www.pnas.org/content/106/6/1716.full.pdf"&gt;Press (2008)&lt;/a&gt;. I'm borrowing
heavily from that paper as well as an email exchange from Nikos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Suppose you're an aspiring chef with a severe head injury affecting
  your long- and short- term memory trying to find a special recipe from a
  cookbook that you made one time but just can't remember exactly which recipe
  it was. So, based on the ingredients of each recipe, you come up with a prior
  probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; that recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the one you're looking for. In total, the
  cookbook has &lt;span class="math"&gt;\(n\)&lt;/span&gt; recipes and &lt;span class="math"&gt;\(\sum_{i=1}^n p_i = 1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A good strategy would be to sort recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; and cook the most promising
ones first. Unfortunately, you're not a great chef so there is some probability
that you'll mess-up the recipe. So, it's a good idea to try recipes multiple
times. Also, you have no short term memory...&lt;/p&gt;
&lt;p&gt;This suggests a &lt;em&gt;sampling with replacement&lt;/em&gt; strategy, where we sample a recipe
from the cookbook to try &lt;em&gt;independently&lt;/em&gt; of whether we've tried it before
(called a &lt;em&gt;memoryless&lt;/em&gt; strategy). Let's give this strategy the name
&lt;span class="math"&gt;\(\boldsymbol{q}.\)&lt;/span&gt; Note that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is a probability distribution over
the recipes in the cookbook, just like &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many recipes until we find the special one?&lt;/strong&gt; To start, suppose the
special recipe is &lt;span class="math"&gt;\(j.\)&lt;/span&gt; Then, the expected number of recipes we have to make
until we find &lt;span class="math"&gt;\(j\)&lt;/span&gt; under the strategy &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j} = 1/q_{j}.
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-series')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-series" style="display:none;" class="derivation"&gt;
&lt;strong&gt;Derivation&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We start with
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j},
$$&lt;/div&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(a = (1-q_j)\)&lt;/span&gt;, to clean up notation.
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty t \cdot a^{t-1}
$$&lt;/div&gt;
&lt;p&gt;Use the identity &lt;span class="math"&gt;\(\nabla_a [ a^t ] = t \cdot a^{t-1}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty \nabla_a[ a^{t} ].
$$&lt;/div&gt;
&lt;p&gt;Fish the gradient out of the sum and tweak summation index,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ \sum_{t=1}^\infty a^{t} \right]
= q_{j} \nabla_a\left[ -1 + \sum_{t=0}^\infty a^{t}\right]
$$&lt;/div&gt;
&lt;p&gt;Plugin in the solution to the geometric series,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ -1 + \frac{1}{1-a} \right].
$$&lt;/div&gt;
&lt;p&gt;Take derivative, expand &lt;span class="math"&gt;\(a\)&lt;/span&gt; and simplify,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \frac{1}{(1-a)^2}
= \frac{1}{q_j}
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The equation says that expected time it takes to sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; for &lt;em&gt;the first time&lt;/em&gt;
is the probability we didn't sample for &lt;span class="math"&gt;\((t-1)\)&lt;/span&gt; steps times the probability we
sample it at time &lt;span class="math"&gt;\(t.\)&lt;/span&gt; We multiply this probability by the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; to get the
&lt;em&gt;expected&lt;/em&gt; time.&lt;/p&gt;
&lt;p&gt;Note that this equation assumes that we known &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the special recipe &lt;em&gt;with
certainty&lt;/em&gt; when we sample it. We'll revisit this assumption later when we
consider potential errors in executing the recipe.&lt;/p&gt;
&lt;p&gt;Since we don't known which &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the right one, we take an expectation over it
according to the prior distribution, which yields the following equation,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{q_i}.
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The first surprising thing&lt;/strong&gt;: Uniform is just as good as &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt;,
  yikes! &lt;span class="math"&gt;\(f(\boldsymbol{p}) = \sum_{i=1}^n \frac{p_i}{p_i} = n\)&lt;/span&gt; and
  &lt;span class="math"&gt;\(f(\text{uniform}(n)) = \sum_{i=1}^n \frac{p_i }{ 1/n } = n.\)&lt;/span&gt; (Assume, without
  loss of generality, that &lt;span class="math"&gt;\(p_i &amp;gt; 0\)&lt;/span&gt; since we can just drop these elements from
  &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the &lt;em&gt;optimal&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;?&lt;/strong&gt; We can address this question by
solving the following optimization (which will have a nice closed form
solution),&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \boldsymbol{q}^* = \underset{\boldsymbol{q}}{\operatorname{argmin}} \sum_{i=1}^n \frac{p_i}{q_i} \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \text{ s.t. } \sum_{i=1}^n q_i = 1 \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \ \ \ \ \, q_1 \ldots q_n \ge 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;The optimization problem says minimize the expected time to find the special
recipe. The constraints enforce that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; be a valid probability
distribution.&lt;/p&gt;
&lt;p&gt;The optimal strategy, which we get via Lagrange multipliers, turns out to be,
&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;p&gt;&lt;button onclick="toggle('#Lagrange')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="Lagrange" style="display:none;" class="derivation"&gt;
To solve this constrained optimization problem, we form the
Lagrangian,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\boldsymbol{q}, \lambda) = \sum_{i=1}^n \frac{p_i}{q_i} - \lambda\cdot \left(1 - \sum_{i=1}^n q_i\right),$$&lt;/div&gt;
&lt;p&gt;and solve for &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; and multiplier &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; such that partial
derivatives are all equal to zero. This gives us the following system of
nonlinear equations,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \lambda - \frac{p_i}{q_i^2} = 0 \ \ \ \text{for } 1 \le i \le n \\
&amp;amp;&amp;amp; \lambda \cdot \left(1 - \sum_{i=1}^n q_i \right) = 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We see that &lt;span class="math"&gt;\(q_i = \pm \sqrt{\frac{p_i}{\lambda}}\)&lt;/span&gt; works for the first set of
equations, but since we need &lt;span class="math"&gt;\(q_i \ge 0\)&lt;/span&gt;, we take the positive one. Solving for
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and plugging it in, we get a normalized distribution,&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How much better is &lt;span class="math"&gt;\(q^*\)&lt;/span&gt;?&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(q^*) = \sum_i \frac{p_i}{q^*_i}
= \sum_i \frac{p_i}{ \frac{\sqrt{p_i} }{ \sum_j \sqrt{p_j}} }
= \left( \sum_i \frac{p_i}{ \sqrt{p_i} } \right) \left( \sum_j \sqrt{p_j} \right)
= \left( \sum_i \sqrt{p_i} \right)^2
$$&lt;/div&gt;
&lt;p&gt;which sometimes equals &lt;span class="math"&gt;\(n\)&lt;/span&gt;, e.g., when &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; is uniform, but is never
bigger than &lt;span class="math"&gt;\(n.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the intuition?&lt;/strong&gt; The reason why the &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-scheme is preferred is
because we save on &lt;em&gt;additional&lt;/em&gt; cooking experiments. For example, if a recipe
has &lt;span class="math"&gt;\(k\)&lt;/span&gt; times higher prior probability than the average recipe, then we will try
that recipe &lt;span class="math"&gt;\(\sqrt{k}\)&lt;/span&gt; times more often; compared to &lt;span class="math"&gt;\(k\)&lt;/span&gt;, which we'd get under
&lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt; Additional cooking experiments are not so advantageous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Allowing for noise in the cooking process&lt;/strong&gt;: Suppose that for each recipe we
  had a prior belief about how hard that recipe is for us to cook. Denote that
  belief &lt;span class="math"&gt;\(s_i\)&lt;/span&gt;, these belief are between zero (never get it right) and one
  (perfect every time) and do not sum to one over the cookbook.&lt;/p&gt;
&lt;p&gt;Following a similar derivation to before, the time to cook the special recipe
&lt;span class="math"&gt;\(j\)&lt;/span&gt; and cook it correctly is,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - \color{red}{s_j} q_j)^{t-1} q_{j} \color{red}{s_j} = \frac{1}{s_j \cdot q_j}
$$&lt;/div&gt;
&lt;p&gt;
That gives rise to a modified objective,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{\color{red}{s_i} \cdot q_i}
$$&lt;/div&gt;
&lt;p&gt;This is exactly the same as the previous objective, except we've replaced &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;
with &lt;span class="math"&gt;\(p_i/s_i.\)&lt;/span&gt; Thus, we can reuse our previous derivation to get the optimal
strategy, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i / s_i}.\)&lt;/span&gt; If noise is constant, then we
recover the original solution, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension to finding multiple tasty recipes&lt;/strong&gt;: Suppose we're trying to find
  several tasty recipes, not just a single special one. Now, &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is our prior
  belief that we'll like the recipe at all. How do we minimize the time until we
  find a tasty one? It turns out the same trick works without modification
  because all derivations apply to each recipe independently. The same trick
  works if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; does not sums to one over &lt;span class="math"&gt;\(n.\)&lt;/span&gt; For example, if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the
  independent probability that you'll like recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; at all, not the
  probability that it's the special one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond memoryless policies&lt;/strong&gt;: Clearly, our choice of a memoryless policy can
  be beat by a policy family that balances exploration (trying new recipes) and
  exploitation (trying our best guess).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Overall, the problem we've posed is similar to a
    &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;multi-armed bandit&lt;/a&gt;. In
    our case, the arms are the recipes, pulling the arm is trying the recipe and
    the reward is whether or not we liked the recipe (possibly noisy). The key
    difference between our setup and multi-armed bandits is that we trust our
    prior distribution &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; and noise model &lt;span class="math"&gt;\(\boldsymbol{s}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the amount of noise &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; is known and we trust the prior &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; then
    there is an optimal deterministic (without-replacement) strategy that we can
    get by sorting the recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; accounting for the error rates
    &lt;span class="math"&gt;\(s_i.\)&lt;/span&gt; This approach is described in the original paper.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A more realistic application&lt;/strong&gt;: In certain language modeling applications, we
  avoid computing normalization constants (which require summing over a massive
  vocabulary) by using importance sampling, negative sampling or noise
  contrastive estimation techniques (e.g.,
  &lt;a href="https://arxiv.org/pdf/1511.06909.pdf"&gt;Ji+,16&lt;/a&gt;;
  &lt;a href="http://www.aclweb.org/anthology/Q15-1016"&gt;Levy+,15&lt;/a&gt;). These techniques depend
  on a proposal distribution, which folks often take to be the unigram
  distribution. Unfortunately, this gives too many samples of stop words (e.g.,
  "the", "an", "a"), so practitioners "anneal" the unigram distribution (to
  increase the entropy), that is sample from &lt;span class="math"&gt;\(q_i \propto
  p_{\text{unigram},i}^\alpha.\)&lt;/span&gt; Typically, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is set by grid search and
  (no surprise) &lt;span class="math"&gt;\(\alpha \approx 1/2\)&lt;/span&gt; tends to work best! The &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-sampling
  trick is possibly a reverse-engineered justification in favor of annealing as
  "the right thing to do" (e.g., why not do additive smoothing?) and it even
  tells us how to set the annealing parameter &lt;span class="math"&gt;\(\alpha.\)&lt;/span&gt; The key assumption is
  that we want to sample the actual word at a given position as often as
  possible while still being diverse thanks to the coverage of unigram
  prior. (Furthermore, memoryless sampling leads to simpler algorithms.)&lt;/p&gt;
&lt;!--
Actually, many word2vec papers use $\alpha=3/4$, which was suggested in
[Levy+,15](http://www.aclweb.org/anthology/Q15-1016), including the default
value in
[gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L462). So,
[Ryan Cotterell](https://ryancotterell.github.io/) ran a quick experiment with
gensim, which confirmed the suspicion that $1/2$ may be better than $3/4.$

    Word similarity accuracy (avg of 10 runs)
    | alpha | accuracy |
    +==================+
    |  0.00 |    0.354 |
    |  0.25 |    0.403 |
    |  0.50 |    0.414 |
    |  0.75 |    0.395 |
    |  1.00 |    0.345 |
--&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;The following post is about instance of "sampling in proportion to &lt;span class="math"&gt;\(p\)&lt;/span&gt; is not
optimal, but you probably think it is." It's surprising how few people seem to
know this trick. Myself included! It was brought to my attention recently by
&lt;a href="http://lowrank.net/nikos/"&gt;Nikos Karampatziakis&lt;/a&gt;. (Thanks, Nikos!)&lt;/p&gt;
&lt;p&gt;The paper credited for this trick is
&lt;a href="http://www.pnas.org/content/106/6/1716.full.pdf"&gt;Press (2008)&lt;/a&gt;. I'm borrowing
heavily from that paper as well as an email exchange from Nikos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Suppose you're an aspiring chef with a severe head injury affecting
  your long- and short- term memory trying to find a special recipe from a
  cookbook that you made one time but just can't remember exactly which recipe
  it was. So, based on the ingredients of each recipe, you come up with a prior
  probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; that recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the one you're looking for. In total, the
  cookbook has &lt;span class="math"&gt;\(n\)&lt;/span&gt; recipes and &lt;span class="math"&gt;\(\sum_{i=1}^n p_i = 1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A good strategy would be to sort recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; and cook the most promising
ones first. Unfortunately, you're not a great chef so there is some probability
that you'll mess-up the recipe. So, it's a good idea to try recipes multiple
times. Also, you have no short term memory...&lt;/p&gt;
&lt;p&gt;This suggests a &lt;em&gt;sampling with replacement&lt;/em&gt; strategy, where we sample a recipe
from the cookbook to try &lt;em&gt;independently&lt;/em&gt; of whether we've tried it before
(called a &lt;em&gt;memoryless&lt;/em&gt; strategy). Let's give this strategy the name
&lt;span class="math"&gt;\(\boldsymbol{q}.\)&lt;/span&gt; Note that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is a probability distribution over
the recipes in the cookbook, just like &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many recipes until we find the special one?&lt;/strong&gt; To start, suppose the
special recipe is &lt;span class="math"&gt;\(j.\)&lt;/span&gt; Then, the expected number of recipes we have to make
until we find &lt;span class="math"&gt;\(j\)&lt;/span&gt; under the strategy &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j} = 1/q_{j}.
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-series')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-series" style="display:none;" class="derivation"&gt;
&lt;strong&gt;Derivation&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We start with
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j},
$$&lt;/div&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(a = (1-q_j)\)&lt;/span&gt;, to clean up notation.
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty t \cdot a^{t-1}
$$&lt;/div&gt;
&lt;p&gt;Use the identity &lt;span class="math"&gt;\(\nabla_a [ a^t ] = t \cdot a^{t-1}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty \nabla_a[ a^{t} ].
$$&lt;/div&gt;
&lt;p&gt;Fish the gradient out of the sum and tweak summation index,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ \sum_{t=1}^\infty a^{t} \right]
= q_{j} \nabla_a\left[ -1 + \sum_{t=0}^\infty a^{t}\right]
$$&lt;/div&gt;
&lt;p&gt;Plugin in the solution to the geometric series,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ -1 + \frac{1}{1-a} \right].
$$&lt;/div&gt;
&lt;p&gt;Take derivative, expand &lt;span class="math"&gt;\(a\)&lt;/span&gt; and simplify,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \frac{1}{(1-a)^2}
= \frac{1}{q_j}
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The equation says that expected time it takes to sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; for &lt;em&gt;the first time&lt;/em&gt;
is the probability we didn't sample for &lt;span class="math"&gt;\((t-1)\)&lt;/span&gt; steps times the probability we
sample it at time &lt;span class="math"&gt;\(t.\)&lt;/span&gt; We multiply this probability by the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; to get the
&lt;em&gt;expected&lt;/em&gt; time.&lt;/p&gt;
&lt;p&gt;Note that this equation assumes that we known &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the special recipe &lt;em&gt;with
certainty&lt;/em&gt; when we sample it. We'll revisit this assumption later when we
consider potential errors in executing the recipe.&lt;/p&gt;
&lt;p&gt;Since we don't known which &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the right one, we take an expectation over it
according to the prior distribution, which yields the following equation,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{q_i}.
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The first surprising thing&lt;/strong&gt;: Uniform is just as good as &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt;,
  yikes! &lt;span class="math"&gt;\(f(\boldsymbol{p}) = \sum_{i=1}^n \frac{p_i}{p_i} = n\)&lt;/span&gt; and
  &lt;span class="math"&gt;\(f(\text{uniform}(n)) = \sum_{i=1}^n \frac{p_i }{ 1/n } = n.\)&lt;/span&gt; (Assume, without
  loss of generality, that &lt;span class="math"&gt;\(p_i &amp;gt; 0\)&lt;/span&gt; since we can just drop these elements from
  &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the &lt;em&gt;optimal&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;?&lt;/strong&gt; We can address this question by
solving the following optimization (which will have a nice closed form
solution),&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \boldsymbol{q}^* = \underset{\boldsymbol{q}}{\operatorname{argmin}} \sum_{i=1}^n \frac{p_i}{q_i} \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \text{ s.t. } \sum_{i=1}^n q_i = 1 \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \ \ \ \ \, q_1 \ldots q_n \ge 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;The optimization problem says minimize the expected time to find the special
recipe. The constraints enforce that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; be a valid probability
distribution.&lt;/p&gt;
&lt;p&gt;The optimal strategy, which we get via Lagrange multipliers, turns out to be,
&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;p&gt;&lt;button onclick="toggle('#Lagrange')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="Lagrange" style="display:none;" class="derivation"&gt;
To solve this constrained optimization problem, we form the
Lagrangian,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\boldsymbol{q}, \lambda) = \sum_{i=1}^n \frac{p_i}{q_i} - \lambda\cdot \left(1 - \sum_{i=1}^n q_i\right),$$&lt;/div&gt;
&lt;p&gt;and solve for &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; and multiplier &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; such that partial
derivatives are all equal to zero. This gives us the following system of
nonlinear equations,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \lambda - \frac{p_i}{q_i^2} = 0 \ \ \ \text{for } 1 \le i \le n \\
&amp;amp;&amp;amp; \lambda \cdot \left(1 - \sum_{i=1}^n q_i \right) = 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We see that &lt;span class="math"&gt;\(q_i = \pm \sqrt{\frac{p_i}{\lambda}}\)&lt;/span&gt; works for the first set of
equations, but since we need &lt;span class="math"&gt;\(q_i \ge 0\)&lt;/span&gt;, we take the positive one. Solving for
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and plugging it in, we get a normalized distribution,&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How much better is &lt;span class="math"&gt;\(q^*\)&lt;/span&gt;?&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(q^*) = \sum_i \frac{p_i}{q^*_i}
= \sum_i \frac{p_i}{ \frac{\sqrt{p_i} }{ \sum_j \sqrt{p_j}} }
= \left( \sum_i \frac{p_i}{ \sqrt{p_i} } \right) \left( \sum_j \sqrt{p_j} \right)
= \left( \sum_i \sqrt{p_i} \right)^2
$$&lt;/div&gt;
&lt;p&gt;which sometimes equals &lt;span class="math"&gt;\(n\)&lt;/span&gt;, e.g., when &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; is uniform, but is never
bigger than &lt;span class="math"&gt;\(n.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the intuition?&lt;/strong&gt; The reason why the &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-scheme is preferred is
because we save on &lt;em&gt;additional&lt;/em&gt; cooking experiments. For example, if a recipe
has &lt;span class="math"&gt;\(k\)&lt;/span&gt; times higher prior probability than the average recipe, then we will try
that recipe &lt;span class="math"&gt;\(\sqrt{k}\)&lt;/span&gt; times more often; compared to &lt;span class="math"&gt;\(k\)&lt;/span&gt;, which we'd get under
&lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt; Additional cooking experiments are not so advantageous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Allowing for noise in the cooking process&lt;/strong&gt;: Suppose that for each recipe we
  had a prior belief about how hard that recipe is for us to cook. Denote that
  belief &lt;span class="math"&gt;\(s_i\)&lt;/span&gt;, these belief are between zero (never get it right) and one
  (perfect every time) and do not sum to one over the cookbook.&lt;/p&gt;
&lt;p&gt;Following a similar derivation to before, the time to cook the special recipe
&lt;span class="math"&gt;\(j\)&lt;/span&gt; and cook it correctly is,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - \color{red}{s_j} q_j)^{t-1} q_{j} \color{red}{s_j} = \frac{1}{s_j \cdot q_j}
$$&lt;/div&gt;
&lt;p&gt;
That gives rise to a modified objective,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{\color{red}{s_i} \cdot q_i}
$$&lt;/div&gt;
&lt;p&gt;This is exactly the same as the previous objective, except we've replaced &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;
with &lt;span class="math"&gt;\(p_i/s_i.\)&lt;/span&gt; Thus, we can reuse our previous derivation to get the optimal
strategy, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i / s_i}.\)&lt;/span&gt; If noise is constant, then we
recover the original solution, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension to finding multiple tasty recipes&lt;/strong&gt;: Suppose we're trying to find
  several tasty recipes, not just a single special one. Now, &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is our prior
  belief that we'll like the recipe at all. How do we minimize the time until we
  find a tasty one? It turns out the same trick works without modification
  because all derivations apply to each recipe independently. The same trick
  works if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; does not sums to one over &lt;span class="math"&gt;\(n.\)&lt;/span&gt; For example, if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the
  independent probability that you'll like recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; at all, not the
  probability that it's the special one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond memoryless policies&lt;/strong&gt;: Clearly, our choice of a memoryless policy can
  be beat by a policy family that balances exploration (trying new recipes) and
  exploitation (trying our best guess).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Overall, the problem we've posed is similar to a
    &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;multi-armed bandit&lt;/a&gt;. In
    our case, the arms are the recipes, pulling the arm is trying the recipe and
    the reward is whether or not we liked the recipe (possibly noisy). The key
    difference between our setup and multi-armed bandits is that we trust our
    prior distribution &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; and noise model &lt;span class="math"&gt;\(\boldsymbol{s}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the amount of noise &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; is known and we trust the prior &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; then
    there is an optimal deterministic (without-replacement) strategy that we can
    get by sorting the recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; accounting for the error rates
    &lt;span class="math"&gt;\(s_i.\)&lt;/span&gt; This approach is described in the original paper.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A more realistic application&lt;/strong&gt;: In certain language modeling applications, we
  avoid computing normalization constants (which require summing over a massive
  vocabulary) by using importance sampling, negative sampling or noise
  contrastive estimation techniques (e.g.,
  &lt;a href="https://arxiv.org/pdf/1511.06909.pdf"&gt;Ji+,16&lt;/a&gt;;
  &lt;a href="http://www.aclweb.org/anthology/Q15-1016"&gt;Levy+,15&lt;/a&gt;). These techniques depend
  on a proposal distribution, which folks often take to be the unigram
  distribution. Unfortunately, this gives too many samples of stop words (e.g.,
  "the", "an", "a"), so practitioners "anneal" the unigram distribution (to
  increase the entropy), that is sample from &lt;span class="math"&gt;\(q_i \propto
  p_{\text{unigram},i}^\alpha.\)&lt;/span&gt; Typically, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is set by grid search and
  (no surprise) &lt;span class="math"&gt;\(\alpha \approx 1/2\)&lt;/span&gt; tends to work best! The &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-sampling
  trick is possibly a reverse-engineered justification in favor of annealing as
  "the right thing to do" (e.g., why not do additive smoothing?) and it even
  tells us how to set the annealing parameter &lt;span class="math"&gt;\(\alpha.\)&lt;/span&gt; The key assumption is
  that we want to sample the actual word at a given position as often as
  possible while still being diverse thanks to the coverage of unigram
  prior. (Furthermore, memoryless sampling leads to simpler algorithms.)&lt;/p&gt;
&lt;!--
Actually, many word2vec papers use $\alpha=3/4$, which was suggested in
[Levy+,15](http://www.aclweb.org/anthology/Q15-1016), including the default
value in
[gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L462). So,
[Ryan Cotterell](https://ryancotterell.github.io/) ran a quick experiment with
gensim, which confirmed the suspicion that $1/2$ may be better than $3/4.$

    Word similarity accuracy (avg of 10 runs)
    | alpha | accuracy |
    +==================+
    |  0.00 |    0.354 |
    |  0.25 |    0.403 |
    |  0.50 |    0.414 |
    |  0.75 |    0.395 |
    |  1.00 |    0.345 |
--&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="sampling"></category><category term="decision-making"></category></entry><entry><title>The optimal proposal distribution is not p</title><link href="http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/" rel="alternate"></link><published>2016-05-28T00:00:00-04:00</published><updated>2016-05-28T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-05-28:/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/</id><summary type="html">&lt;p&gt;The following is a quick rant about
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;
(see that post for notation).&lt;/p&gt;
&lt;p&gt;I've heard the following &lt;strong&gt;incorrect&lt;/strong&gt; statement one too many times,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We chose &lt;span class="math"&gt;\(q \approx p\)&lt;/span&gt; because &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is the "optimal" proposal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While it is certainly a good idea to pick &lt;span class="math"&gt;\(q\)&lt;/span&gt; to be as similar as possible to
&lt;span class="math"&gt;\(p\)&lt;/span&gt;, it is by no means &lt;em&gt;optimal&lt;/em&gt; because it is oblivious to &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;With importance sampling, it is possible to achieve a variance reduction over
Monte Carlo estimation. The optimal proposal distribution, assuming &lt;span class="math"&gt;\(f(x) \ge 0\)&lt;/span&gt;
for all &lt;span class="math"&gt;\(x\)&lt;/span&gt;, is &lt;span class="math"&gt;\(q(x) \propto p(x) f(x).\)&lt;/span&gt; This choice of &lt;span class="math"&gt;\(q\)&lt;/span&gt; gives us a &lt;em&gt;zero
variance&lt;/em&gt; estimate &lt;em&gt;with a single sample&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Of course, this is an unreasonable distribution to use because the normalizing
constant &lt;em&gt;is the thing you are trying to estimate&lt;/em&gt;, but it is proof that &lt;em&gt;better
proposal distributions exist&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The key to doing better than &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is to take &lt;span class="math"&gt;\(f\)&lt;/span&gt; into account. Look up
"importance sampling for variance reduction" to learn more.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;The following is a quick rant about
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;
(see that post for notation).&lt;/p&gt;
&lt;p&gt;I've heard the following &lt;strong&gt;incorrect&lt;/strong&gt; statement one too many times,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We chose &lt;span class="math"&gt;\(q \approx p\)&lt;/span&gt; because &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is the "optimal" proposal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While it is certainly a good idea to pick &lt;span class="math"&gt;\(q\)&lt;/span&gt; to be as similar as possible to
&lt;span class="math"&gt;\(p\)&lt;/span&gt;, it is by no means &lt;em&gt;optimal&lt;/em&gt; because it is oblivious to &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;With importance sampling, it is possible to achieve a variance reduction over
Monte Carlo estimation. The optimal proposal distribution, assuming &lt;span class="math"&gt;\(f(x) \ge 0\)&lt;/span&gt;
for all &lt;span class="math"&gt;\(x\)&lt;/span&gt;, is &lt;span class="math"&gt;\(q(x) \propto p(x) f(x).\)&lt;/span&gt; This choice of &lt;span class="math"&gt;\(q\)&lt;/span&gt; gives us a &lt;em&gt;zero
variance&lt;/em&gt; estimate &lt;em&gt;with a single sample&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Of course, this is an unreasonable distribution to use because the normalizing
constant &lt;em&gt;is the thing you are trying to estimate&lt;/em&gt;, but it is proof that &lt;em&gt;better
proposal distributions exist&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The key to doing better than &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is to take &lt;span class="math"&gt;\(f\)&lt;/span&gt; into account. Look up
"importance sampling for variance reduction" to learn more.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="statistics"></category><category term="sampling"></category><category term="importance-sampling"></category></entry><entry><title>Dimensional analysis of gradient ascent</title><link href="http://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/" rel="alternate"></link><published>2016-05-27T00:00:00-04:00</published><updated>2016-05-27T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-05-27:/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/</id><summary type="html">&lt;p&gt;In physical sciences, numbers are paired with units and called quantities. In
this augmented number system, dimensional analysis provides a crucial sanity
check, much like type checking in a programming language. There are simple rules
for building up units and constraints on what operations are allowed. For
example, you can't multiply quantities which are not conformable or add
quantities with different units. Also, we generally know the units of the input
and desired output, which allows us to check that our computations at least
produce the right units.&lt;/p&gt;
&lt;p&gt;In this post, we'll discuss the dimensional analysis of gradient ascent, which
will hopefully help us understand why the "step size" is parameter so finicky
and why it even exists.&lt;/p&gt;
&lt;p&gt;Gradient ascent is an iterative procedure for (locally) maximizing a function,
&lt;span class="math"&gt;\(f: \mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
x_{t+1} = x_t + \alpha \frac{\partial f(x_t)}{\partial x}
$$&lt;/div&gt;
&lt;p&gt;In general, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a &lt;span class="math"&gt;\(d \times d\)&lt;/span&gt; matrix, but often we constrain the matrix
to be simple, e.g., &lt;span class="math"&gt;\(a\cdot I\)&lt;/span&gt; for some scalar &lt;span class="math"&gt;\(a\)&lt;/span&gt; or &lt;span class="math"&gt;\(\text{diag}(a)\)&lt;/span&gt; for some
vector &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at the units of the change in &lt;span class="math"&gt;\(\Delta x=x_{t+1} - x_t\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\Delta x) = \left(\textbf{units }\alpha\cdot \frac{\partial f(x_t)}{\partial x}\right) = (\textbf{units }\alpha) \frac{(\textbf{units }f)}{(\textbf{units }x)}.
$$&lt;/div&gt;
&lt;p&gt;The units of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; must be &lt;span class="math"&gt;\((\textbf{units }x)\)&lt;/span&gt;. However, if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt;
is unit free, we're happy with &lt;span class="math"&gt;\((\textbf{units }x) / (\textbf{units }f)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Solving for the units of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; we get,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\alpha) = \frac{(\textbf{units }x)^2}{(\textbf{units }f)}.
$$&lt;/div&gt;
&lt;p&gt;This gives us an idea for what &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; should be.&lt;/p&gt;
&lt;p&gt;For example, the inverse Hessian passes the unit check (if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt; unit
free). The disadvantages of the Hessian is that it needs to be positive-definite
(or at least invertible) in order to be a valid "step size" (i.e., we need
step sizes to be &lt;span class="math"&gt;\(&amp;gt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Another method for handling step sizes is line search. However, line search
won't let us run online. Furthermore, line search would be too slow in the case
where we want a step size for each dimension.&lt;/p&gt;
&lt;p&gt;In machine learning, we've become fond of online methods, which adapt the step
size as they go. The general idea is to estimate a step size matrix that passes
the unit check (for each dimension of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). Furthermore, we want do as little
extra work as possible to get this estimate (e.g., we want to avoid computing a
Hessian because that would be extra work). So, the step size should be based
only iterates and gradients up to time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf"&gt;AdaGrad&lt;/a&gt; doesn't doesn't
  pass the unit check. This motivated AdaDelta.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1212.5701"&gt;AdaDelta&lt;/a&gt; uses the ratio of (running
  estimates of) the root-mean-squares of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\partial f / \partial
  x\)&lt;/span&gt;. The mean is taken using an exponentially weighted moving average. See
  paper for actual implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; came later and made some tweaks to
  remove (unintended) bias in the AdaDelta estimates of the numerator and
  denominator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, it's important/useful to analyze the units of numerical algorithms
in order to get a sanity check (i.e., catch mistakes) as well as to develop an
understanding of why certain parameters exist and how properties of a problem
affect the values we should use for them.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;In physical sciences, numbers are paired with units and called quantities. In
this augmented number system, dimensional analysis provides a crucial sanity
check, much like type checking in a programming language. There are simple rules
for building up units and constraints on what operations are allowed. For
example, you can't multiply quantities which are not conformable or add
quantities with different units. Also, we generally know the units of the input
and desired output, which allows us to check that our computations at least
produce the right units.&lt;/p&gt;
&lt;p&gt;In this post, we'll discuss the dimensional analysis of gradient ascent, which
will hopefully help us understand why the "step size" is parameter so finicky
and why it even exists.&lt;/p&gt;
&lt;p&gt;Gradient ascent is an iterative procedure for (locally) maximizing a function,
&lt;span class="math"&gt;\(f: \mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
x_{t+1} = x_t + \alpha \frac{\partial f(x_t)}{\partial x}
$$&lt;/div&gt;
&lt;p&gt;In general, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a &lt;span class="math"&gt;\(d \times d\)&lt;/span&gt; matrix, but often we constrain the matrix
to be simple, e.g., &lt;span class="math"&gt;\(a\cdot I\)&lt;/span&gt; for some scalar &lt;span class="math"&gt;\(a\)&lt;/span&gt; or &lt;span class="math"&gt;\(\text{diag}(a)\)&lt;/span&gt; for some
vector &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at the units of the change in &lt;span class="math"&gt;\(\Delta x=x_{t+1} - x_t\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\Delta x) = \left(\textbf{units }\alpha\cdot \frac{\partial f(x_t)}{\partial x}\right) = (\textbf{units }\alpha) \frac{(\textbf{units }f)}{(\textbf{units }x)}.
$$&lt;/div&gt;
&lt;p&gt;The units of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; must be &lt;span class="math"&gt;\((\textbf{units }x)\)&lt;/span&gt;. However, if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt;
is unit free, we're happy with &lt;span class="math"&gt;\((\textbf{units }x) / (\textbf{units }f)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Solving for the units of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; we get,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\alpha) = \frac{(\textbf{units }x)^2}{(\textbf{units }f)}.
$$&lt;/div&gt;
&lt;p&gt;This gives us an idea for what &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; should be.&lt;/p&gt;
&lt;p&gt;For example, the inverse Hessian passes the unit check (if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt; unit
free). The disadvantages of the Hessian is that it needs to be positive-definite
(or at least invertible) in order to be a valid "step size" (i.e., we need
step sizes to be &lt;span class="math"&gt;\(&amp;gt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Another method for handling step sizes is line search. However, line search
won't let us run online. Furthermore, line search would be too slow in the case
where we want a step size for each dimension.&lt;/p&gt;
&lt;p&gt;In machine learning, we've become fond of online methods, which adapt the step
size as they go. The general idea is to estimate a step size matrix that passes
the unit check (for each dimension of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). Furthermore, we want do as little
extra work as possible to get this estimate (e.g., we want to avoid computing a
Hessian because that would be extra work). So, the step size should be based
only iterates and gradients up to time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf"&gt;AdaGrad&lt;/a&gt; doesn't doesn't
  pass the unit check. This motivated AdaDelta.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1212.5701"&gt;AdaDelta&lt;/a&gt; uses the ratio of (running
  estimates of) the root-mean-squares of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\partial f / \partial
  x\)&lt;/span&gt;. The mean is taken using an exponentially weighted moving average. See
  paper for actual implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; came later and made some tweaks to
  remove (unintended) bias in the AdaDelta estimates of the numerator and
  denominator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, it's important/useful to analyze the units of numerical algorithms
in order to get a sanity check (i.e., catch mistakes) as well as to develop an
understanding of why certain parameters exist and how properties of a problem
affect the values we should use for them.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="optimization"></category></entry><entry><title>Gradient-based hyperparameter optimization and the implicit function theorem</title><link href="http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/" rel="alternate"></link><published>2016-03-05T00:00:00-05:00</published><updated>2016-03-05T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-03-05:/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/</id><summary type="html">&lt;p&gt;The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization&amp;mdash;the "inner" optimization optimizes training loss (wrt &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;),
while the "outer" optimizes hyperparameters (&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$&lt;/div&gt;
&lt;p&gt;Can we estimate &lt;span class="math"&gt;\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)&lt;/span&gt; so
that we can run gradient-based optimization over &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Well, what does it mean to have an &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; inside a function?&lt;/p&gt;
&lt;p&gt;Well, it means that there is a &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that gets passed to
&lt;span class="math"&gt;\(\mathcal{L}_{\text{dev}}\)&lt;/span&gt;. And, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, denoted
&lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt;. Furthermore, &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt; is the solution to,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$&lt;/div&gt;
&lt;p&gt;Now how does &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; change as the result of an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The constraint on the derivative implies a type of "equilibrium"&amp;mdash;the inner
optimization process will continue to optimize regardless of how we change
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Assuming we don't change &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; too much, then the inner
optimization shouldn't change &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; too much and it will change in a
predictable way.&lt;/p&gt;
&lt;p&gt;To do this, we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are related through a
function &lt;span class="math"&gt;\(g\)&lt;/span&gt; as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$g(x,y) = 0.$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;span class="math"&gt;\(g\)&lt;/span&gt; is a smooth function in &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we can perturb either
argument, say &lt;span class="math"&gt;\(x\)&lt;/span&gt; by a small amount &lt;span class="math"&gt;\(\Delta_x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; by &lt;span class="math"&gt;\(\Delta_y\)&lt;/span&gt;. Because
system preserves the constraint, i.e.,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$&lt;/div&gt;
&lt;p&gt;We can solve for the change of &lt;span class="math"&gt;\(x\)&lt;/span&gt; as a result of an infinitesimal change in
&lt;span class="math"&gt;\(y\)&lt;/span&gt;. We take the first-order expansion,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(g(x,y)\)&lt;/span&gt; is already zero,&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Next, we solve for &lt;span class="math"&gt;\(\frac{\Delta_x}{\Delta_y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$&lt;/div&gt;
&lt;p&gt;Back to the original problem: Now we can use the implicit function theorem to
estimate how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; varies in &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; by plugging in &lt;span class="math"&gt;\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)&lt;/span&gt;, &lt;span class="math"&gt;\(x \mapsto \theta\)&lt;/span&gt;
and &lt;span class="math"&gt;\(y \mapsto \lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$&lt;/div&gt;
&lt;p&gt;This tells us how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; changes with respect to an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$&lt;/div&gt;
&lt;p&gt;Since we don't like (explicit) matrix inverses, we compute &lt;span class="math"&gt;\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)&lt;/span&gt; as the solution to &lt;span class="math"&gt;\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)&lt;/span&gt;. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products&amp;mdash;i.e., you never have to materialize the Hessian. (Apparently,
&lt;a href="https://en.wikipedia.org/wiki/Matrix-free_methods"&gt;matrix-free linear algebra&lt;/a&gt;
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
&lt;a href="/blog/post/2014/02/10/gradient-vector-product/"&gt;earlier post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.&lt;/p&gt;
&lt;p&gt;In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/"&gt;Truncated Bi-Level Optimization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf"&gt;Efficient multiple hyperparameter learning for log-linear models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1502.03492"&gt;Gradient-based Hyperparameter Optimization through Reversible Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://fa.bianp.net/blog/2016/hyperparameter-optimization-with-approximate-gradient/"&gt;Hyperparameter optimization with approximate gradient&lt;/a&gt;
   (&lt;a href="https://arxiv.org/pdf/1602.02355.pdf"&gt;paper&lt;/a&gt;): This paper looks at the implicit
   differentiation approach where you have an &lt;em&gt;approximate&lt;/em&gt;
   solution to the inner optimization problem. They are able to provide error bounds and
   convergence guarantees under some reasonable conditions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization&amp;mdash;the "inner" optimization optimizes training loss (wrt &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;),
while the "outer" optimizes hyperparameters (&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$&lt;/div&gt;
&lt;p&gt;Can we estimate &lt;span class="math"&gt;\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)&lt;/span&gt; so
that we can run gradient-based optimization over &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Well, what does it mean to have an &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; inside a function?&lt;/p&gt;
&lt;p&gt;Well, it means that there is a &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that gets passed to
&lt;span class="math"&gt;\(\mathcal{L}_{\text{dev}}\)&lt;/span&gt;. And, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, denoted
&lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt;. Furthermore, &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt; is the solution to,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$&lt;/div&gt;
&lt;p&gt;Now how does &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; change as the result of an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The constraint on the derivative implies a type of "equilibrium"&amp;mdash;the inner
optimization process will continue to optimize regardless of how we change
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Assuming we don't change &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; too much, then the inner
optimization shouldn't change &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; too much and it will change in a
predictable way.&lt;/p&gt;
&lt;p&gt;To do this, we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are related through a
function &lt;span class="math"&gt;\(g\)&lt;/span&gt; as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$g(x,y) = 0.$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;span class="math"&gt;\(g\)&lt;/span&gt; is a smooth function in &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we can perturb either
argument, say &lt;span class="math"&gt;\(x\)&lt;/span&gt; by a small amount &lt;span class="math"&gt;\(\Delta_x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; by &lt;span class="math"&gt;\(\Delta_y\)&lt;/span&gt;. Because
system preserves the constraint, i.e.,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$&lt;/div&gt;
&lt;p&gt;We can solve for the change of &lt;span class="math"&gt;\(x\)&lt;/span&gt; as a result of an infinitesimal change in
&lt;span class="math"&gt;\(y\)&lt;/span&gt;. We take the first-order expansion,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(g(x,y)\)&lt;/span&gt; is already zero,&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Next, we solve for &lt;span class="math"&gt;\(\frac{\Delta_x}{\Delta_y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$&lt;/div&gt;
&lt;p&gt;Back to the original problem: Now we can use the implicit function theorem to
estimate how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; varies in &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; by plugging in &lt;span class="math"&gt;\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)&lt;/span&gt;, &lt;span class="math"&gt;\(x \mapsto \theta\)&lt;/span&gt;
and &lt;span class="math"&gt;\(y \mapsto \lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$&lt;/div&gt;
&lt;p&gt;This tells us how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; changes with respect to an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$&lt;/div&gt;
&lt;p&gt;Since we don't like (explicit) matrix inverses, we compute &lt;span class="math"&gt;\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)&lt;/span&gt; as the solution to &lt;span class="math"&gt;\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)&lt;/span&gt;. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products&amp;mdash;i.e., you never have to materialize the Hessian. (Apparently,
&lt;a href="https://en.wikipedia.org/wiki/Matrix-free_methods"&gt;matrix-free linear algebra&lt;/a&gt;
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
&lt;a href="/blog/post/2014/02/10/gradient-vector-product/"&gt;earlier post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.&lt;/p&gt;
&lt;p&gt;In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/"&gt;Truncated Bi-Level Optimization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf"&gt;Efficient multiple hyperparameter learning for log-linear models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1502.03492"&gt;Gradient-based Hyperparameter Optimization through Reversible Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://fa.bianp.net/blog/2016/hyperparameter-optimization-with-approximate-gradient/"&gt;Hyperparameter optimization with approximate gradient&lt;/a&gt;
   (&lt;a href="https://arxiv.org/pdf/1602.02355.pdf"&gt;paper&lt;/a&gt;): This paper looks at the implicit
   differentiation approach where you have an &lt;em&gt;approximate&lt;/em&gt;
   solution to the inner optimization problem. They are able to provide error bounds and
   convergence guarantees under some reasonable conditions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="calculus"></category><category term="hyperparameter-optimization"></category></entry><entry><title>Multidimensional array index</title><link href="http://timvieira.github.io/blog/post/2016/01/17/multidimensional-array-index/" rel="alternate"></link><published>2016-01-17T00:00:00-05:00</published><updated>2016-01-17T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-01-17:/blog/post/2016/01/17/multidimensional-array-index/</id><summary type="html">&lt;p&gt;This is a simple note on how to compute a bijective mapping between the indices
of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional array and a flat, one-dimensional array. We'll look at
both directions of the mapping: &lt;code&gt;(tuple-&amp;gt;int)&lt;/code&gt; and &lt;code&gt;(int -&amp;gt; tuple)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We'll assume each dimension &lt;span class="math"&gt;\(a, b, c, \ldots\)&lt;/span&gt; is a positive integer and bounded
&lt;span class="math"&gt;\(a \le A, b \le B, c \le C, \ldots\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Start small&lt;/h3&gt;
&lt;p&gt;Let's start by looking at &lt;span class="math"&gt;\(n = 3\)&lt;/span&gt; and generalize from there.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's our test case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: This is not the only bijective mapping from &lt;code&gt;tuple&lt;/code&gt; to &lt;code&gt;int&lt;/code&gt; that we
could have come up with. The one we chose corresponds to the particular layout,
which is apparent in the test case.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(n=4\)&lt;/span&gt; the pattern is &lt;span class="math"&gt;\(((a \cdot B + b) \cdot C + d) \cdot D + d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Sidenote: We don't actually need the bound &lt;span class="math"&gt;\(a \le A\)&lt;/span&gt; in either &lt;code&gt;index&lt;/code&gt; or
&lt;code&gt;inverse&lt;/code&gt;. This gives us a little extra flexibility because our first
dimension can be infinite/unknown.&lt;/p&gt;
&lt;h3&gt;General case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Map tuple ``a`` to index with known bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# the pattern:&lt;/span&gt;
    &lt;span class="c1"&gt;# ((i*J + j)*K + k)*L + l&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Find key given index ``ix`` and bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Testing the general case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Test that `index` produces the layout we expect.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;got&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;got&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;This is a simple note on how to compute a bijective mapping between the indices
of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional array and a flat, one-dimensional array. We'll look at
both directions of the mapping: &lt;code&gt;(tuple-&amp;gt;int)&lt;/code&gt; and &lt;code&gt;(int -&amp;gt; tuple)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We'll assume each dimension &lt;span class="math"&gt;\(a, b, c, \ldots\)&lt;/span&gt; is a positive integer and bounded
&lt;span class="math"&gt;\(a \le A, b \le B, c \le C, \ldots\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Start small&lt;/h3&gt;
&lt;p&gt;Let's start by looking at &lt;span class="math"&gt;\(n = 3\)&lt;/span&gt; and generalize from there.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's our test case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: This is not the only bijective mapping from &lt;code&gt;tuple&lt;/code&gt; to &lt;code&gt;int&lt;/code&gt; that we
could have come up with. The one we chose corresponds to the particular layout,
which is apparent in the test case.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(n=4\)&lt;/span&gt; the pattern is &lt;span class="math"&gt;\(((a \cdot B + b) \cdot C + d) \cdot D + d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Sidenote: We don't actually need the bound &lt;span class="math"&gt;\(a \le A\)&lt;/span&gt; in either &lt;code&gt;index&lt;/code&gt; or
&lt;code&gt;inverse&lt;/code&gt;. This gives us a little extra flexibility because our first
dimension can be infinite/unknown.&lt;/p&gt;
&lt;h3&gt;General case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Map tuple ``a`` to index with known bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# the pattern:&lt;/span&gt;
    &lt;span class="c1"&gt;# ((i*J + j)*K + k)*L + l&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Find key given index ``ix`` and bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Testing the general case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Test that `index` produces the layout we expect.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;got&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;got&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category></entry><entry><title>Gradient of a product</title><link href="http://timvieira.github.io/blog/post/2015/07/29/gradient-of-a-product/" rel="alternate"></link><published>2015-07-29T00:00:00-04:00</published><updated>2015-07-29T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-07-29:/blog/post/2015/07/29/gradient-of-a-product/</id><summary type="html">&lt;div class="math"&gt;$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$&lt;/div&gt;
&lt;p&gt;In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).&lt;/p&gt;
&lt;p&gt;Given a collection of functions with a common argument &lt;span class="math"&gt;\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define their product &lt;span class="math"&gt;\(p(x) = \prod_{i=1}^n f_i(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."&lt;/p&gt;
&lt;p&gt;A naive method for computing the gradient computes the leave-one-out products
from scratch for each &lt;span class="math"&gt;\(i\)&lt;/span&gt; (outer loop)&amp;mdash;resulting in a overall runtime of
&lt;span class="math"&gt;\(O(n^2)\)&lt;/span&gt; to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Division trick&lt;/strong&gt;: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f \ne 0\)&lt;/span&gt;. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requires multiplicative inverse operator (division) &lt;em&gt;and&lt;/em&gt;
   associative-commutative multiplication, which means it's not applicable to
   matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Log trick&lt;/strong&gt;: Suppose &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;). It's common practice (especially in machine learning) to replace
&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log f_i\)&lt;/span&gt;, which turns products into sums, &lt;span class="math"&gt;\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)&lt;/span&gt;, and tiny numbers (like
&lt;span class="math"&gt;\(\texttt{3.72e-44}\)&lt;/span&gt;) into large ones (like &lt;span class="math"&gt;\(\texttt{-100}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Furthermore, using the identity &lt;span class="math"&gt;\((\nabla g) = g \cdot \nabla \log g\)&lt;/span&gt;, we can
operate exclusively in the "&lt;span class="math"&gt;\(\log\)&lt;/span&gt;-domain".&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Numerically stable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't require multiplicative inverse assuming you can compute &lt;span class="math"&gt;\(\gradx{ \log
   f_i(x) }\)&lt;/span&gt; without it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f &amp;gt; 0\)&lt;/span&gt;. But, we can use
   &lt;a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/"&gt;LogReal number class&lt;/a&gt;
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't easily generalize to other notions of multiplication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic programming trick&lt;/strong&gt;: &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; runtime and &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. &lt;a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf"&gt;Wallach (2004)&lt;/a&gt;,
section 7).&lt;/p&gt;
&lt;p&gt;The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.&lt;/p&gt;
&lt;p&gt;Here are the equations:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\alpha_0(x) &amp;amp;=&amp;amp; 1 \\
\alpha_t(x)
   &amp;amp;=&amp;amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;amp;=&amp;amp; 1 \\
\beta_t(x)
  &amp;amp;=&amp;amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \prod_{j &amp;lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &amp;gt; i} f_j(x) \right) \\
&amp;amp;=&amp;amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Clearly, this requires &lt;span class="math"&gt;\(O(n)\)&lt;/span&gt; additional space.&lt;/p&gt;
&lt;p&gt;Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).&lt;/p&gt;
&lt;p&gt;Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where &lt;span class="math"&gt;\(A B C \ne B C A\)&lt;/span&gt;, even if all matrices have the
conformable dimensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Connections to automatic differentiation&lt;/strong&gt;: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
&lt;em&gt;can&lt;/em&gt; compute it's gradient with the same asymptotic complexity, &lt;em&gt;but&lt;/em&gt; you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with &lt;span class="math"&gt;\(\bigo(n^2)\)&lt;/span&gt; time and &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt; space
(other than the space to store the &lt;span class="math"&gt;\(n\)&lt;/span&gt; functions) and ended up with a &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt;
time &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space algorithm with a little clever thinking. What I'm saying
is autodiff&amp;mdash;even if you don't use a magical package&amp;mdash;tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sketch&lt;/em&gt;: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). It turns out that the backward pass gives us &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as
adjoints. Lastly, we'd propagate gradients through the &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s to get
&lt;span class="math"&gt;\(\frac{\partial p}{\partial x}\)&lt;/span&gt;. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;div class="math"&gt;$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$&lt;/div&gt;
&lt;p&gt;In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).&lt;/p&gt;
&lt;p&gt;Given a collection of functions with a common argument &lt;span class="math"&gt;\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define their product &lt;span class="math"&gt;\(p(x) = \prod_{i=1}^n f_i(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."&lt;/p&gt;
&lt;p&gt;A naive method for computing the gradient computes the leave-one-out products
from scratch for each &lt;span class="math"&gt;\(i\)&lt;/span&gt; (outer loop)&amp;mdash;resulting in a overall runtime of
&lt;span class="math"&gt;\(O(n^2)\)&lt;/span&gt; to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Division trick&lt;/strong&gt;: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f \ne 0\)&lt;/span&gt;. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requires multiplicative inverse operator (division) &lt;em&gt;and&lt;/em&gt;
   associative-commutative multiplication, which means it's not applicable to
   matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Log trick&lt;/strong&gt;: Suppose &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;). It's common practice (especially in machine learning) to replace
&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log f_i\)&lt;/span&gt;, which turns products into sums, &lt;span class="math"&gt;\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)&lt;/span&gt;, and tiny numbers (like
&lt;span class="math"&gt;\(\texttt{3.72e-44}\)&lt;/span&gt;) into large ones (like &lt;span class="math"&gt;\(\texttt{-100}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Furthermore, using the identity &lt;span class="math"&gt;\((\nabla g) = g \cdot \nabla \log g\)&lt;/span&gt;, we can
operate exclusively in the "&lt;span class="math"&gt;\(\log\)&lt;/span&gt;-domain".&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Numerically stable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't require multiplicative inverse assuming you can compute &lt;span class="math"&gt;\(\gradx{ \log
   f_i(x) }\)&lt;/span&gt; without it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f &amp;gt; 0\)&lt;/span&gt;. But, we can use
   &lt;a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/"&gt;LogReal number class&lt;/a&gt;
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't easily generalize to other notions of multiplication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic programming trick&lt;/strong&gt;: &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; runtime and &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. &lt;a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf"&gt;Wallach (2004)&lt;/a&gt;,
section 7).&lt;/p&gt;
&lt;p&gt;The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.&lt;/p&gt;
&lt;p&gt;Here are the equations:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\alpha_0(x) &amp;amp;=&amp;amp; 1 \\
\alpha_t(x)
   &amp;amp;=&amp;amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;amp;=&amp;amp; 1 \\
\beta_t(x)
  &amp;amp;=&amp;amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \prod_{j &amp;lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &amp;gt; i} f_j(x) \right) \\
&amp;amp;=&amp;amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Clearly, this requires &lt;span class="math"&gt;\(O(n)\)&lt;/span&gt; additional space.&lt;/p&gt;
&lt;p&gt;Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).&lt;/p&gt;
&lt;p&gt;Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where &lt;span class="math"&gt;\(A B C \ne B C A\)&lt;/span&gt;, even if all matrices have the
conformable dimensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Connections to automatic differentiation&lt;/strong&gt;: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
&lt;em&gt;can&lt;/em&gt; compute it's gradient with the same asymptotic complexity, &lt;em&gt;but&lt;/em&gt; you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with &lt;span class="math"&gt;\(\bigo(n^2)\)&lt;/span&gt; time and &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt; space
(other than the space to store the &lt;span class="math"&gt;\(n\)&lt;/span&gt; functions) and ended up with a &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt;
time &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space algorithm with a little clever thinking. What I'm saying
is autodiff&amp;mdash;even if you don't use a magical package&amp;mdash;tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sketch&lt;/em&gt;: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). It turns out that the backward pass gives us &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as
adjoints. Lastly, we'd propagate gradients through the &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s to get
&lt;span class="math"&gt;\(\frac{\partial p}{\partial x}\)&lt;/span&gt;. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="calculus"></category><category term="numerical"></category><category term="automatic-differentiation"></category></entry><entry><title>Multiclass logistic regression and conditional random fields are the same thing</title><link href="http://timvieira.github.io/blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/" rel="alternate"></link><published>2015-04-29T00:00:00-04:00</published><updated>2015-04-29T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-04-29:/blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/</id><summary type="html">&lt;p&gt;A short rant: Multiclass logistic regression and conditional random fields (CRF)
are the same thing. This comes to a surprise to many people because CRFs tend to
be surrounded by additional "stuff."&lt;/p&gt;
&lt;p&gt;Understanding this very basic connection not only deepens our understanding, but
also suggests a method for testing complex CRF code.&lt;/p&gt;
&lt;p&gt;Multiclass logistic regression is simple. The goal is to predict the correct
label &lt;span class="math"&gt;\(y^*\)&lt;/span&gt; from handful of labels &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt; given the observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; based
on features &lt;span class="math"&gt;\(\phi(x,y)\)&lt;/span&gt;. Training this model typically requires computing the
gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla \log p(y^* \mid x) = \phi(x,y^*) - \sum_{y \in \mathcal{Y}} p(y|x) \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
p(y|x) &amp;amp;=&amp;amp; \frac{1}{Z(x)} \exp(\theta^\top \phi(x,y)) &amp;amp; \ \ \ \ \text{and} \ \ \ \ &amp;amp;
Z(x) &amp;amp;=&amp;amp; \sum_{y \in \mathcal{Y}} \exp(\theta^\top \phi(x,y))
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;At test-time, we often take the highest-scoring label under the model.&lt;/p&gt;
&lt;div class="math"&gt;$$
\widehat{y}(x) = \underset{y \in \mathcal{Y}}{\textrm{argmax}}\ \theta^\top \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;A conditional random field is &lt;em&gt;exactly&lt;/em&gt; multiclass logistic regression. The only
difference is that the sums (&lt;span class="math"&gt;\(Z(x)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sum_{y \in \mathcal{Y}} p(y|x)
\phi(x,y)\)&lt;/span&gt;) and the argmax &lt;span class="math"&gt;\(\widehat{y}(x)\)&lt;/span&gt; are inefficient to compute naively
(i.e., by brute-force enumeration). This point is often lost when people first
learn about CRFs. Some people never make this connection.&lt;/p&gt;
&lt;p&gt;Brute-force enumeration is a very useful method for testing complex dynamic
programming procedures for computing the sums and the argmax on relatively small
examples. Don't just copy code for dynamic programming out of a paper! Test it!&lt;/p&gt;
&lt;p&gt;Here's some stuff you'll see once we start talking about CRFs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inference algorithms (e.g., Viterbi decoding, forward-backward, Junction
   tree)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graphical models (factor graphs, Bayes nets, Markov random fields)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model templates (i.e., repeated feature functions)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the logistic regression case, we'd never use the term "inference" to describe
the "sum" and "max" over a handful of categories. Once we move to a structured
label space, this term gets throw around. (BTW, this isn't "statistical
inference," just algorithms to compute sum and max over &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;Graphical models establish a notation and structural properties which allow
efficient inference&amp;mdash;things like cycles and treewidth.&lt;/p&gt;
&lt;p&gt;Model templating is the only essential trick to move from logistic regression to
a CRF. Templating "solves" the problem that not all training examples have the
same "size"&amp;mdash;the set of outputs &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; now depends on &lt;span class="math"&gt;\(x\)&lt;/span&gt;. A model
template specifies how to compute the features for an entire output, by looking
at interactions between subsets of variables.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,\boldsymbol{y}) = \sum_{\alpha \in A(x)} \phi_\alpha(x,
\boldsymbol{y}_\alpha)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a labeled subset of variables often called a factor and
&lt;span class="math"&gt;\(\boldsymbol{y}_\alpha\)&lt;/span&gt; is the subvector containing values of variables
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. Basically, the feature function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gets to look at some subset of
the variables being predicted &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the entire input &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The ability to look
at more of &lt;span class="math"&gt;\(y\)&lt;/span&gt; allows the model to make more coherent predictions.&lt;/p&gt;
&lt;p&gt;Anywho, it's often useful to take a step back and think about what you are
trying to compute instead of how you're computing it. In this post, this allowed
us see the similarity between logistic regression and CRFs even though they seem
quite different.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;A short rant: Multiclass logistic regression and conditional random fields (CRF)
are the same thing. This comes to a surprise to many people because CRFs tend to
be surrounded by additional "stuff."&lt;/p&gt;
&lt;p&gt;Understanding this very basic connection not only deepens our understanding, but
also suggests a method for testing complex CRF code.&lt;/p&gt;
&lt;p&gt;Multiclass logistic regression is simple. The goal is to predict the correct
label &lt;span class="math"&gt;\(y^*\)&lt;/span&gt; from handful of labels &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt; given the observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; based
on features &lt;span class="math"&gt;\(\phi(x,y)\)&lt;/span&gt;. Training this model typically requires computing the
gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla \log p(y^* \mid x) = \phi(x,y^*) - \sum_{y \in \mathcal{Y}} p(y|x) \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
p(y|x) &amp;amp;=&amp;amp; \frac{1}{Z(x)} \exp(\theta^\top \phi(x,y)) &amp;amp; \ \ \ \ \text{and} \ \ \ \ &amp;amp;
Z(x) &amp;amp;=&amp;amp; \sum_{y \in \mathcal{Y}} \exp(\theta^\top \phi(x,y))
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;At test-time, we often take the highest-scoring label under the model.&lt;/p&gt;
&lt;div class="math"&gt;$$
\widehat{y}(x) = \underset{y \in \mathcal{Y}}{\textrm{argmax}}\ \theta^\top \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;A conditional random field is &lt;em&gt;exactly&lt;/em&gt; multiclass logistic regression. The only
difference is that the sums (&lt;span class="math"&gt;\(Z(x)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sum_{y \in \mathcal{Y}} p(y|x)
\phi(x,y)\)&lt;/span&gt;) and the argmax &lt;span class="math"&gt;\(\widehat{y}(x)\)&lt;/span&gt; are inefficient to compute naively
(i.e., by brute-force enumeration). This point is often lost when people first
learn about CRFs. Some people never make this connection.&lt;/p&gt;
&lt;p&gt;Brute-force enumeration is a very useful method for testing complex dynamic
programming procedures for computing the sums and the argmax on relatively small
examples. Don't just copy code for dynamic programming out of a paper! Test it!&lt;/p&gt;
&lt;p&gt;Here's some stuff you'll see once we start talking about CRFs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inference algorithms (e.g., Viterbi decoding, forward-backward, Junction
   tree)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graphical models (factor graphs, Bayes nets, Markov random fields)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model templates (i.e., repeated feature functions)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the logistic regression case, we'd never use the term "inference" to describe
the "sum" and "max" over a handful of categories. Once we move to a structured
label space, this term gets throw around. (BTW, this isn't "statistical
inference," just algorithms to compute sum and max over &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;Graphical models establish a notation and structural properties which allow
efficient inference&amp;mdash;things like cycles and treewidth.&lt;/p&gt;
&lt;p&gt;Model templating is the only essential trick to move from logistic regression to
a CRF. Templating "solves" the problem that not all training examples have the
same "size"&amp;mdash;the set of outputs &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; now depends on &lt;span class="math"&gt;\(x\)&lt;/span&gt;. A model
template specifies how to compute the features for an entire output, by looking
at interactions between subsets of variables.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,\boldsymbol{y}) = \sum_{\alpha \in A(x)} \phi_\alpha(x,
\boldsymbol{y}_\alpha)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a labeled subset of variables often called a factor and
&lt;span class="math"&gt;\(\boldsymbol{y}_\alpha\)&lt;/span&gt; is the subvector containing values of variables
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. Basically, the feature function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gets to look at some subset of
the variables being predicted &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the entire input &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The ability to look
at more of &lt;span class="math"&gt;\(y\)&lt;/span&gt; allows the model to make more coherent predictions.&lt;/p&gt;
&lt;p&gt;Anywho, it's often useful to take a step back and think about what you are
trying to compute instead of how you're computing it. In this post, this allowed
us see the similarity between logistic regression and CRFs even though they seem
quite different.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category><category term="rant"></category><category term="crf"></category></entry><entry><title>Conditional random fields as deep learning models?</title><link href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/" rel="alternate"></link><published>2015-02-05T00:00:00-05:00</published><updated>2015-02-05T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-02-05:/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/</id><summary type="html">&lt;p&gt;This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.&lt;/p&gt;
&lt;p&gt;In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Limit the expressiveness of the model (i.e., don't use to model you want)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the training objective&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approximate inference&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Approximate inference is tricky. Things can easily go awry.&lt;/p&gt;
&lt;p&gt;For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
&lt;a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf"&gt;(Kulesza &amp;amp; Pereira, 2007)&lt;/a&gt;. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
&lt;a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf"&gt;sometimes&lt;/a&gt; these loopy BP
approximations work fine.)&lt;/p&gt;
&lt;p&gt;It looks like using approximate inference during training changes the training
objective.&lt;/p&gt;
&lt;p&gt;So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.&lt;/p&gt;
&lt;p&gt;Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp;amp; Eisner.&lt;/p&gt;
&lt;p&gt;Unsuprisingly, it works pretty well.&lt;/p&gt;
&lt;p&gt;I first saw this idea in
&lt;a href="http://proceedings.mlr.press/v15/stoyanov11a/stoyanov11a.pdf"&gt;Stoyanov &amp;amp; Eisner (2011)&lt;/a&gt;. They
use loopy belief propagation as their approximate inference algorithm. At the
end of the day, their model is essentially a deep recurrent network, which came
from unrolling inference in a graphical model. This idea really struck me
because it's clearly right in the middle between graphical models and deep
learning.&lt;/p&gt;
&lt;p&gt;You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.&lt;/p&gt;
&lt;p&gt;Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.&lt;/p&gt;
&lt;p&gt;In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Limit the expressiveness of the model (i.e., don't use to model you want)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the training objective&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approximate inference&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Approximate inference is tricky. Things can easily go awry.&lt;/p&gt;
&lt;p&gt;For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
&lt;a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf"&gt;(Kulesza &amp;amp; Pereira, 2007)&lt;/a&gt;. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
&lt;a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf"&gt;sometimes&lt;/a&gt; these loopy BP
approximations work fine.)&lt;/p&gt;
&lt;p&gt;It looks like using approximate inference during training changes the training
objective.&lt;/p&gt;
&lt;p&gt;So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.&lt;/p&gt;
&lt;p&gt;Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp;amp; Eisner.&lt;/p&gt;
&lt;p&gt;Unsuprisingly, it works pretty well.&lt;/p&gt;
&lt;p&gt;I first saw this idea in
&lt;a href="http://proceedings.mlr.press/v15/stoyanov11a/stoyanov11a.pdf"&gt;Stoyanov &amp;amp; Eisner (2011)&lt;/a&gt;. They
use loopy belief propagation as their approximate inference algorithm. At the
end of the day, their model is essentially a deep recurrent network, which came
from unrolling inference in a graphical model. This idea really struck me
because it's clearly right in the middle between graphical models and deep
learning.&lt;/p&gt;
&lt;p&gt;You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.&lt;/p&gt;
&lt;p&gt;Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.&lt;/p&gt;</content><category term="machine-learning"></category><category term="deep-learning"></category><category term="structured-prediction"></category></entry><entry><title>Log-Real number class</title><link href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/" rel="alternate"></link><published>2015-02-01T00:00:00-05:00</published><updated>2015-02-01T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-02-01:/blog/post/2015/02/01/log-real-number-class/</id><summary type="html">&lt;p&gt;Most people know how to avoid numerical underflow in probability computations by
representing intermediate quantities in the log-domain. This trick turns
"multiplication" into "addition", "addition" into "logsumexp", "0" into
&lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; and "1" into &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Most importantly, it turns really small numbers into
reasonable-size numbers.&lt;/p&gt;
&lt;p&gt;Unfortunately, without modification, this trick is limited to positive numbers
because &lt;code&gt;log&lt;/code&gt; of a negative number is &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Well, there is good news! For the cost of an extra bit, we can extend this trick
to the negative reals and furthermore, we get a bonafide ring instead of a mere
semiring.&lt;/p&gt;
&lt;p&gt;I first saw this trick in
&lt;a href="http://www.aclweb.org/anthology/D09-1005"&gt;Li and Eisner (2009)&lt;/a&gt;. The trick is
nicely summarized in Table 3 of that paper, which I've pasted below.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
&lt;img src="/blog/images/logreal.png"/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Why do I care?&lt;/strong&gt; When computing gradients (e.g., gradient of risk),
intermediate values are rarely all positive. Furthermore, we're often
multiplying small things together. I've recently found log-reals to be effective
at squeaking a bit more numerical accuracy.&lt;/p&gt;
&lt;p&gt;This trick is useful for almost all backprop computations because backprop is
essentially:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;adjoint(u) += adjoint(v) * dv/du.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only tricky bit is lifting all &lt;code&gt;du/dv&lt;/code&gt; computations into the log-reals.&lt;/p&gt;
&lt;p&gt;Implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This trick is better suited to programming languages with structs. Using
  objects will probably in an horrible slow down and using parallel arrays to
  store the sign bit and double is probably too tedious and error prone. (Sorry
  java folks.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here's a &lt;a href="https://github.com/andre-martins/TurboParser/blob/master/src/util/logval.h"&gt;C++ implementation&lt;/a&gt;
  with operator overloading from Andre Martins&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that log-real &lt;code&gt;+=&lt;/code&gt; involves calls to &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;, which will
  definitely slow your code down a bit (these functions are much slower than
  addition).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Most people know how to avoid numerical underflow in probability computations by
representing intermediate quantities in the log-domain. This trick turns
"multiplication" into "addition", "addition" into "logsumexp", "0" into
&lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; and "1" into &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Most importantly, it turns really small numbers into
reasonable-size numbers.&lt;/p&gt;
&lt;p&gt;Unfortunately, without modification, this trick is limited to positive numbers
because &lt;code&gt;log&lt;/code&gt; of a negative number is &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Well, there is good news! For the cost of an extra bit, we can extend this trick
to the negative reals and furthermore, we get a bonafide ring instead of a mere
semiring.&lt;/p&gt;
&lt;p&gt;I first saw this trick in
&lt;a href="http://www.aclweb.org/anthology/D09-1005"&gt;Li and Eisner (2009)&lt;/a&gt;. The trick is
nicely summarized in Table 3 of that paper, which I've pasted below.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
&lt;img src="/blog/images/logreal.png"/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Why do I care?&lt;/strong&gt; When computing gradients (e.g., gradient of risk),
intermediate values are rarely all positive. Furthermore, we're often
multiplying small things together. I've recently found log-reals to be effective
at squeaking a bit more numerical accuracy.&lt;/p&gt;
&lt;p&gt;This trick is useful for almost all backprop computations because backprop is
essentially:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;adjoint(u) += adjoint(v) * dv/du.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only tricky bit is lifting all &lt;code&gt;du/dv&lt;/code&gt; computations into the log-reals.&lt;/p&gt;
&lt;p&gt;Implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This trick is better suited to programming languages with structs. Using
  objects will probably in an horrible slow down and using parallel arrays to
  store the sign bit and double is probably too tedious and error prone. (Sorry
  java folks.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here's a &lt;a href="https://github.com/andre-martins/TurboParser/blob/master/src/util/logval.h"&gt;C++ implementation&lt;/a&gt;
  with operator overloading from Andre Martins&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that log-real &lt;code&gt;+=&lt;/code&gt; involves calls to &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;, which will
  definitely slow your code down a bit (these functions are much slower than
  addition).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="numerical"></category></entry><entry><title>Importance sampling</title><link href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/" rel="alternate"></link><published>2014-12-21T00:00:00-05:00</published><updated>2014-12-21T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-12-21:/blog/post/2014/12/21/importance-sampling/</id><summary type="html">&lt;p&gt;Importance sampling is a powerful and pervasive technique in statistics, machine
learning and randomized algorithms.&lt;/p&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Importance sampling is a technique for estimating the expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of a
random variable &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; under distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; from samples of a different
distribution &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The key observation is that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is can expressed as the expectation of a
different random variable &lt;span class="math"&gt;\(f^*(x)=\frac{p(x)}{q(x)}\! \cdot\! f(x)\)&lt;/span&gt; under &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}_{q}\! \left[ f^*(x) \right] = \mathbb{E}_{q}\! \left[ \frac{p(x)}{q(x)} f(x) \right] = \sum_{x} q(x) \frac{p(x)}{q(x)} f(x) = \sum_{x} p(x) f(x) = \mathbb{E}_{p}\! \left[ f(x) \right] = \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Technical condition: &lt;span class="math"&gt;\(q\)&lt;/span&gt; must have support everywhere &lt;span class="math"&gt;\(p\)&lt;/span&gt; does, &lt;span class="math"&gt;\(p(x) &amp;gt; 0
\Rightarrow q(x) &amp;gt; 0.\)&lt;/span&gt; Without this condition, the equation is biased! Note: &lt;span class="math"&gt;\(q\)&lt;/span&gt;
can support things that &lt;span class="math"&gt;\(p\)&lt;/span&gt; doesn't.&lt;/p&gt;
&lt;p&gt;Terminology: The quantity &lt;span class="math"&gt;\(w(x) = \frac{p(x)}{q(x)}\)&lt;/span&gt; is often referred to as the
"importance weight" or "importance correction". We often refer to &lt;span class="math"&gt;\(p\)&lt;/span&gt; as the
target density and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the proposal density.&lt;/p&gt;
&lt;p&gt;Now, given samples &lt;span class="math"&gt;\(\{ x^{(i)} \}_{i=1}^{n}\)&lt;/span&gt; from &lt;span class="math"&gt;\(q,\)&lt;/span&gt; we can use the Monte
Carlo estimate, &lt;span class="math"&gt;\(\hat{\mu} \approx \frac{1}{n} \sum_{i=1}^n f^{*}(x^{(i)}),\)&lt;/span&gt; as
an unbiased estimator of &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;There are a few reasons we might want use importance sampling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convenience&lt;/strong&gt;: It might be trickier to sample directly from &lt;span class="math"&gt;\(p.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bias-correction&lt;/strong&gt;: Suppose, we're developing an algorithm which requires
     samples to satisfy some "safety" condition (e.g., a minimum support
     threshold) and be unbiased. Importance sampling can be used to remove bias,
     while satisfying the condition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: It might be the case that sampling directly from
     &lt;span class="math"&gt;\(p\)&lt;/span&gt; would require more samples to estimate &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt; Check out these
     &lt;a href="http://www.columbia.edu/~mh2078/MCS04/MCS_var_red2.pdf"&gt;great notes&lt;/a&gt; for
     more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-policy evaluation and learning&lt;/strong&gt;: We might want to collect some
     "exploratory data" from &lt;span class="math"&gt;\(q\)&lt;/span&gt; and evaluate different "policies", &lt;span class="math"&gt;\(p\)&lt;/span&gt; (e.g.,
     to pick the best one). Here's a link to a future post on
     &lt;a href="http://timvieira.github.io/blog/post/2016/12/13/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation and counterfactual reasoning&lt;/a&gt;
     and some cool papers:
     &lt;a href="http://arxiv.org/abs/1209.2355"&gt;counterfactual reasoning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;reinforcement learning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/1103.4601"&gt;contextual bandits&lt;/a&gt;,
     &lt;a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;domain adaptation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few common cases for &lt;span class="math"&gt;\(q\)&lt;/span&gt; worth separate consideration:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: This is the case in experimental design, variance
     reduction, active learning and reinforcement learning. It's often difficult
     to design &lt;span class="math"&gt;\(q,\)&lt;/span&gt; which results in an estimator with "reasonable" variance. A
     very difficult case is in off-policy evaluation because it (essentially)
     requires a good exploratory distribution for every possible policy. (I have
     much more to say on this topic.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Little to no control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: For example, you're given some dataset
     (e.g., new articles) and you want to estimate performance on a different
     dataset (e.g., Twitter).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: In this case, we want to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; (typically referred
     to as the propensity score) and use it in the importance sampling
     estimator. This technique, as far as I can tell, is widely used to remove
     selection bias when estimating effects of different treatments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;: The main drawback of importance sampling is variance. A few bad
samples with large weights can drastically throw off the estimator. Thus, it's
often the case that a biased estimator is preferred, e.g.,
&lt;a href="https://hips.seas.harvard.edu/blog/2013/01/14/unbiased-estimators-of-partition-functions-are-basically-lower-bounds/"&gt;estimating the partition function&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/1209.2355"&gt;clipping weights&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/cs/0204043"&gt;indirect importance sampling&lt;/a&gt;. A secondary
drawback is that both densities must be normalized, which is often intractable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's next?&lt;/strong&gt; I plan to cover "variance reduction" and
&lt;a href="http://timvieira.github.io/blog/post/2016/12/13/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation&lt;/a&gt;
in more detail in future posts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Importance sampling is a powerful and pervasive technique in statistics, machine
learning and randomized algorithms.&lt;/p&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Importance sampling is a technique for estimating the expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of a
random variable &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; under distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; from samples of a different
distribution &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The key observation is that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is can expressed as the expectation of a
different random variable &lt;span class="math"&gt;\(f^*(x)=\frac{p(x)}{q(x)}\! \cdot\! f(x)\)&lt;/span&gt; under &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}_{q}\! \left[ f^*(x) \right] = \mathbb{E}_{q}\! \left[ \frac{p(x)}{q(x)} f(x) \right] = \sum_{x} q(x) \frac{p(x)}{q(x)} f(x) = \sum_{x} p(x) f(x) = \mathbb{E}_{p}\! \left[ f(x) \right] = \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Technical condition: &lt;span class="math"&gt;\(q\)&lt;/span&gt; must have support everywhere &lt;span class="math"&gt;\(p\)&lt;/span&gt; does, &lt;span class="math"&gt;\(p(x) &amp;gt; 0
\Rightarrow q(x) &amp;gt; 0.\)&lt;/span&gt; Without this condition, the equation is biased! Note: &lt;span class="math"&gt;\(q\)&lt;/span&gt;
can support things that &lt;span class="math"&gt;\(p\)&lt;/span&gt; doesn't.&lt;/p&gt;
&lt;p&gt;Terminology: The quantity &lt;span class="math"&gt;\(w(x) = \frac{p(x)}{q(x)}\)&lt;/span&gt; is often referred to as the
"importance weight" or "importance correction". We often refer to &lt;span class="math"&gt;\(p\)&lt;/span&gt; as the
target density and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the proposal density.&lt;/p&gt;
&lt;p&gt;Now, given samples &lt;span class="math"&gt;\(\{ x^{(i)} \}_{i=1}^{n}\)&lt;/span&gt; from &lt;span class="math"&gt;\(q,\)&lt;/span&gt; we can use the Monte
Carlo estimate, &lt;span class="math"&gt;\(\hat{\mu} \approx \frac{1}{n} \sum_{i=1}^n f^{*}(x^{(i)}),\)&lt;/span&gt; as
an unbiased estimator of &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;There are a few reasons we might want use importance sampling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convenience&lt;/strong&gt;: It might be trickier to sample directly from &lt;span class="math"&gt;\(p.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bias-correction&lt;/strong&gt;: Suppose, we're developing an algorithm which requires
     samples to satisfy some "safety" condition (e.g., a minimum support
     threshold) and be unbiased. Importance sampling can be used to remove bias,
     while satisfying the condition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: It might be the case that sampling directly from
     &lt;span class="math"&gt;\(p\)&lt;/span&gt; would require more samples to estimate &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt; Check out these
     &lt;a href="http://www.columbia.edu/~mh2078/MCS04/MCS_var_red2.pdf"&gt;great notes&lt;/a&gt; for
     more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-policy evaluation and learning&lt;/strong&gt;: We might want to collect some
     "exploratory data" from &lt;span class="math"&gt;\(q\)&lt;/span&gt; and evaluate different "policies", &lt;span class="math"&gt;\(p\)&lt;/span&gt; (e.g.,
     to pick the best one). Here's a link to a future post on
     &lt;a href="http://timvieira.github.io/blog/post/2016/12/13/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation and counterfactual reasoning&lt;/a&gt;
     and some cool papers:
     &lt;a href="http://arxiv.org/abs/1209.2355"&gt;counterfactual reasoning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;reinforcement learning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/1103.4601"&gt;contextual bandits&lt;/a&gt;,
     &lt;a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;domain adaptation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few common cases for &lt;span class="math"&gt;\(q\)&lt;/span&gt; worth separate consideration:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: This is the case in experimental design, variance
     reduction, active learning and reinforcement learning. It's often difficult
     to design &lt;span class="math"&gt;\(q,\)&lt;/span&gt; which results in an estimator with "reasonable" variance. A
     very difficult case is in off-policy evaluation because it (essentially)
     requires a good exploratory distribution for every possible policy. (I have
     much more to say on this topic.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Little to no control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: For example, you're given some dataset
     (e.g., new articles) and you want to estimate performance on a different
     dataset (e.g., Twitter).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: In this case, we want to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; (typically referred
     to as the propensity score) and use it in the importance sampling
     estimator. This technique, as far as I can tell, is widely used to remove
     selection bias when estimating effects of different treatments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;: The main drawback of importance sampling is variance. A few bad
samples with large weights can drastically throw off the estimator. Thus, it's
often the case that a biased estimator is preferred, e.g.,
&lt;a href="https://hips.seas.harvard.edu/blog/2013/01/14/unbiased-estimators-of-partition-functions-are-basically-lower-bounds/"&gt;estimating the partition function&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/1209.2355"&gt;clipping weights&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/cs/0204043"&gt;indirect importance sampling&lt;/a&gt;. A secondary
drawback is that both densities must be normalized, which is often intractable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's next?&lt;/strong&gt; I plan to cover "variance reduction" and
&lt;a href="http://timvieira.github.io/blog/post/2016/12/13/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation&lt;/a&gt;
in more detail in future posts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="statistics"></category><category term="importance-sampling"></category><category term="sampling"></category></entry><entry><title>Numerically stable p-norms</title><link href="http://timvieira.github.io/blog/post/2014/11/10/numerically-stable-p-norms/" rel="alternate"></link><published>2014-11-10T00:00:00-05:00</published><updated>2014-11-10T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-11-10:/blog/post/2014/11/10/numerically-stable-p-norms/</id><summary type="html">&lt;p&gt;Consider the p-norm
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$&lt;/div&gt;
&lt;p&gt;In python this translates to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;First-pass implementation of p-norm.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, suppose &lt;span class="math"&gt;\(|x_i|^p\)&lt;/span&gt; causes overflow (for some &lt;span class="math"&gt;\(i\)&lt;/span&gt;). This will occur for sufficiently large &lt;span class="math"&gt;\(p\)&lt;/span&gt; or sufficiently large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;&amp;mdash;even if &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is representable (i.e., not NaN or &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;big&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e300&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c1"&gt;# expected: 1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This fails because we can't square &lt;code&gt;big&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;A little math&lt;/h2&gt;
&lt;p&gt;There is a way to avoid overflowing because of a few large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here's a little fact about p-norms: for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \alpha \cdot \boldsymbol{x} ||_p = |\alpha| \cdot || \boldsymbol{x}   ||_p
$$&lt;/div&gt;
&lt;p&gt;We'll use the following version (harder to remember)
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p  = |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
$$&lt;/div&gt;
&lt;p&gt;Don't believe it? Here's some algebra:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
|| \boldsymbol{x} ||_p
&amp;amp;=&amp;amp; \left( \sum_i |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; \left( \sum_i \frac{|\alpha|^p}{|\alpha|^p} \cdot |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left( \frac{|x_i| }{|\alpha|} \right)^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left| \frac{x_i }{\alpha} \right|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
\end{eqnarray*}
$$&lt;/div&gt;
&lt;h2&gt;Back to numerical stability&lt;/h2&gt;
&lt;p&gt;Suppose we pick &lt;span class="math"&gt;\(\alpha = \max_i |x_i|\)&lt;/span&gt;. Now, the largest number we have to take
the power of is one&amp;mdash;making it very difficult to overflow on the account of
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. This should remind you of the infamous log-sum-exp trick.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, our example from before works :-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It appears as if &lt;code&gt;scipy.linalg.norm&lt;/code&gt; is robust to overflow, while &lt;code&gt;numpy.linalg.norm&lt;/code&gt; is not. Note that &lt;code&gt;scipy.linalg.norm&lt;/code&gt; appears to be a bit slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;logsumexp&lt;/code&gt; trick is nearly identical, but operates in the log-domain, i.e., &lt;span class="math"&gt;\(\text{logsumexp}(\log(|x|) \cdot p) / p = \log || x ||_p\)&lt;/span&gt;. You can implement both tricks with the same code, if you use different number classes for log-domain and real-domain&amp;mdash;a trick you might have seen before.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.math&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Consider the p-norm
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$&lt;/div&gt;
&lt;p&gt;In python this translates to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;First-pass implementation of p-norm.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, suppose &lt;span class="math"&gt;\(|x_i|^p\)&lt;/span&gt; causes overflow (for some &lt;span class="math"&gt;\(i\)&lt;/span&gt;). This will occur for sufficiently large &lt;span class="math"&gt;\(p\)&lt;/span&gt; or sufficiently large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;&amp;mdash;even if &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is representable (i.e., not NaN or &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;big&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e300&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c1"&gt;# expected: 1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This fails because we can't square &lt;code&gt;big&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;A little math&lt;/h2&gt;
&lt;p&gt;There is a way to avoid overflowing because of a few large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here's a little fact about p-norms: for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \alpha \cdot \boldsymbol{x} ||_p = |\alpha| \cdot || \boldsymbol{x}   ||_p
$$&lt;/div&gt;
&lt;p&gt;We'll use the following version (harder to remember)
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p  = |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
$$&lt;/div&gt;
&lt;p&gt;Don't believe it? Here's some algebra:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
|| \boldsymbol{x} ||_p
&amp;amp;=&amp;amp; \left( \sum_i |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; \left( \sum_i \frac{|\alpha|^p}{|\alpha|^p} \cdot |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left( \frac{|x_i| }{|\alpha|} \right)^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left| \frac{x_i }{\alpha} \right|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
\end{eqnarray*}
$$&lt;/div&gt;
&lt;h2&gt;Back to numerical stability&lt;/h2&gt;
&lt;p&gt;Suppose we pick &lt;span class="math"&gt;\(\alpha = \max_i |x_i|\)&lt;/span&gt;. Now, the largest number we have to take
the power of is one&amp;mdash;making it very difficult to overflow on the account of
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. This should remind you of the infamous log-sum-exp trick.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, our example from before works :-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It appears as if &lt;code&gt;scipy.linalg.norm&lt;/code&gt; is robust to overflow, while &lt;code&gt;numpy.linalg.norm&lt;/code&gt; is not. Note that &lt;code&gt;scipy.linalg.norm&lt;/code&gt; appears to be a bit slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;logsumexp&lt;/code&gt; trick is nearly identical, but operates in the log-domain, i.e., &lt;span class="math"&gt;\(\text{logsumexp}(\log(|x|) \cdot p) / p = \log || x ||_p\)&lt;/span&gt;. You can implement both tricks with the same code, if you use different number classes for log-domain and real-domain&amp;mdash;a trick you might have seen before.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.math&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="numerical"></category></entry><entry><title>KL-divergence as an objective function</title><link href="http://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/" rel="alternate"></link><published>2014-10-06T00:00:00-04:00</published><updated>2014-10-06T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-10-06:/blog/post/2014/10/06/kl-divergence-as-an-objective-function/</id><summary type="html">&lt;p&gt;It's well-known that
&lt;a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL-divergence&lt;/a&gt;
is not symmetric, but which direction is right for fitting your model?&lt;/p&gt;
&lt;h4&gt;Which KL is which? A cheat sheet&lt;/h4&gt;
&lt;p&gt;If we're fitting &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; to &lt;span class="math"&gt;\(p\)&lt;/span&gt; using&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mean-seeking, &lt;em&gt;inclusive&lt;/em&gt; (more principled because approximates the &lt;em&gt;full&lt;/em&gt; distribution)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;requires normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; (i.e., often &lt;em&gt;not&lt;/em&gt; computationally convenient)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mode-seeking, &lt;em&gt;exclusive&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;no normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; (i.e., computationally convenient)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mnemonic&lt;/strong&gt;: "When the truth comes first, you get the whole truth" (h/t
&lt;a href="https://www.umiacs.umd.edu/~resnik/"&gt;Philip Resnik&lt;/a&gt;). Here "whole truth"
corresponds to the &lt;em&gt;inclusiveness&lt;/em&gt; of &lt;span class="math"&gt;\(\textbf{KL}(p || q)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As far as remembering the equation, I pretend that "&lt;span class="math"&gt;\(||\)&lt;/span&gt;" is a division symbol,
which happens to correspond nicely to a division symbol in the equation (I'm not
sure it's intentional).&lt;/p&gt;
&lt;h2&gt;Inclusive vs. exclusive divergence&lt;/h2&gt;
&lt;div style="background-color: #f2f2f2; border: 2px solid #ggg; padding: 10px;"&gt;

&lt;img src="http://timvieira.github.io/blog/images/KL-inclusive-exclusive.png" /&gt;
Figure by &lt;a href="http://www.johnwinn.org/"&gt;John Winn&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Computational perspecive&lt;/h2&gt;
&lt;p&gt;Let's look at what's involved in fitting a model &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; in each
direction. In this section, I'll describe the gradient and pay special attention
to the issue of normalization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;: &lt;span class="math"&gt;\(p,q_\theta\)&lt;/span&gt; are probabilty distributions. &lt;span class="math"&gt;\(p = \bar{p} / Z_p\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is the normalization constant. Similarly for &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The easy direction &lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(q_\theta || p)
&amp;amp;= \sum_d q(d) \log \left( \frac{q(d)}{p(d)} \right) \\
&amp;amp;= \sum_d q(d) \left( \log q(d) - \log p(d) \right) \\
&amp;amp;= \underbrace{\sum_d q(d) \log q(d)}_{-\text{entropy}} - \underbrace{\sum_d q(d) \log p(d)}_{\text{cross-entropy}} \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's look at normalization of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, the entropy term is easy because there is no &lt;span class="math"&gt;\(p\)&lt;/span&gt; in it.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d q(d) \log p(d)
&amp;amp;= \sum_d q(d) \log (\bar{p}(d) / Z_p) \\
&amp;amp;= \sum_d q(d) \left( \log \bar{p}(d) - \log Z_p) \right) \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \sum_d q(d) \log Z_p \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \log Z_p
\end{align*}&lt;/div&gt;
&lt;p&gt;In this case, &lt;span class="math"&gt;\(-\log Z_p\)&lt;/span&gt; is an additive constant, which can be dropped because
we're optimizing.&lt;/p&gt;
&lt;p&gt;This leaves us with the following optimization problem:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \underset{\theta}{\text{argmin}}\, \textbf{KL}(q_\theta || p) \\
&amp;amp;\qquad = \underset{\theta}{\text{argmin}}\, \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d)
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's work out the gradient
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \nabla\left[ \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d) \right] \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \log q_\theta(d) \right] - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) \right) - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) - \log \bar{p}(d) \right) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( \log q_\theta(d) - \log \bar{p}(d) \right) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;We killed the one in the last equality because &lt;span class="math"&gt;\(\sum_d \nabla
\left[ q(d) \right] = \nabla \left[ \sum_d q(d) \right] = \nabla
\left[ 1 \right] = 0\)&lt;/span&gt;, for any &lt;span class="math"&gt;\(q\)&lt;/span&gt; which is a probability distribution.&lt;/p&gt;
&lt;p&gt;This direction is convenient because we don't need to normalize
&lt;span class="math"&gt;\(p\)&lt;/span&gt;. Unfortunately, the "easy" direction is nonconvex in general&amp;mdash;unlike
the "hard" direction, which (as we'll see shortly) is convex.&lt;/p&gt;
&lt;h3&gt;Harder direction &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(p || q_\theta)
&amp;amp;= \sum_d p(d) \log \left( \frac{p(d)}{q(d)} \right) \\
&amp;amp;= \sum_d p(d) \left( \log p(d) - \log q(d) \right) \\
&amp;amp;= \sum_d p(d) \log p(d) - \sum_d p(d) \log q(d) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Clearly the first term (entropy) won't matter if we're just trying optimize wrt
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. So, let's focus on the second term (cross-entropy).
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d p(d) \log q(d)
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \left( \log \bar{q}(d) - \log Z_q \right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log Z_q\right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \left( \log Z_q \right) \left( \frac{1}{Z_p} \sum_d \bar{p}(d)\right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \log Z_q
\end{align*}&lt;/div&gt;
&lt;p&gt;The gradient, when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family, is intuitive:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\nabla \left[ \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \right]
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \nabla \left[ \log \bar{q}(d) \right] - \nabla \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \phi_q(d) - \mathbb{E}_q \left[ \phi_q \right] \\
&amp;amp;= \mathbb{E}_p \left[ \phi_q \right] - \mathbb{E}_q \left[ \phi_q \right]
\end{align*}&lt;/div&gt;
&lt;p&gt;Why do we say this is hard to compute? Well, for most interesting models, we
can't compute &lt;span class="math"&gt;\(Z_p = \sum_d \bar{p}(d)\)&lt;/span&gt;. This is because &lt;span class="math"&gt;\(p\)&lt;/span&gt; is presumed to be a
complex model (e.g., the real world, an intricate factor graph, a complicated
Bayesian posterior). If we can't compute &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt;, it's highly unlikely that we can
compute another (nontrivial) integral under &lt;span class="math"&gt;\(\bar{p}\)&lt;/span&gt;, e.g., &lt;span class="math"&gt;\(\sum_d \bar{p}(d)
\log \bar{q}(d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Nonetheless, optimizing KL in this direction is still useful. Examples include:
expectation propagation, variational decoding, and maximum likelihood
estimation. In the case of maximum likelihood estimation, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the empirical
distribution, so technically you don't have to compute its normalizing constant,
but you do need samples from it, which can be just as hard to get as computing a
normalization constant.&lt;/p&gt;
&lt;p&gt;Optimization problem is &lt;em&gt;convex&lt;/em&gt; when &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; is an exponential
family&amp;mdash;i.e., for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; the &lt;em&gt;optimization&lt;/em&gt; problem is "easy." You can
think of maximum likelihood estimation (MLE) as a method which minimizes KL
divergence based on samples of &lt;span class="math"&gt;\(p\)&lt;/span&gt;. In this case, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the true data
distribution! The first term in the gradient is based on a sample instead of an
exact estimate (often called "observed feature counts"). The downside, of
course, is that computing &lt;span class="math"&gt;\(\mathbb{E}_p \left[ \phi_q \right]\)&lt;/span&gt; might not be
tractable or, for MLE, require tons of samples.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In many ways, optimizing exclusive KL makes no sense at all! Except for the
  fact that it's computable when inclusive KL is often not. Exclusive KL is
  generally regarded as "an approximation" to inclusive KL. This bias in this
  approximation can be quite large.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inclusive vs. exclusive is an important distinction: Inclusive divergences
  require &lt;span class="math"&gt;\(q &amp;gt; 0\)&lt;/span&gt; whenever &lt;span class="math"&gt;\(p &amp;gt; 0\)&lt;/span&gt; (i.e., no "false negatives"), whereas
  exclusive divergences favor a single mode (i.e., only a good fit around a that
  mode).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an exponential family, &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt; will be convex
  in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, no matter how complicated &lt;span class="math"&gt;\(p\)&lt;/span&gt; is, whereas &lt;span class="math"&gt;\(\textbf{KL}(q_\theta
  || p)\)&lt;/span&gt; is generally nonconvex (e.g., if &lt;span class="math"&gt;\(p\)&lt;/span&gt; is multimodal).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computing the value of either KL divergence requires normalization. However,
  in the "easy" (exclusive) direction, we can optimize KL without computing
  &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; (as it results in only an additive constant difference).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both directions of KL are special cases of
  &lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"&gt;&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence&lt;/a&gt;. For a
  unified account of both directions consider looking into &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;I'd like to thank the following people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/_shrdlu_"&gt;Ryan Cotterell&lt;/a&gt; for an email exchange which
  spawned this article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/adveisner"&gt;Jason Eisner&lt;/a&gt; for teaching me all this stuff.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/florian_shkurti"&gt;Florian Shkurti&lt;/a&gt; for a useful email
  discussion, which caugh a bug in my explanation of why inclusive KL is hard to
  compute/optimize.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/sjmielke"&gt;Sebastian Mielke&lt;/a&gt; for the suggesting the
  "inclusive vs. exclusive" figure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;It's well-known that
&lt;a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL-divergence&lt;/a&gt;
is not symmetric, but which direction is right for fitting your model?&lt;/p&gt;
&lt;h4&gt;Which KL is which? A cheat sheet&lt;/h4&gt;
&lt;p&gt;If we're fitting &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; to &lt;span class="math"&gt;\(p\)&lt;/span&gt; using&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mean-seeking, &lt;em&gt;inclusive&lt;/em&gt; (more principled because approximates the &lt;em&gt;full&lt;/em&gt; distribution)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;requires normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; (i.e., often &lt;em&gt;not&lt;/em&gt; computationally convenient)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mode-seeking, &lt;em&gt;exclusive&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;no normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; (i.e., computationally convenient)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mnemonic&lt;/strong&gt;: "When the truth comes first, you get the whole truth" (h/t
&lt;a href="https://www.umiacs.umd.edu/~resnik/"&gt;Philip Resnik&lt;/a&gt;). Here "whole truth"
corresponds to the &lt;em&gt;inclusiveness&lt;/em&gt; of &lt;span class="math"&gt;\(\textbf{KL}(p || q)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As far as remembering the equation, I pretend that "&lt;span class="math"&gt;\(||\)&lt;/span&gt;" is a division symbol,
which happens to correspond nicely to a division symbol in the equation (I'm not
sure it's intentional).&lt;/p&gt;
&lt;h2&gt;Inclusive vs. exclusive divergence&lt;/h2&gt;
&lt;div style="background-color: #f2f2f2; border: 2px solid #ggg; padding: 10px;"&gt;

&lt;img src="http://timvieira.github.io/blog/images/KL-inclusive-exclusive.png" /&gt;
Figure by &lt;a href="http://www.johnwinn.org/"&gt;John Winn&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Computational perspecive&lt;/h2&gt;
&lt;p&gt;Let's look at what's involved in fitting a model &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; in each
direction. In this section, I'll describe the gradient and pay special attention
to the issue of normalization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;: &lt;span class="math"&gt;\(p,q_\theta\)&lt;/span&gt; are probabilty distributions. &lt;span class="math"&gt;\(p = \bar{p} / Z_p\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is the normalization constant. Similarly for &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The easy direction &lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(q_\theta || p)
&amp;amp;= \sum_d q(d) \log \left( \frac{q(d)}{p(d)} \right) \\
&amp;amp;= \sum_d q(d) \left( \log q(d) - \log p(d) \right) \\
&amp;amp;= \underbrace{\sum_d q(d) \log q(d)}_{-\text{entropy}} - \underbrace{\sum_d q(d) \log p(d)}_{\text{cross-entropy}} \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's look at normalization of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, the entropy term is easy because there is no &lt;span class="math"&gt;\(p\)&lt;/span&gt; in it.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d q(d) \log p(d)
&amp;amp;= \sum_d q(d) \log (\bar{p}(d) / Z_p) \\
&amp;amp;= \sum_d q(d) \left( \log \bar{p}(d) - \log Z_p) \right) \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \sum_d q(d) \log Z_p \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \log Z_p
\end{align*}&lt;/div&gt;
&lt;p&gt;In this case, &lt;span class="math"&gt;\(-\log Z_p\)&lt;/span&gt; is an additive constant, which can be dropped because
we're optimizing.&lt;/p&gt;
&lt;p&gt;This leaves us with the following optimization problem:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \underset{\theta}{\text{argmin}}\, \textbf{KL}(q_\theta || p) \\
&amp;amp;\qquad = \underset{\theta}{\text{argmin}}\, \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d)
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's work out the gradient
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \nabla\left[ \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d) \right] \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \log q_\theta(d) \right] - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) \right) - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) - \log \bar{p}(d) \right) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( \log q_\theta(d) - \log \bar{p}(d) \right) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;We killed the one in the last equality because &lt;span class="math"&gt;\(\sum_d \nabla
\left[ q(d) \right] = \nabla \left[ \sum_d q(d) \right] = \nabla
\left[ 1 \right] = 0\)&lt;/span&gt;, for any &lt;span class="math"&gt;\(q\)&lt;/span&gt; which is a probability distribution.&lt;/p&gt;
&lt;p&gt;This direction is convenient because we don't need to normalize
&lt;span class="math"&gt;\(p\)&lt;/span&gt;. Unfortunately, the "easy" direction is nonconvex in general&amp;mdash;unlike
the "hard" direction, which (as we'll see shortly) is convex.&lt;/p&gt;
&lt;h3&gt;Harder direction &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(p || q_\theta)
&amp;amp;= \sum_d p(d) \log \left( \frac{p(d)}{q(d)} \right) \\
&amp;amp;= \sum_d p(d) \left( \log p(d) - \log q(d) \right) \\
&amp;amp;= \sum_d p(d) \log p(d) - \sum_d p(d) \log q(d) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Clearly the first term (entropy) won't matter if we're just trying optimize wrt
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. So, let's focus on the second term (cross-entropy).
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d p(d) \log q(d)
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \left( \log \bar{q}(d) - \log Z_q \right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log Z_q\right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \left( \log Z_q \right) \left( \frac{1}{Z_p} \sum_d \bar{p}(d)\right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \log Z_q
\end{align*}&lt;/div&gt;
&lt;p&gt;The gradient, when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family, is intuitive:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\nabla \left[ \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \right]
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \nabla \left[ \log \bar{q}(d) \right] - \nabla \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \phi_q(d) - \mathbb{E}_q \left[ \phi_q \right] \\
&amp;amp;= \mathbb{E}_p \left[ \phi_q \right] - \mathbb{E}_q \left[ \phi_q \right]
\end{align*}&lt;/div&gt;
&lt;p&gt;Why do we say this is hard to compute? Well, for most interesting models, we
can't compute &lt;span class="math"&gt;\(Z_p = \sum_d \bar{p}(d)\)&lt;/span&gt;. This is because &lt;span class="math"&gt;\(p\)&lt;/span&gt; is presumed to be a
complex model (e.g., the real world, an intricate factor graph, a complicated
Bayesian posterior). If we can't compute &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt;, it's highly unlikely that we can
compute another (nontrivial) integral under &lt;span class="math"&gt;\(\bar{p}\)&lt;/span&gt;, e.g., &lt;span class="math"&gt;\(\sum_d \bar{p}(d)
\log \bar{q}(d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Nonetheless, optimizing KL in this direction is still useful. Examples include:
expectation propagation, variational decoding, and maximum likelihood
estimation. In the case of maximum likelihood estimation, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the empirical
distribution, so technically you don't have to compute its normalizing constant,
but you do need samples from it, which can be just as hard to get as computing a
normalization constant.&lt;/p&gt;
&lt;p&gt;Optimization problem is &lt;em&gt;convex&lt;/em&gt; when &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; is an exponential
family&amp;mdash;i.e., for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; the &lt;em&gt;optimization&lt;/em&gt; problem is "easy." You can
think of maximum likelihood estimation (MLE) as a method which minimizes KL
divergence based on samples of &lt;span class="math"&gt;\(p\)&lt;/span&gt;. In this case, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the true data
distribution! The first term in the gradient is based on a sample instead of an
exact estimate (often called "observed feature counts"). The downside, of
course, is that computing &lt;span class="math"&gt;\(\mathbb{E}_p \left[ \phi_q \right]\)&lt;/span&gt; might not be
tractable or, for MLE, require tons of samples.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In many ways, optimizing exclusive KL makes no sense at all! Except for the
  fact that it's computable when inclusive KL is often not. Exclusive KL is
  generally regarded as "an approximation" to inclusive KL. This bias in this
  approximation can be quite large.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inclusive vs. exclusive is an important distinction: Inclusive divergences
  require &lt;span class="math"&gt;\(q &amp;gt; 0\)&lt;/span&gt; whenever &lt;span class="math"&gt;\(p &amp;gt; 0\)&lt;/span&gt; (i.e., no "false negatives"), whereas
  exclusive divergences favor a single mode (i.e., only a good fit around a that
  mode).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an exponential family, &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt; will be convex
  in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, no matter how complicated &lt;span class="math"&gt;\(p\)&lt;/span&gt; is, whereas &lt;span class="math"&gt;\(\textbf{KL}(q_\theta
  || p)\)&lt;/span&gt; is generally nonconvex (e.g., if &lt;span class="math"&gt;\(p\)&lt;/span&gt; is multimodal).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computing the value of either KL divergence requires normalization. However,
  in the "easy" (exclusive) direction, we can optimize KL without computing
  &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; (as it results in only an additive constant difference).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both directions of KL are special cases of
  &lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"&gt;&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence&lt;/a&gt;. For a
  unified account of both directions consider looking into &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;I'd like to thank the following people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/_shrdlu_"&gt;Ryan Cotterell&lt;/a&gt; for an email exchange which
  spawned this article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/adveisner"&gt;Jason Eisner&lt;/a&gt; for teaching me all this stuff.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/florian_shkurti"&gt;Florian Shkurti&lt;/a&gt; for a useful email
  discussion, which caugh a bug in my explanation of why inclusive KL is hard to
  compute/optimize.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/sjmielke"&gt;Sebastian Mielke&lt;/a&gt; for the suggesting the
  "inclusive vs. exclusive" figure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="statistics"></category><category term="machine-learning"></category></entry><entry><title>Complex-step derivative</title><link href="http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/" rel="alternate"></link><published>2014-08-07T00:00:00-04:00</published><updated>2014-08-07T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-08-07:/blog/post/2014/08/07/complex-step-derivative/</id><summary type="html">&lt;p&gt;Estimate derivatives by simply passing in a complex number to your function!&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Recall, the centered-difference approximation is a fairly accurate method for
approximating derivatives of a univariate function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, which only requires two
function evaluations. A similar derivation, based on the Taylor series expansion
with a complex perturbation, gives us a similarly-accurate approximation with a
single (complex) function evaluation instead of two (real-valued) function
evaluations. Note: &lt;span class="math"&gt;\(f\)&lt;/span&gt; must support complex inputs (in frameworks, such as numpy
or matlab, this often requires no modification to source code).&lt;/p&gt;
&lt;p&gt;This post is based on
&lt;a href="http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf"&gt;Martins+'03&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;: Start with the Taylor series approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x + i \cdot \varepsilon) =
  \frac{i^0 \varepsilon^0}{0!} f(x)
+ \frac{i^1 \varepsilon^1}{1!} f'(x)
+ \frac{i^2 \varepsilon^2}{2!} f''(x)
+ \frac{i^3 \varepsilon^3}{3!} f'''(x)
+ \cdots
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Take the imaginary part of both sides and solve for &lt;span class="math"&gt;\(f'(x)\)&lt;/span&gt;. Note: the &lt;span class="math"&gt;\(f\)&lt;/span&gt; and
&lt;span class="math"&gt;\(f''\)&lt;/span&gt; term disappear because &lt;span class="math"&gt;\(i^0\)&lt;/span&gt; and &lt;span class="math"&gt;\(i^2\)&lt;/span&gt; are real-valued.&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) = \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big] + \frac{\varepsilon^2}{3!} f'''(x) + \cdots
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;As usual, using a small &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; let's us throw out higher-order
terms. And, we arrive at the following approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;If instead, we take the real part and solve for &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, we get an approximation
to the function's value at &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) \approx \text{Re}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;In other words, a single (complex) function evaluations computes both the
function's value and the derivative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Higher-order function takes univariate function which computes a value and&lt;/span&gt;
&lt;span class="sd"&gt;    returns a function which returns value-derivative pair approximation.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;         &lt;span class="c1"&gt;# convert input to complex number&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imag&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# return function value and gradient&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simple test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="c1"&gt;# function&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# gradient&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Other comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using the complex-step method to estimate the gradients of multivariate
  functions requires independent approximations for each dimension of the
  input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although the complex-step approximation only requires a single function
  evaluation, it's unlikely faster than performing two function evaluations
  because operations on complex numbers are generally much slower than on floats
  or doubles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Check out the
&lt;a href="https://gist.github.com/timvieira/3d3db3e5e78e17cdd103"&gt;gist&lt;/a&gt; for this post.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Estimate derivatives by simply passing in a complex number to your function!&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Recall, the centered-difference approximation is a fairly accurate method for
approximating derivatives of a univariate function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, which only requires two
function evaluations. A similar derivation, based on the Taylor series expansion
with a complex perturbation, gives us a similarly-accurate approximation with a
single (complex) function evaluation instead of two (real-valued) function
evaluations. Note: &lt;span class="math"&gt;\(f\)&lt;/span&gt; must support complex inputs (in frameworks, such as numpy
or matlab, this often requires no modification to source code).&lt;/p&gt;
&lt;p&gt;This post is based on
&lt;a href="http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf"&gt;Martins+'03&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;: Start with the Taylor series approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x + i \cdot \varepsilon) =
  \frac{i^0 \varepsilon^0}{0!} f(x)
+ \frac{i^1 \varepsilon^1}{1!} f'(x)
+ \frac{i^2 \varepsilon^2}{2!} f''(x)
+ \frac{i^3 \varepsilon^3}{3!} f'''(x)
+ \cdots
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Take the imaginary part of both sides and solve for &lt;span class="math"&gt;\(f'(x)\)&lt;/span&gt;. Note: the &lt;span class="math"&gt;\(f\)&lt;/span&gt; and
&lt;span class="math"&gt;\(f''\)&lt;/span&gt; term disappear because &lt;span class="math"&gt;\(i^0\)&lt;/span&gt; and &lt;span class="math"&gt;\(i^2\)&lt;/span&gt; are real-valued.&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) = \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big] + \frac{\varepsilon^2}{3!} f'''(x) + \cdots
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;As usual, using a small &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; let's us throw out higher-order
terms. And, we arrive at the following approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;If instead, we take the real part and solve for &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, we get an approximation
to the function's value at &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) \approx \text{Re}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;In other words, a single (complex) function evaluations computes both the
function's value and the derivative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Higher-order function takes univariate function which computes a value and&lt;/span&gt;
&lt;span class="sd"&gt;    returns a function which returns value-derivative pair approximation.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;         &lt;span class="c1"&gt;# convert input to complex number&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imag&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# return function value and gradient&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simple test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="c1"&gt;# function&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# gradient&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Other comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using the complex-step method to estimate the gradients of multivariate
  functions requires independent approximations for each dimension of the
  input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although the complex-step approximation only requires a single function
  evaluation, it's unlikely faster than performing two function evaluations
  because operations on complex numbers are generally much slower than on floats
  or doubles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Check out the
&lt;a href="https://gist.github.com/timvieira/3d3db3e5e78e17cdd103"&gt;gist&lt;/a&gt; for this post.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="calculus"></category></entry><entry><title>Gumbel-max trick and weighted reservoir sampling</title><link href="http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/" rel="alternate"></link><published>2014-08-01T00:00:00-04:00</published><updated>2014-08-01T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-08-01:/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</id><summary type="html">&lt;p&gt;A while back, &lt;a href="http://people.cs.umass.edu/~wallach/"&gt;Hanna&lt;/a&gt; and I stumbled upon
the following blog post:
&lt;a href="http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm"&gt;Algorithms Every Data Scientist Should Know: Reservoir Sampling&lt;/a&gt;,
which got us excited about reservior sampling.&lt;/p&gt;
&lt;p&gt;Around the same time, I attended a talk by
&lt;a href="http://cs.haifa.ac.il/~tamir/"&gt;Tamir Hazan&lt;/a&gt; about some of his work on
perturb-and-MAP
&lt;a href="http://cs.haifa.ac.il/~tamir/papers/mean-width-icml12.pdf"&gt;(Hazan &amp;amp; Jaakkola, 2012)&lt;/a&gt;,
which is inspired by the
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;Gumbel-max-trick&lt;/a&gt;
(see &lt;a href="/blog/post/2014/07/31/gumbel-max-trick/"&gt;previous post&lt;/a&gt;). The apparent
similarity between weighted reservior sampling and the Gumbel-max trick lead us
to make some cute connections, which I'll describe in this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We're given a stream of unnormalized probabilities, &lt;span class="math"&gt;\(x_1,
x_2, \cdots\)&lt;/span&gt;. At any point in time &lt;span class="math"&gt;\(t\)&lt;/span&gt; we'd like to have a sampled index &lt;span class="math"&gt;\(i\)&lt;/span&gt;
available, where the probability of &lt;span class="math"&gt;\(i\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(\pi_t(i) = \frac{x_i}{
\sum_{j=1}^t x_j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume, without loss of generality, that &lt;span class="math"&gt;\(x_i &amp;gt; 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. (If any element
has a zero weight we can safely ignore it since it should never be sampled.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Gumbel-max sampler&lt;/strong&gt;: I came up with the following algorithm, which
is a simple "modification" of the Gumbel-max-trick for handling streaming data:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}  \ \ \text{# maximum value and index}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Compute log-unnormalized probabilities&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(w_i = \log(x_i)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(z_i \sim \text{Gumbel}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(k_i = w_i + z_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; (i.e. the argmax)&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If we interrupt this algorithm at any point, we have a sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After convincing myself this algorithm was correct, I sat down to try to
understand the algorithm in the blog post, which is due to Efraimidis and
Spirakis (2005) (&lt;a href="http://dl.acm.org/citation.cfm?id=1138834"&gt;paywall&lt;/a&gt;,
&lt;a href="http://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf"&gt;free summary&lt;/a&gt;). They
looked similar in many ways but used different sorting keys / perturbations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efraimidis and Spirakis (2005)&lt;/strong&gt;: Here is the ES algorithm for weighted
reservior sampling&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# compute randomized key&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(u_i \sim \text{Uniform}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(e_i = u_i^{(\frac{1}{x_i})}\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Again, if interrupt this algorithm at any point, we have our sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;. Note
that you can simplify &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; so that you don't have to compute pow (which is nice
because pow is pretty slow). It's equivalent to use &lt;span class="math"&gt;\(e'_i = \log(e_i) =
\log(u_i)/x_i\)&lt;/span&gt; because &lt;span class="math"&gt;\(\log\)&lt;/span&gt; is monotonic. (Note that &lt;span class="math"&gt;\(-e'_i \sim
\textrm{Exponential}(x_i)\)&lt;/span&gt;.)&lt;/p&gt;
&lt;!--
I find this version of the algorithm more intuitive, since it's well-known that
$\left(\underset{{i=1 \ldots t}}{\min} \textrm{Exponential}(x_i) \right) =
\textrm{Exponential}\left(\sum_{i=1}^t x_i \right)$. This version makes it clear
that minimizing is actually summing. However, we want the argmin, which is
distributed according to $\pi_t$.
--&gt;

&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: Let's try to relate these algorithms. At a high level, both
algorithms compute a randomized key and take an argmax. What's the relationship
between the keys?&lt;/p&gt;
&lt;p&gt;First, note that a &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; variate can be generated via
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1)))\)&lt;/span&gt;. This is a straightforward application of
the
&lt;a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling"&gt;inverse transform sampling&lt;/a&gt;
method for random number generation. This means that if we use the same sequence
of uniform random variates then, &lt;span class="math"&gt;\(z_i = -\log(-\log(u_i))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, this does not give use equality between &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;, but it does
turn out that &lt;span class="math"&gt;\(k_i = -\log(-\log(e_i))\)&lt;/span&gt;, which is useful because this is a
monotonic transformation on the interval &lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;. Since monotonic
transformations preserve ordering, the sequences &lt;span class="math"&gt;\(k\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; result in the same
comparison decisions, as well as, the same argmax. In summary, the algorithms
are the same!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extensions&lt;/strong&gt;: After reading a little further along in the ES paper, we see
that the same algorithm can be used to perform &lt;em&gt;sampling without replacement&lt;/em&gt; by
sorting and taking the elements with the highest keys. This same modification is
applicable to the Gumbel-max-trick because the keys have exactly the same
ordering as ES. In practice we don't sort the key, but instead use a bounded
priority queue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing&lt;/strong&gt;: To the best of my knowledge, the connection between the
Gumbel-max-trick and ES is undocumented. Furthermore, the Gumbel-max-trick is
not known as a streaming algorithm, much less known to perform sampling without
replacement! If you know of a reference let me know. Perhaps, we'll finish
turning these connections into a
&lt;a href="https://github.com/timvieira/gumbel"&gt;short tech report&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;A while back, &lt;a href="http://people.cs.umass.edu/~wallach/"&gt;Hanna&lt;/a&gt; and I stumbled upon
the following blog post:
&lt;a href="http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm"&gt;Algorithms Every Data Scientist Should Know: Reservoir Sampling&lt;/a&gt;,
which got us excited about reservior sampling.&lt;/p&gt;
&lt;p&gt;Around the same time, I attended a talk by
&lt;a href="http://cs.haifa.ac.il/~tamir/"&gt;Tamir Hazan&lt;/a&gt; about some of his work on
perturb-and-MAP
&lt;a href="http://cs.haifa.ac.il/~tamir/papers/mean-width-icml12.pdf"&gt;(Hazan &amp;amp; Jaakkola, 2012)&lt;/a&gt;,
which is inspired by the
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;Gumbel-max-trick&lt;/a&gt;
(see &lt;a href="/blog/post/2014/07/31/gumbel-max-trick/"&gt;previous post&lt;/a&gt;). The apparent
similarity between weighted reservior sampling and the Gumbel-max trick lead us
to make some cute connections, which I'll describe in this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We're given a stream of unnormalized probabilities, &lt;span class="math"&gt;\(x_1,
x_2, \cdots\)&lt;/span&gt;. At any point in time &lt;span class="math"&gt;\(t\)&lt;/span&gt; we'd like to have a sampled index &lt;span class="math"&gt;\(i\)&lt;/span&gt;
available, where the probability of &lt;span class="math"&gt;\(i\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(\pi_t(i) = \frac{x_i}{
\sum_{j=1}^t x_j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume, without loss of generality, that &lt;span class="math"&gt;\(x_i &amp;gt; 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. (If any element
has a zero weight we can safely ignore it since it should never be sampled.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Gumbel-max sampler&lt;/strong&gt;: I came up with the following algorithm, which
is a simple "modification" of the Gumbel-max-trick for handling streaming data:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}  \ \ \text{# maximum value and index}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Compute log-unnormalized probabilities&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(w_i = \log(x_i)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(z_i \sim \text{Gumbel}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(k_i = w_i + z_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; (i.e. the argmax)&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If we interrupt this algorithm at any point, we have a sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After convincing myself this algorithm was correct, I sat down to try to
understand the algorithm in the blog post, which is due to Efraimidis and
Spirakis (2005) (&lt;a href="http://dl.acm.org/citation.cfm?id=1138834"&gt;paywall&lt;/a&gt;,
&lt;a href="http://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf"&gt;free summary&lt;/a&gt;). They
looked similar in many ways but used different sorting keys / perturbations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efraimidis and Spirakis (2005)&lt;/strong&gt;: Here is the ES algorithm for weighted
reservior sampling&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# compute randomized key&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(u_i \sim \text{Uniform}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(e_i = u_i^{(\frac{1}{x_i})}\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Again, if interrupt this algorithm at any point, we have our sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;. Note
that you can simplify &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; so that you don't have to compute pow (which is nice
because pow is pretty slow). It's equivalent to use &lt;span class="math"&gt;\(e'_i = \log(e_i) =
\log(u_i)/x_i\)&lt;/span&gt; because &lt;span class="math"&gt;\(\log\)&lt;/span&gt; is monotonic. (Note that &lt;span class="math"&gt;\(-e'_i \sim
\textrm{Exponential}(x_i)\)&lt;/span&gt;.)&lt;/p&gt;
&lt;!--
I find this version of the algorithm more intuitive, since it's well-known that
$\left(\underset{{i=1 \ldots t}}{\min} \textrm{Exponential}(x_i) \right) =
\textrm{Exponential}\left(\sum_{i=1}^t x_i \right)$. This version makes it clear
that minimizing is actually summing. However, we want the argmin, which is
distributed according to $\pi_t$.
--&gt;

&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: Let's try to relate these algorithms. At a high level, both
algorithms compute a randomized key and take an argmax. What's the relationship
between the keys?&lt;/p&gt;
&lt;p&gt;First, note that a &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; variate can be generated via
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1)))\)&lt;/span&gt;. This is a straightforward application of
the
&lt;a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling"&gt;inverse transform sampling&lt;/a&gt;
method for random number generation. This means that if we use the same sequence
of uniform random variates then, &lt;span class="math"&gt;\(z_i = -\log(-\log(u_i))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, this does not give use equality between &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;, but it does
turn out that &lt;span class="math"&gt;\(k_i = -\log(-\log(e_i))\)&lt;/span&gt;, which is useful because this is a
monotonic transformation on the interval &lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;. Since monotonic
transformations preserve ordering, the sequences &lt;span class="math"&gt;\(k\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; result in the same
comparison decisions, as well as, the same argmax. In summary, the algorithms
are the same!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extensions&lt;/strong&gt;: After reading a little further along in the ES paper, we see
that the same algorithm can be used to perform &lt;em&gt;sampling without replacement&lt;/em&gt; by
sorting and taking the elements with the highest keys. This same modification is
applicable to the Gumbel-max-trick because the keys have exactly the same
ordering as ES. In practice we don't sort the key, but instead use a bounded
priority queue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing&lt;/strong&gt;: To the best of my knowledge, the connection between the
Gumbel-max-trick and ES is undocumented. Furthermore, the Gumbel-max-trick is
not known as a streaming algorithm, much less known to perform sampling without
replacement! If you know of a reference let me know. Perhaps, we'll finish
turning these connections into a
&lt;a href="https://github.com/timvieira/gumbel"&gt;short tech report&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="sampling"></category><category term="Gumbel"></category><category term="reservoir-sampling"></category></entry><entry><title>Gumbel-max trick</title><link href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/" rel="alternate"></link><published>2014-07-31T00:00:00-04:00</published><updated>2014-07-31T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-31:/blog/post/2014/07/31/gumbel-max-trick/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Sampling from a discrete distribution parametrized by unnormalized
log-probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_k = \frac{1}{z} \exp(x_k)   \ \ \ \text{where } z = \sum_{j=1}^K \exp(x_j)
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The usual way&lt;/strong&gt;: Exponentiate and normalize (using the
&lt;a href="/blog/post/2014/02/11/exp-normalize-trick/"&gt;exp-normalize trick&lt;/a&gt;), then use the
an algorithm for sampling from a discrete distribution (aka categorical):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;usual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;     &lt;span class="c1"&gt;# the exp-normalize trick&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The Gumbel-max trick&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \underset{ i \in \{1,\cdots,K\} }{\operatorname{argmax}} x_i + z_i
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_1 \cdots z_K\)&lt;/span&gt; are i.i.d. &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; random variates. It
turns out that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is distributed according to &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. (See the short derivations
in this
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;blog post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Implementing the Gumbel-max trick is remarkable easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gumbel_max_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gumbel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't have access to a Gumbel random variate generator, you can use
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gumbel-max requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; samples from a uniform. Usual requires only &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gumbel is one-pass because it does not require normalization (or a pass to
     compute the max for use in the exp-normalize trick). More on this in a
     later post!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Gumbel-max trick requires &lt;span class="math"&gt;\(2K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt;, whereas ordinary
     requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log\)&lt;/span&gt; are expensive
     function, we'd like to avoid calling them. What gives? Well, Gumbel's calls
     to &lt;span class="math"&gt;\(\log\)&lt;/span&gt; do not depend on the data so they can be precomputed; this is
     handy for implementations which rely on vectorization for efficiency,
     e.g. python+numpy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;: I have a few posts relating to the Gumbel-max trick. Have a
look at &lt;a href="/blog/tag/gumbel.html"&gt;posts tagged with Gumbel&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Sampling from a discrete distribution parametrized by unnormalized
log-probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_k = \frac{1}{z} \exp(x_k)   \ \ \ \text{where } z = \sum_{j=1}^K \exp(x_j)
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The usual way&lt;/strong&gt;: Exponentiate and normalize (using the
&lt;a href="/blog/post/2014/02/11/exp-normalize-trick/"&gt;exp-normalize trick&lt;/a&gt;), then use the
an algorithm for sampling from a discrete distribution (aka categorical):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;usual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;     &lt;span class="c1"&gt;# the exp-normalize trick&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The Gumbel-max trick&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \underset{ i \in \{1,\cdots,K\} }{\operatorname{argmax}} x_i + z_i
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_1 \cdots z_K\)&lt;/span&gt; are i.i.d. &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; random variates. It
turns out that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is distributed according to &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. (See the short derivations
in this
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;blog post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Implementing the Gumbel-max trick is remarkable easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gumbel_max_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gumbel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't have access to a Gumbel random variate generator, you can use
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gumbel-max requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; samples from a uniform. Usual requires only &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gumbel is one-pass because it does not require normalization (or a pass to
     compute the max for use in the exp-normalize trick). More on this in a
     later post!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Gumbel-max trick requires &lt;span class="math"&gt;\(2K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt;, whereas ordinary
     requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log\)&lt;/span&gt; are expensive
     function, we'd like to avoid calling them. What gives? Well, Gumbel's calls
     to &lt;span class="math"&gt;\(\log\)&lt;/span&gt; do not depend on the data so they can be precomputed; this is
     handy for implementations which rely on vectorization for efficiency,
     e.g. python+numpy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;: I have a few posts relating to the Gumbel-max trick. Have a
look at &lt;a href="/blog/tag/gumbel.html"&gt;posts tagged with Gumbel&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="sampling"></category><category term="Gumbel"></category></entry><entry><title>Rant against grid search</title><link href="http://timvieira.github.io/blog/post/2014/07/22/rant-against-grid-search/" rel="alternate"></link><published>2014-07-22T00:00:00-04:00</published><updated>2014-07-22T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-22:/blog/post/2014/07/22/rant-against-grid-search/</id><summary type="html">&lt;p&gt;Grid search is a simple and intuitive algorithm for optimizing and/or exploring
the effects of parameters to a function. However, given its rigid definition
grid search is susceptible to degenerate behavior. One type of unfortunate
behavior occurs in the presence of unimportant parameters, which results in many
(potentially expensive) function evaluations being wasted.&lt;/p&gt;
&lt;p&gt;This is a very simple point, but nonetheless I'll illustrate with a simple
example.&lt;/p&gt;
&lt;p&gt;Consider the following simple example, let's find the argmax of &lt;span class="math"&gt;\(f(x,y) = -x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we search over a &lt;span class="math"&gt;\(10\)&lt;/span&gt;-by-&lt;span class="math"&gt;\(10\)&lt;/span&gt; grid, resulting in a total of &lt;span class="math"&gt;\(100\)&lt;/span&gt;
function evaluations. For this function, we expect precision which proportional
of the number of samples in the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dimension, which is only &lt;span class="math"&gt;\(10\)&lt;/span&gt; samples! On
the other hand, randomly sampling points over the same space results in &lt;span class="math"&gt;\(100\)&lt;/span&gt;
samples in every dimension.&lt;/p&gt;
&lt;p&gt;In other words, randomly sample instead of using a rigid grid. If you have
points, which are not uniformly spaced, I'm willing to bet that an appropriate
probability distribution exists.&lt;/p&gt;
&lt;p&gt;This type of problem is common on hyperparameter optimizations. For futher
reading see
&lt;a href="http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;Bergstra &amp;amp; Bengio (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other thoughts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Local search is often much more effective. For example, gradient-based
   optimization, Nelder-Mead, stochastic local search, coordinate ascent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grid search tends to produce nicer-looking plots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What about variance in the results? Two things: (a) This is a concern for
   replicability, but is easily remedied by making sampled parameters
   available. (b) There is always some probability that the sampling gives you a
   terrible set of points. This shouldn't be a problem if you use enough
   samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Grid search is a simple and intuitive algorithm for optimizing and/or exploring
the effects of parameters to a function. However, given its rigid definition
grid search is susceptible to degenerate behavior. One type of unfortunate
behavior occurs in the presence of unimportant parameters, which results in many
(potentially expensive) function evaluations being wasted.&lt;/p&gt;
&lt;p&gt;This is a very simple point, but nonetheless I'll illustrate with a simple
example.&lt;/p&gt;
&lt;p&gt;Consider the following simple example, let's find the argmax of &lt;span class="math"&gt;\(f(x,y) = -x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we search over a &lt;span class="math"&gt;\(10\)&lt;/span&gt;-by-&lt;span class="math"&gt;\(10\)&lt;/span&gt; grid, resulting in a total of &lt;span class="math"&gt;\(100\)&lt;/span&gt;
function evaluations. For this function, we expect precision which proportional
of the number of samples in the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dimension, which is only &lt;span class="math"&gt;\(10\)&lt;/span&gt; samples! On
the other hand, randomly sampling points over the same space results in &lt;span class="math"&gt;\(100\)&lt;/span&gt;
samples in every dimension.&lt;/p&gt;
&lt;p&gt;In other words, randomly sample instead of using a rigid grid. If you have
points, which are not uniformly spaced, I'm willing to bet that an appropriate
probability distribution exists.&lt;/p&gt;
&lt;p&gt;This type of problem is common on hyperparameter optimizations. For futher
reading see
&lt;a href="http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;Bergstra &amp;amp; Bengio (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other thoughts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Local search is often much more effective. For example, gradient-based
   optimization, Nelder-Mead, stochastic local search, coordinate ascent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grid search tends to produce nicer-looking plots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What about variance in the results? Two things: (a) This is a concern for
   replicability, but is easily remedied by making sampled parameters
   available. (b) There is always some probability that the sampling gives you a
   terrible set of points. This shouldn't be a problem if you use enough
   samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="hyperparameter-optimization"></category></entry><entry><title>Expected value of a quadratic and the Delta method</title><link href="http://timvieira.github.io/blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/" rel="alternate"></link><published>2014-07-21T00:00:00-04:00</published><updated>2014-07-21T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-21:/blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Expected value of a quadratic&lt;/strong&gt;: Suppose we'd like to compute the expectation
of a quadratic function, i.e.,
&lt;span class="math"&gt;\(\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]\)&lt;/span&gt; , where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is
a random vector and &lt;span class="math"&gt;\(A\)&lt;/span&gt; is deterministic &lt;em&gt;symmetric&lt;/em&gt; matrix. Let &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; be the mean and variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. It turns out the expected value of a
quadratic has the following simple form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]
=
\text{trace}\left( A \Sigma \right) + \mu^{\top}\negthinspace A \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Delta Method&lt;/strong&gt;: Suppose we'd like to compute expected value of a nonlinear
function &lt;span class="math"&gt;\(f\)&lt;/span&gt; applied our random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\mathbb{E}\left[ f(x) \right]\)&lt;/span&gt;. The Delta method approximates this expection by
replacing &lt;span class="math"&gt;\(f\)&lt;/span&gt; by it's second-order Talylor approximation &lt;span class="math"&gt;\(\hat{f_{a}}\)&lt;/span&gt; taken at
some point &lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{f_{a}}(x) = f(a) + \nabla f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^\top H(a) (x - a)
$$&lt;/div&gt;
&lt;p&gt;The expectation of this Talyor approximation is a quadratic function! Let's try
to apply our new equation for the expected value of quadratic. We can use the
trick from above with &lt;span class="math"&gt;\(A=H(a)\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = (x-a)\)&lt;/span&gt;. Note, covariance matrix is shift
invariant and the Hessian is a symmetric matrix!&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\mathbb{E}\left[ \hat{f_{a}}(x) \right]
 &amp;amp; = \mathbb{E} \left[ f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) + \frac{1}{2} \mathbb{E} \left[ (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) +
   \frac{1}{2}\left( \text{trace}\left( H(a) \, \Sigma \right) + (\mu - a)^{\top} H(a)\, (\mu - a) \right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;It's common to take the Taylor expansion around &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. This simplifies the equation&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\mathbb{E}\left[ \hat{f_{\mu}} (x) \right]
&amp;amp;= \mathbb{E}\left[ f(\mu) + \nabla\negthinspace f(\mu) (x - \mu) + \frac{1}{2} (x - \mu)^{\top} H(\mu)\, (x - \mu) \right] \\\
&amp;amp;= f(\mu) + \frac{1}{2} \, \text{trace}\Big( H(\mu) \, \Sigma \Big)
\end{aligned}&lt;/div&gt;
&lt;p&gt;That looks much more tractable! Error bounds are possible to derive, but outside
to scope of this post. For a nice use of the delta method in machine learning
see &lt;a href="http://arxiv.org/pdf/1307.1493v2.pdf"&gt;(Wager+,'13)&lt;/a&gt; and
&lt;a href="http://cs.jhu.edu/~jason/papers/smith+eisner.acl06-risk.pdf"&gt;(Smith &amp;amp; Eisner,'06)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Expected value of a quadratic&lt;/strong&gt;: Suppose we'd like to compute the expectation
of a quadratic function, i.e.,
&lt;span class="math"&gt;\(\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]\)&lt;/span&gt; , where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is
a random vector and &lt;span class="math"&gt;\(A\)&lt;/span&gt; is deterministic &lt;em&gt;symmetric&lt;/em&gt; matrix. Let &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; be the mean and variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. It turns out the expected value of a
quadratic has the following simple form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]
=
\text{trace}\left( A \Sigma \right) + \mu^{\top}\negthinspace A \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Delta Method&lt;/strong&gt;: Suppose we'd like to compute expected value of a nonlinear
function &lt;span class="math"&gt;\(f\)&lt;/span&gt; applied our random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\mathbb{E}\left[ f(x) \right]\)&lt;/span&gt;. The Delta method approximates this expection by
replacing &lt;span class="math"&gt;\(f\)&lt;/span&gt; by it's second-order Talylor approximation &lt;span class="math"&gt;\(\hat{f_{a}}\)&lt;/span&gt; taken at
some point &lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{f_{a}}(x) = f(a) + \nabla f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^\top H(a) (x - a)
$$&lt;/div&gt;
&lt;p&gt;The expectation of this Talyor approximation is a quadratic function! Let's try
to apply our new equation for the expected value of quadratic. We can use the
trick from above with &lt;span class="math"&gt;\(A=H(a)\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = (x-a)\)&lt;/span&gt;. Note, covariance matrix is shift
invariant and the Hessian is a symmetric matrix!&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\mathbb{E}\left[ \hat{f_{a}}(x) \right]
 &amp;amp; = \mathbb{E} \left[ f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) + \frac{1}{2} \mathbb{E} \left[ (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) +
   \frac{1}{2}\left( \text{trace}\left( H(a) \, \Sigma \right) + (\mu - a)^{\top} H(a)\, (\mu - a) \right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;It's common to take the Taylor expansion around &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. This simplifies the equation&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\mathbb{E}\left[ \hat{f_{\mu}} (x) \right]
&amp;amp;= \mathbb{E}\left[ f(\mu) + \nabla\negthinspace f(\mu) (x - \mu) + \frac{1}{2} (x - \mu)^{\top} H(\mu)\, (x - \mu) \right] \\\
&amp;amp;= f(\mu) + \frac{1}{2} \, \text{trace}\Big( H(\mu) \, \Sigma \Big)
\end{aligned}&lt;/div&gt;
&lt;p&gt;That looks much more tractable! Error bounds are possible to derive, but outside
to scope of this post. For a nice use of the delta method in machine learning
see &lt;a href="http://arxiv.org/pdf/1307.1493v2.pdf"&gt;(Wager+,'13)&lt;/a&gt; and
&lt;a href="http://cs.jhu.edu/~jason/papers/smith+eisner.acl06-risk.pdf"&gt;(Smith &amp;amp; Eisner,'06)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="statistics"></category></entry><entry><title>Visualizing high-dimensional functions with cross-sections</title><link href="http://timvieira.github.io/blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/" rel="alternate"></link><published>2014-02-12T00:00:00-05:00</published><updated>2014-02-12T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-02-12:/blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</id><summary type="html">&lt;p&gt;Last September, I gave a talk which included a bunch of two-dimensional plots of
a high-dimensional objective I was developing specialized algorithms for
optimizing. A month later, at least three of my colleagues told me that my plots
had inspired them to make similar plots. The plotting trick is really simple and
not original, but nonetheless I'll still write it up for all to enjoy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example plot&lt;/strong&gt;: This image shows cross-sections of two related functions: a
non-smooth (black) and a smooth approximating function (blue). The plot shows
that the approximation is faithful to the overall shape, but sometimes
over-smooths. In this case, we miss the maximum, which happens near the middle
of the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/blog/images/cross-section.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Let &lt;span class="math"&gt;\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)&lt;/span&gt; be a high-dimensional
function (&lt;span class="math"&gt;\(d \gg 2\)&lt;/span&gt;), which you'd like to visualize. Unfortunately, you are like
me and can't see in high-dimensions what do you do?&lt;/p&gt;
&lt;p&gt;One simple thing to do is take a nonzero vector &lt;span class="math"&gt;\(\boldsymbol{d} \in
\mathbb{R}^d\)&lt;/span&gt;, take a point of interest &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;, and build a local
picture of &lt;span class="math"&gt;\(f\)&lt;/span&gt; by evaluating it at various intervals along the chosen direction
as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$
f_i = f(\boldsymbol{x} + \alpha_i \ \boldsymbol{d}) \ \ \text{for } \alpha_i \in [\alpha_\min, \alpha_\max]
$$&lt;/div&gt;
&lt;p&gt;Of course, you'll have to pick a reasonable range and discretize it. Note,
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; are fixed for all &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;. Now, you can
plot &lt;span class="math"&gt;\((\alpha_i,f_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Picking directions&lt;/strong&gt;: There are many alternatives for picking
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;, my favorites are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Coordinate vectors: Varying one (or two) dimensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gradient (if it exists), this direction is guaranteed to show a local
    increase/decrease in the objective, unless it's zero because we're at a
    local optimum. Some variations on "descent" directions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the gradient direction of a &lt;em&gt;different&lt;/em&gt; objective, e.g., plot
  (nondifferentiable) accuracy on dev data along the (differentiable)
  likelihood direction on training data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimizer trajectory: Use PCA on the optimizer's trajectory to find the
  directions which summarize the most variation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The difference of two interesting points, e.g., the start and end points of
    your optimization, two different solutions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random:&lt;/p&gt;
&lt;p&gt;If all your parameters are on an equal scale, I recommend directions drawn
from a spherical Gaussian.&lt;sup id="fnref-sphericalgaussian"&gt;&lt;a class="footnote-ref" href="#fn-sphericalgaussian"&gt;1&lt;/a&gt;&lt;/sup&gt; The reason being that such a
vector is uniformly distributed across all unit-length directions (i.e., the
angle of the vector, not it's length). We will vary the length ourselves via
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, often components of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; have different scales, so
finding a "natural scale" is crucial if we are going to draw conclusions
that require a comparison of the perturbation sensitivities across several
dimensions&amp;mdash;this is closely related to why we like second-order and
adaptive optimization algorithms
(&lt;a href="https://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/"&gt;discussion&lt;/a&gt;);
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;'s units must match the units of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; in each
coordinate!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximize "interestingness": You can also use a direction-optimization
    procedure to maximize some measure of "interestingness" (e.g., the direction
    in which training and dev loss differ the most; the "bumpiest" direction or
    direction taking the biggest range of values).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Extension to 3d&lt;/strong&gt;: It's pretty easy to extend these ideas to generating
three-dimensional plots by using two vectors, &lt;span class="math"&gt;\(\boldsymbol{d_1}\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\boldsymbol{d_2},\)&lt;/span&gt; and varying two parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{x} + \alpha \ \boldsymbol{d_1} + \beta \ \boldsymbol{d_2})
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Closing remarks&lt;/strong&gt;: These types of plots are probably best used to: empirically
verify/explore properties of an objective function, compare approximations, test
sensitivity to certain parameters/hyperparameters, visually debug optimization
algorithms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1712.09913"&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-sphericalgaussian"&gt;
&lt;p&gt;More formally, vectors drawn from a spherical Gaussian are
points uniformly distributed on the surface of a &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional unit sphere,
&lt;span class="math"&gt;\(\mathbb{S}^d\)&lt;/span&gt;. Sampling a vector from a spherical Gaussian is straightforward:
sample &lt;span class="math"&gt;\(\boldsymbol{d'} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\boldsymbol{d} = \boldsymbol{d'} / \| \boldsymbol{d'} \|_2\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-sphericalgaussian" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;Last September, I gave a talk which included a bunch of two-dimensional plots of
a high-dimensional objective I was developing specialized algorithms for
optimizing. A month later, at least three of my colleagues told me that my plots
had inspired them to make similar plots. The plotting trick is really simple and
not original, but nonetheless I'll still write it up for all to enjoy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example plot&lt;/strong&gt;: This image shows cross-sections of two related functions: a
non-smooth (black) and a smooth approximating function (blue). The plot shows
that the approximation is faithful to the overall shape, but sometimes
over-smooths. In this case, we miss the maximum, which happens near the middle
of the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/blog/images/cross-section.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Let &lt;span class="math"&gt;\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)&lt;/span&gt; be a high-dimensional
function (&lt;span class="math"&gt;\(d \gg 2\)&lt;/span&gt;), which you'd like to visualize. Unfortunately, you are like
me and can't see in high-dimensions what do you do?&lt;/p&gt;
&lt;p&gt;One simple thing to do is take a nonzero vector &lt;span class="math"&gt;\(\boldsymbol{d} \in
\mathbb{R}^d\)&lt;/span&gt;, take a point of interest &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;, and build a local
picture of &lt;span class="math"&gt;\(f\)&lt;/span&gt; by evaluating it at various intervals along the chosen direction
as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$
f_i = f(\boldsymbol{x} + \alpha_i \ \boldsymbol{d}) \ \ \text{for } \alpha_i \in [\alpha_\min, \alpha_\max]
$$&lt;/div&gt;
&lt;p&gt;Of course, you'll have to pick a reasonable range and discretize it. Note,
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; are fixed for all &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;. Now, you can
plot &lt;span class="math"&gt;\((\alpha_i,f_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Picking directions&lt;/strong&gt;: There are many alternatives for picking
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;, my favorites are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Coordinate vectors: Varying one (or two) dimensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gradient (if it exists), this direction is guaranteed to show a local
    increase/decrease in the objective, unless it's zero because we're at a
    local optimum. Some variations on "descent" directions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the gradient direction of a &lt;em&gt;different&lt;/em&gt; objective, e.g., plot
  (nondifferentiable) accuracy on dev data along the (differentiable)
  likelihood direction on training data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimizer trajectory: Use PCA on the optimizer's trajectory to find the
  directions which summarize the most variation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The difference of two interesting points, e.g., the start and end points of
    your optimization, two different solutions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random:&lt;/p&gt;
&lt;p&gt;If all your parameters are on an equal scale, I recommend directions drawn
from a spherical Gaussian.&lt;sup id="fnref-sphericalgaussian"&gt;&lt;a class="footnote-ref" href="#fn-sphericalgaussian"&gt;1&lt;/a&gt;&lt;/sup&gt; The reason being that such a
vector is uniformly distributed across all unit-length directions (i.e., the
angle of the vector, not it's length). We will vary the length ourselves via
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, often components of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; have different scales, so
finding a "natural scale" is crucial if we are going to draw conclusions
that require a comparison of the perturbation sensitivities across several
dimensions&amp;mdash;this is closely related to why we like second-order and
adaptive optimization algorithms
(&lt;a href="https://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/"&gt;discussion&lt;/a&gt;);
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;'s units must match the units of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; in each
coordinate!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximize "interestingness": You can also use a direction-optimization
    procedure to maximize some measure of "interestingness" (e.g., the direction
    in which training and dev loss differ the most; the "bumpiest" direction or
    direction taking the biggest range of values).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Extension to 3d&lt;/strong&gt;: It's pretty easy to extend these ideas to generating
three-dimensional plots by using two vectors, &lt;span class="math"&gt;\(\boldsymbol{d_1}\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\boldsymbol{d_2},\)&lt;/span&gt; and varying two parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{x} + \alpha \ \boldsymbol{d_1} + \beta \ \boldsymbol{d_2})
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Closing remarks&lt;/strong&gt;: These types of plots are probably best used to: empirically
verify/explore properties of an objective function, compare approximations, test
sensitivity to certain parameters/hyperparameters, visually debug optimization
algorithms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1712.09913"&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-sphericalgaussian"&gt;
&lt;p&gt;More formally, vectors drawn from a spherical Gaussian are
points uniformly distributed on the surface of a &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional unit sphere,
&lt;span class="math"&gt;\(\mathbb{S}^d\)&lt;/span&gt;. Sampling a vector from a spherical Gaussian is straightforward:
sample &lt;span class="math"&gt;\(\boldsymbol{d'} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\boldsymbol{d} = \boldsymbol{d'} / \| \boldsymbol{d'} \|_2\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-sphericalgaussian" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="visualization"></category></entry><entry><title>Exp-normalize trick</title><link href="http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/" rel="alternate"></link><published>2014-02-11T00:00:00-05:00</published><updated>2014-02-11T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-02-11:/blog/post/2014/02/11/exp-normalize-trick/</id><summary type="html">&lt;p&gt;This trick is the very close cousin of the infamous log-sum-exp trick
(&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html"&gt;scipy.misc.logsumexp&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Supposed you'd like to evaluate a probability distribution &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt;
parametrized by a vector &lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}^n\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i = \frac{ \exp(x_i) }{ \sum_{j=1}^n \exp(x_j) }
$$&lt;/div&gt;
&lt;p&gt;The exp-normalize trick leverages the following identity to avoid numerical
overflow. For any &lt;span class="math"&gt;\(b \in \mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i
= \frac{ \exp(x_i - b) \exp(b) }{ \sum_{j=1}^n \exp(x_j - b) \exp(b) }
= \frac{ \exp(x_i - b) }{ \sum_{j=1}^n \exp(x_j - b) }
$$&lt;/div&gt;
&lt;p&gt;In other words, the &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is shift-invariant. A reasonable choice
is &lt;span class="math"&gt;\(b = \max_{i=1}^n x_i\)&lt;/span&gt;. With this choice, overflow due to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; is
impossible&lt;span class="math"&gt;\(-\)&lt;/span&gt;the largest number exponentiated after shifting is &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The naive implementation is terrible when there are large numbers!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="ne"&gt;RuntimeWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;overflow&lt;/span&gt; &lt;span class="n"&gt;encountered&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="ne"&gt;RuntimeWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;invalid&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;encountered&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;true_divide&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nan&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The exp-normalize trick avoid this common problem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;exp_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;exp_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Log-sum-exp for computing the log-distibution&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log \pi_i = x_i - \mathrm{logsumexp}(\boldsymbol{x})
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{logsumexp}(\boldsymbol{x}) = b + \log \sum_{j=1}^n \exp(x_j - b)
$$&lt;/div&gt;
&lt;p&gt;Typically with the same choice for &lt;span class="math"&gt;\(b\)&lt;/span&gt; as above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exp-normalize v. log-sum-exp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Exp-normalize is the gradient of log-sum-exp. So you probably need to know both
tricks!&lt;/p&gt;
&lt;p&gt;If what you want to remain in log-space, that is, compute
&lt;span class="math"&gt;\(\log(\boldsymbol{\pi})\)&lt;/span&gt;, you should use logsumexp. However, if
&lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is your goal, then exp-normalize trick is for you! Since it
avoids additional calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;, which would be required if using log-sum-exp
and more importantly exp-normalize is more numerically stable!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numerically stable sigmoid function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sigmoid function can be computed with the exp-normalize trick in order to
avoid numerical overflow. In the case of &lt;span class="math"&gt;\(\text{sigmoid}(x)\)&lt;/span&gt;, we have a
distribution with unnormalized log probabilities &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt;, where we are only
interested in the probability of the first event. From the exp-normalize
identity, we know that the distributions &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt; and &lt;span class="math"&gt;\([0,-x]\)&lt;/span&gt; are equivalent (to
see why, plug in &lt;span class="math"&gt;\(b=\max(0,x)\)&lt;/span&gt;). This is why sigmoid is often expressed in one
of two equivalent ways:&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{sigmoid}(x) = 1/(1+\exp(-x)) = \exp(x) / (\exp(x) + 1)
$$&lt;/div&gt;
&lt;p&gt;Interestingly, each version covers an extreme case: &lt;span class="math"&gt;\(x=\infty\)&lt;/span&gt; and &lt;span class="math"&gt;\(x=-\infty\)&lt;/span&gt;,
respectively. Below is some python code which implements the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Numerically stable sigmoid function.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# if x is less than zero then z will be small, denom can&amp;#39;t be&lt;/span&gt;
        &lt;span class="c1"&gt;# zero because it&amp;#39;s 1+z.&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Closing remarks:&lt;/strong&gt; The exp-normalize distribution is also known as a
&lt;a href="https://en.wikipedia.org/wiki/Gibbs_measure"&gt;Gibbs measure&lt;/a&gt; (sometimes called a
Boltzmann distribution) when it is augmented with a temperature
parameter. Exp-normalize is often called "softmax," which is unfortunate because
log-sum-exp is &lt;em&gt;also&lt;/em&gt; called "softmax." However, unlike exp-normalize, it
&lt;em&gt;earned&lt;/em&gt; the name because it is acutally a soft version of the max function,
where as exp-normalize is closer to "soft argmax."  Nonetheless, most people
still call exp-normalize "softmax."&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;This trick is the very close cousin of the infamous log-sum-exp trick
(&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html"&gt;scipy.misc.logsumexp&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Supposed you'd like to evaluate a probability distribution &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt;
parametrized by a vector &lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}^n\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i = \frac{ \exp(x_i) }{ \sum_{j=1}^n \exp(x_j) }
$$&lt;/div&gt;
&lt;p&gt;The exp-normalize trick leverages the following identity to avoid numerical
overflow. For any &lt;span class="math"&gt;\(b \in \mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i
= \frac{ \exp(x_i - b) \exp(b) }{ \sum_{j=1}^n \exp(x_j - b) \exp(b) }
= \frac{ \exp(x_i - b) }{ \sum_{j=1}^n \exp(x_j - b) }
$$&lt;/div&gt;
&lt;p&gt;In other words, the &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is shift-invariant. A reasonable choice
is &lt;span class="math"&gt;\(b = \max_{i=1}^n x_i\)&lt;/span&gt;. With this choice, overflow due to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; is
impossible&lt;span class="math"&gt;\(-\)&lt;/span&gt;the largest number exponentiated after shifting is &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The naive implementation is terrible when there are large numbers!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="ne"&gt;RuntimeWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;overflow&lt;/span&gt; &lt;span class="n"&gt;encountered&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="ne"&gt;RuntimeWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;invalid&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;encountered&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;true_divide&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nan&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The exp-normalize trick avoid this common problem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;exp_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;exp_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Log-sum-exp for computing the log-distibution&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log \pi_i = x_i - \mathrm{logsumexp}(\boldsymbol{x})
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{logsumexp}(\boldsymbol{x}) = b + \log \sum_{j=1}^n \exp(x_j - b)
$$&lt;/div&gt;
&lt;p&gt;Typically with the same choice for &lt;span class="math"&gt;\(b\)&lt;/span&gt; as above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exp-normalize v. log-sum-exp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Exp-normalize is the gradient of log-sum-exp. So you probably need to know both
tricks!&lt;/p&gt;
&lt;p&gt;If what you want to remain in log-space, that is, compute
&lt;span class="math"&gt;\(\log(\boldsymbol{\pi})\)&lt;/span&gt;, you should use logsumexp. However, if
&lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is your goal, then exp-normalize trick is for you! Since it
avoids additional calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;, which would be required if using log-sum-exp
and more importantly exp-normalize is more numerically stable!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numerically stable sigmoid function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sigmoid function can be computed with the exp-normalize trick in order to
avoid numerical overflow. In the case of &lt;span class="math"&gt;\(\text{sigmoid}(x)\)&lt;/span&gt;, we have a
distribution with unnormalized log probabilities &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt;, where we are only
interested in the probability of the first event. From the exp-normalize
identity, we know that the distributions &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt; and &lt;span class="math"&gt;\([0,-x]\)&lt;/span&gt; are equivalent (to
see why, plug in &lt;span class="math"&gt;\(b=\max(0,x)\)&lt;/span&gt;). This is why sigmoid is often expressed in one
of two equivalent ways:&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{sigmoid}(x) = 1/(1+\exp(-x)) = \exp(x) / (\exp(x) + 1)
$$&lt;/div&gt;
&lt;p&gt;Interestingly, each version covers an extreme case: &lt;span class="math"&gt;\(x=\infty\)&lt;/span&gt; and &lt;span class="math"&gt;\(x=-\infty\)&lt;/span&gt;,
respectively. Below is some python code which implements the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Numerically stable sigmoid function.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# if x is less than zero then z will be small, denom can&amp;#39;t be&lt;/span&gt;
        &lt;span class="c1"&gt;# zero because it&amp;#39;s 1+z.&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Closing remarks:&lt;/strong&gt; The exp-normalize distribution is also known as a
&lt;a href="https://en.wikipedia.org/wiki/Gibbs_measure"&gt;Gibbs measure&lt;/a&gt; (sometimes called a
Boltzmann distribution) when it is augmented with a temperature
parameter. Exp-normalize is often called "softmax," which is unfortunate because
log-sum-exp is &lt;em&gt;also&lt;/em&gt; called "softmax." However, unlike exp-normalize, it
&lt;em&gt;earned&lt;/em&gt; the name because it is acutally a soft version of the max function,
where as exp-normalize is closer to "soft argmax."  Nonetheless, most people
still call exp-normalize "softmax."&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="numerical"></category></entry><entry><title>Gradient-vector product</title><link href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/" rel="alternate"></link><published>2014-02-10T00:00:00-05:00</published><updated>2014-02-10T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-02-10:/blog/post/2014/02/10/gradient-vector-product/</id><summary type="html">&lt;p&gt;We've all written the following test for our gradient code (known as the
finite-difference approximation).&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial x_i} f(\boldsymbol{x}) \approx
 \frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{e_i})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{e_i})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt; is a vector of zeros except at
&lt;span class="math"&gt;\(i\)&lt;/span&gt; where it is &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This approximation is exact in the limit, and accurate to
&lt;span class="math"&gt;\(o(\varepsilon^2)\)&lt;/span&gt; additive error.&lt;/p&gt;
&lt;p&gt;This is a specific instance of a more general approximation! The dot product of
the gradient and any (conformable) vector &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; can be approximated
with the following formula,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla f(\boldsymbol{x})^{\top} \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;We get the special case above when &lt;span class="math"&gt;\(\boldsymbol{d}=\boldsymbol{e_i}\)&lt;/span&gt;. This also
exact in the limit and just as accurate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Runtime?&lt;/strong&gt; Finite-difference approximation is probably too slow for
  approximating a high-dimensional gradient because the number of function
  evaluations required is &lt;span class="math"&gt;\(2 n\)&lt;/span&gt; where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the dimensionality of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. However,
  if the end goal is to approximate a gradient-vector product, a mere &lt;span class="math"&gt;\(2\)&lt;/span&gt;
  function evaluations is probably faster than specialized code for computing
  the gradient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to set &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;?&lt;/strong&gt; The second approach is more sensitive to
  &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; because &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; is arbitrary, unlike
  &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt;, which is a simple unit-norm vector. Luckily some guidance
  is available. Andrei (2009) reccommends&lt;/p&gt;
&lt;div class="math"&gt;$$
\varepsilon = \sqrt{\epsilon_{\text{mach}}} (1 + \|\boldsymbol{x} \|_{\infty}) / \| \boldsymbol{d} \|_{\infty}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\epsilon_{\text{mach}}\)&lt;/span&gt; is
&lt;a href="http://en.wikipedia.org/wiki/Machine_epsilon"&gt;machine epsilon&lt;/a&gt;. (Numpy users:
&lt;code&gt;numpy.finfo(x.dtype).eps&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Why do I care?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Well, I tend to work on sparse, but high-dimensional problems where
   finite-difference would be too slow. Thus, my usual solution is to only test
   several randomly selected dimensions&lt;span class="math"&gt;\(-\)&lt;/span&gt;biasing samples toward dimensions
   which should be nonzero. With the new trick, I can effectively test more
   dimensions at once by taking random vectors &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;. I recommend
   sampling &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; from a spherical Gaussian so that we're uniform on
   the angle of the vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sometimes the gradient-vector dot product is the end goal. This is the case
   with Hessian-vector products, which arises in many optimization algorithms,
   such as stochastic meta descent. Hessian-vector products are an instance of
   the gradient-vector dot product because the Hessian is just the gradient of
   the gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Hessian-vector product&lt;/h2&gt;
&lt;p&gt;Hessian-vector products are an instance of the gradient-vector dot product
because since the Hessian is just the gradient of the gradient! Now you only
need to remember one formula!&lt;/p&gt;
&lt;div class="math"&gt;$$
H(\boldsymbol{x})\, \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
  \nabla f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
- \nabla f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
\Big)
$$&lt;/div&gt;
&lt;p&gt;With this trick you never have to actually compute the gnarly Hessian! More on
&lt;a href="http://justindomke.wordpress.com/2009/01/17/hessian-vector-products/"&gt;Justin Domke's blog&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;We've all written the following test for our gradient code (known as the
finite-difference approximation).&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial x_i} f(\boldsymbol{x}) \approx
 \frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{e_i})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{e_i})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt; is a vector of zeros except at
&lt;span class="math"&gt;\(i\)&lt;/span&gt; where it is &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This approximation is exact in the limit, and accurate to
&lt;span class="math"&gt;\(o(\varepsilon^2)\)&lt;/span&gt; additive error.&lt;/p&gt;
&lt;p&gt;This is a specific instance of a more general approximation! The dot product of
the gradient and any (conformable) vector &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; can be approximated
with the following formula,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla f(\boldsymbol{x})^{\top} \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;We get the special case above when &lt;span class="math"&gt;\(\boldsymbol{d}=\boldsymbol{e_i}\)&lt;/span&gt;. This also
exact in the limit and just as accurate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Runtime?&lt;/strong&gt; Finite-difference approximation is probably too slow for
  approximating a high-dimensional gradient because the number of function
  evaluations required is &lt;span class="math"&gt;\(2 n\)&lt;/span&gt; where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the dimensionality of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. However,
  if the end goal is to approximate a gradient-vector product, a mere &lt;span class="math"&gt;\(2\)&lt;/span&gt;
  function evaluations is probably faster than specialized code for computing
  the gradient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to set &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;?&lt;/strong&gt; The second approach is more sensitive to
  &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; because &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; is arbitrary, unlike
  &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt;, which is a simple unit-norm vector. Luckily some guidance
  is available. Andrei (2009) reccommends&lt;/p&gt;
&lt;div class="math"&gt;$$
\varepsilon = \sqrt{\epsilon_{\text{mach}}} (1 + \|\boldsymbol{x} \|_{\infty}) / \| \boldsymbol{d} \|_{\infty}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\epsilon_{\text{mach}}\)&lt;/span&gt; is
&lt;a href="http://en.wikipedia.org/wiki/Machine_epsilon"&gt;machine epsilon&lt;/a&gt;. (Numpy users:
&lt;code&gt;numpy.finfo(x.dtype).eps&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Why do I care?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Well, I tend to work on sparse, but high-dimensional problems where
   finite-difference would be too slow. Thus, my usual solution is to only test
   several randomly selected dimensions&lt;span class="math"&gt;\(-\)&lt;/span&gt;biasing samples toward dimensions
   which should be nonzero. With the new trick, I can effectively test more
   dimensions at once by taking random vectors &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;. I recommend
   sampling &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; from a spherical Gaussian so that we're uniform on
   the angle of the vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sometimes the gradient-vector dot product is the end goal. This is the case
   with Hessian-vector products, which arises in many optimization algorithms,
   such as stochastic meta descent. Hessian-vector products are an instance of
   the gradient-vector dot product because the Hessian is just the gradient of
   the gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Hessian-vector product&lt;/h2&gt;
&lt;p&gt;Hessian-vector products are an instance of the gradient-vector dot product
because since the Hessian is just the gradient of the gradient! Now you only
need to remember one formula!&lt;/p&gt;
&lt;div class="math"&gt;$$
H(\boldsymbol{x})\, \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
  \nabla f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
- \nabla f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
\Big)
$$&lt;/div&gt;
&lt;p&gt;With this trick you never have to actually compute the gnarly Hessian! More on
&lt;a href="http://justindomke.wordpress.com/2009/01/17/hessian-vector-products/"&gt;Justin Domke's blog&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="calculus"></category></entry></feed>