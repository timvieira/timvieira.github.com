<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Graduate Descent</title><link href="http://timvieira.github.io/blog/" rel="alternate"></link><link href="/blog/atom.xml" rel="self"></link><id>http://timvieira.github.io/blog/</id><updated>2016-10-01T00:00:00-04:00</updated><entry><title>Reversing a sequence with sublinear space</title><link href="http://timvieira.github.io/blog/post/2016/10/01/reversing-a-sequence-with-sublinear-space/" rel="alternate"></link><published>2016-10-01T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-10-01:blog/post/2016/10/01/reversing-a-sequence-with-sublinear-space/</id><summary type="html">&lt;p&gt;Suppose we have a computation, which generates sequence of states &lt;span class="math"&gt;\(s_1 \ldots
s_n\)&lt;/span&gt;, such that &lt;span class="math"&gt;\(s_{t} = f(s_{t-1})\)&lt;/span&gt; and &lt;span class="math"&gt;\(s_0\)&lt;/span&gt; is set deterministically.&lt;/p&gt;
&lt;p&gt;So, producing the initial sequence takes &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt; time. If all we care
about is the final state, then we can do it in &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt; space.&lt;/p&gt;
&lt;p&gt;Now, we'd like to devise an algorithm, which can reconstruct each point in the
sequence efficiently as we traverse it backwards. You can think of this as
"hitting undo" from the end of the sequence or reversing a singly-liked list.&lt;/p&gt;
&lt;p&gt;Obviously, we &lt;em&gt;could&lt;/em&gt; just record the entire sequence, but if &lt;span class="math"&gt;\(n\)&lt;/span&gt; is large &lt;em&gt;or&lt;/em&gt;
the size of each state is large, this will be infeasible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 0&lt;/strong&gt;: Rerun the forward pass &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. Runtime &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt;, space
  &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 1&lt;/strong&gt;: Suppose we save &lt;span class="math"&gt;\(k \le n\)&lt;/span&gt; states evenly spaced "checkpoints"?
Clearly, this gives us &lt;span class="math"&gt;\(\mathcal{O}(k)\)&lt;/span&gt; space, but what does it do to
the runtime?  Well, if we are at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; the we have to "replay" computation
from the last recorded checkpoint to get &lt;span class="math"&gt;\(s_t\)&lt;/span&gt;, which takes &lt;span class="math"&gt;\(O(n/k)\)&lt;/span&gt; time. Thus,
the overall runtimes becomes &lt;span class="math"&gt;\(O(n^2/k)\)&lt;/span&gt;. This runtime is not ideal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: &lt;em&gt;Idea 1&lt;/em&gt; did something kind of silly, within a chunk of size &lt;span class="math"&gt;\(n/k\)&lt;/span&gt;,
it does each computation multiple times! Suppose we increase the memory
requirement &lt;em&gt;just a little bit&lt;/em&gt; to remember the current chunk we're working on,
making it now &lt;span class="math"&gt;\(\mathcal{O}(k + n/k)\)&lt;/span&gt;. Now, we compute each state at most &lt;span class="math"&gt;\(2\)&lt;/span&gt;
times: once in the initial sequence and once in the reverse. This implies a
&lt;em&gt;linear&lt;/em&gt; runtime.  Now, the question: how should we set &lt;span class="math"&gt;\(k\)&lt;/span&gt; so that we minimize
extra space? Easy! Solve the following little optimization problem:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{k}{\textrm{argmin}}\ k+n/k = \sqrt{n}
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation" style="display:none;" class="derivation"&gt;
To get the minimum, we solve for &lt;span class="math"&gt;\(k\)&lt;/span&gt; that sets the derivative to zero.
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
    0 &amp;amp;=&amp;amp; \frac{\partial}{\partial k} \left[ k+n/k \right] \\
      &amp;amp;=&amp;amp; 1-n/k^2 \\
n/k^2 &amp;amp;=&amp;amp; 1 \\
  k^2 &amp;amp;=&amp;amp; n \\
    k &amp;amp;=&amp;amp; \sqrt{n}
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Since it's safe to assume that &lt;span class="math"&gt;\(n,k \ge 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial^2}{\partial k\,
\partial k} = 2 n / k^3 &amp;gt; 0\)&lt;/span&gt; this is indeed a minimum. It's also global minimum
because &lt;span class="math"&gt;\(k+n/k\)&lt;/span&gt; is convex in &lt;span class="math"&gt;\(k\)&lt;/span&gt; when &lt;span class="math"&gt;\(n,k &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;That's nuts! We get away with &lt;em&gt;sublinear&lt;/em&gt; space &lt;span class="math"&gt;\(\mathcal{O}(\sqrt{n})\)&lt;/span&gt; and we
only blow up our runtime by a factor of 2. Also, I really love the "introduce a
parameter then optimize it out" trick.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sqrt_space&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# last chunk may be shorter than k.&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
            &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Take `k` steps from state `s`, save path. Cost: O(k) space, O(k) time.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Idea 3&lt;/strong&gt;: What if we apply "the remember &lt;span class="math"&gt;\(k\)&lt;/span&gt; states" trick &lt;em&gt;recursively&lt;/em&gt;? I'm
going to work this out for &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; (and then claim that the value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; doesn't
matter).&lt;/p&gt;
&lt;p&gt;Run forward to get the midpoint at &lt;span class="math"&gt;\(s_{m}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(m=b + \lfloor n/2
\rfloor\)&lt;/span&gt;. Next, recurse on the left and right chunks &lt;code&gt;[b:m]&lt;/code&gt; and &lt;code&gt;[m:e]&lt;/code&gt; (using
Python slice syntax). We hit the base case when the width of the interval is
one.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# do O(n/2) work to find the midpoint with O(1) space.&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that we implicitly store midpoints as we recurse (thanks to the stack
frame).  The max depth of the recursion is &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;, which gives us
a &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; space bound.&lt;/p&gt;
&lt;p&gt;We can characterize runtime with the following recurrence relation, &lt;span class="math"&gt;\(T(n) = 2
\cdot T(n/2) + \mathcal{O}(n)\)&lt;/span&gt;. Since we recognize this as the recurrence for
mergesort, we know that it flattens to &lt;span class="math"&gt;\(\mathcal{O}(n \log n)\)&lt;/span&gt; time. Also, just
like in the case of sorting, the branching factor doesn't matter so we're happy
with or initial assumption that &lt;span class="math"&gt;\(k=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;The algorithms describe in this post are generic algorithmic tricks, which has
been used in a number of place, including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The classic computer science interview problem of reversing a singly-linked list
  under a tight budget on &lt;em&gt;additional&lt;/em&gt; memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backpropagation for computing gradients in sequence models, including HMMs
  (&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2000/01/icslp00_logspace.pdf"&gt;Zweig &amp;amp; Padmanabhan, 2000&lt;/a&gt;)
  and RNNs (&lt;a href="https://arxiv.org/abs/1604.06174v2"&gt;Chen et al., 2016&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory-efficient &lt;a href="https://arxiv.org/pdf/cs/0310016v1"&gt;omniscient debugging&lt;/a&gt;,
  which allows a user to inspect program state while moving forward &lt;em&gt;and
  backward&lt;/em&gt; in time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's the
&lt;a href="https://gist.github.com/timvieira/d2ac72ec3af7972d2471035011cbf1e2"&gt;code gist&lt;/a&gt;,
complete with test cases.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="algorithms"></category></entry><entry><title>Evaluating âˆ‡f(x) is as fast as f(x)</title><link href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/" rel="alternate"></link><published>2016-09-25T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-09-25:blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/</id><summary type="html">&lt;p&gt;Automatic differentiation ('autodiff' or 'backprop') is great&amp;mdash;not just
because it makes it easy to rapidly prototype deep networks with plenty of
doodads and geegaws, but because it means that evaluating the gradient &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; is as fast of computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;. In fact, the gradient provably requires at
most a &lt;em&gt;small&lt;/em&gt; constant factor more arithmetic operations than the function
itself.  Furthermore, autodiff tells us how to derive and implement the gradient
efficiently. This is a fascinating result that is perhaps not emphasized enough
in machine learning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The gradient should never be asymptotically slower than the function.&lt;/strong&gt; In my
recent &lt;a href="/doc/2016-emnlp-vocrf.pdf"&gt;EMNLP'16 paper&lt;/a&gt;, my coauthors and I found a
line of work on variable-order CRFs
(&lt;a href="https://papers.nips.cc/paper/3815-conditional-random-fields-with-high-order-features-for-sequence-labeling.pdf"&gt;Ye+'09&lt;/a&gt;;
&lt;a href="http://www.jmlr.org/papers/volume15/cuong14a/cuong14a.pdf"&gt;Cuong+'14&lt;/a&gt;), which
had an unnecessarily slow and complicated algorithm for computing gradients,
which was asymptotically (and practically) slower than their forward
algorithm. Without breaking a sweat, we derived a simpler and more efficient
gradient algorithm by simply applying backprop to the forward algorithm (and
made some other contributions).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Many algorithms are just backprop.&lt;/strong&gt; For example, forward-backward and
inside-outside, are actually just instances of automatic differentiation
(&lt;a href="https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf"&gt;Eisner,'16&lt;/a&gt;) (i.e.,
outside is just backprop on inside). This shouldn't be a surprise because these
algorithms are used to compute gradients. Basically, if you know backprop and
the inside algorithm, then you can derive the outside algorithm by applying the
backprop transform manually. I find it easier to understand the outside
algorithm via its connection to backprop, then via
&lt;a href="https://www.cs.jhu.edu/~jason/465/iobasics.pdf"&gt;the usual presentation&lt;/a&gt;. Note
that inside-outside and forward-backward pre-date backpropagation and have
additional uses beyond computing gradients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once you've grokked backprop, the world is your oyster!&lt;/strong&gt; You can backprop
through many approximate inference algorithms, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v15/stoyanov11a/stoyanov11a.pdf"&gt;Stoyanov+'11&lt;/a&gt;
and many of Justin Domke's papers, to avoid issues I've mentioned
&lt;a href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/"&gt;before&lt;/a&gt;. You
can even backprop through optimization algorithms to get gradients of dev loss wrt
hyperparameters, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf"&gt;Domke'12&lt;/a&gt; and
&lt;a href="https://arxiv.org/abs/1502.03492"&gt;Maclaurin+'15&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's at least one catch!&lt;/strong&gt; Although the &lt;em&gt;time&lt;/em&gt; complexity of computing the
gradient is as good as the function, the &lt;em&gt;space&lt;/em&gt; complexity may be much larger
because the autodiff recipe (at least the default reverse-mode one) requires memoizing
all intermediate quantities (e.g., the quantities you overwrite in a
loop). There are generic methods for balancing the time-space tradeoff in
autodiff, since you can (at least in theory) reconstruct the intermediate
quantities by playing the forward computation again from intermediate
checkpoints (at a cost to runtime, of course). A recent example is
&lt;a href="https://arxiv.org/abs/1606.03401"&gt;Gruslys+'16&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A final remark&lt;/strong&gt;. Despite the name "automatic" differentiation, there is no
need to rely on software to "automatically" give you gradient routines. Applying
the backprop transformation is generally easy to do manually and sometimes more
efficient than using a library. Many autodiff libraries lack good support for
dynamic computation graph, i.e., when the structure depends on quantities that
vary with the input (e.g., sentence length).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="calculus"></category><category term="autodiff"></category><category term="rant"></category></entry><entry><title>Fast sigmoid sampling</title><link href="http://timvieira.github.io/blog/post/2016/07/04/fast-sigmoid-sampling/" rel="alternate"></link><published>2016-07-04T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-07-04:blog/post/2016/07/04/fast-sigmoid-sampling/</id><summary type="html">&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we describe a simple trick for efficiently sampling a Bernoulli random variable $Y$ from a sigmoid-defined distribution, $p(Y = 1) = (1 + \exp(-x))^{-1}$, where $x \in \mathbb{R}$ is the only parameter of the distribution ($x$ is often defined as the dot product of features and weights).&lt;/p&gt;
&lt;p&gt;The "slow" method for sampling from a sigmoid,&lt;/p&gt;
$$
u \sim \textrm{Uniform}(0,1)
$$$$
Y = sigmoid(x) &gt; u
$$&lt;p&gt;This method is slow because it calls the sigmoid function for every value of $x$. It is slow because $\exp$ is 2-3x slower than basic arithmetic operations.&lt;/p&gt;
&lt;p&gt;In this post, I'll describe a simple trick, which is well-suited to vectorized computations (e.g., numpy, matlab). The way it works is by &lt;em&gt;precomputing&lt;/em&gt; the expensive stuff (i.e., calls to expensive functions like $\exp$).&lt;/p&gt;
$$
sigmoid(x) &gt; u \Leftrightarrow logit(sigmoid(x)) &gt; logit(u) \Leftrightarrow x &gt; logit(u).
$$&lt;p&gt;Some details worth mentioning: (a) &lt;a href="https://en.wikipedia.org/wiki/Logit"&gt;logit&lt;/a&gt; is the inverse of sigmoid and (b) logit is strictly monotonic increasing you can apply it both sides of the greater than and preserves ordering (there's a plot in the appendix).&lt;/p&gt;
&lt;p&gt;The "fast" method derives it's advantage by leveraging the fact that expensive computation can be done independently of the data (i.e., specific values of $x$). The fast method is also interesting as just cute math. In the bonus section of this post, we'll make a connection to the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel max trick&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How fast is it in practice?&lt;/strong&gt; Below, we run a quick experiment to test that the method is correct and how fast it is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.special&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;expit&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.timer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;    &lt;span class="c1"&gt;# https://github.com/timvieira/arsenal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# These are the sigmoid parameters we&amp;#39;re going to sample from.&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# number of runs to average over.&lt;/span&gt;
&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="c1"&gt;# Used for plotting average p(Y=1)&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Temporary array for saving on memory allocation, cf. method slow-2.&lt;/span&gt;
&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# Let&amp;#39;s use the same random variables for all methods. This allows &lt;/span&gt;
    &lt;span class="c1"&gt;# for a lower variance comparsion and equivalence testing.&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# used in fast method: precompute expensive stuff.&lt;/span&gt;

    &lt;span class="c1"&gt;# Requires computing sigmoid for each x.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slow1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;           
        
    &lt;span class="c1"&gt;# Avoid memory allocation in slow-1 by using the out option to sigmoid&lt;/span&gt;
    &lt;span class="c1"&gt;# function. It&amp;#39;s a little bit faster than slow-1.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slow2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;           
        &lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;

    &lt;span class="c1"&gt;# Rolling our sigmoid is a bit slower than using the library function.&lt;/span&gt;
    &lt;span class="c1"&gt;# Not to mention this implementation isn&amp;#39;t as numerically stable.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slow3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;
        
    &lt;span class="c1"&gt;# The fast method.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;
    
    &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;    
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;fast is 28.4239x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.00114061 fast: 4.01285e-05)&lt;/span&gt;
slow2 is 1.0037x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.00114061 slow2: 0.0011364)&lt;/span&gt;
slow1 is 1.0840x faster than slow3 &lt;span class="ansi-yellow-fg"&gt;(avg: slow3: 0.00123637 slow1: 0.00114061)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;It looks like our trick is about $28$x faster than the fastest competing slow method!&lt;/p&gt;
&lt;p&gt;We also see that the assert statements passed, which means that the methods tested produce precisely the same samples.&lt;/p&gt;
&lt;p&gt;The final plot demonstrates that we get the right expected value (red curve) as we sweep the distributions parameter (x-axis).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Bonus"&gt;Bonus&lt;a class="anchor-link" href="#Bonus"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We could alternatively use the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel max trick&lt;/a&gt; to derive a similar algorithm. If we ground out the trick for a sigmoid instead of a general mutlinomal distributions, we end up with&lt;/p&gt;
$$
Z_0 \sim \textrm{Gumbel}(0,1)
$$$$
Z_1 \sim \textrm{Gumbel}(0,1)
$$$$
Y = x &gt; Z_0 - Z_1
$$&lt;p&gt;Much like our new trick, this one benefits from the fact that all expensive stuff is done independent of the data (i.e., the value of $x$). However, it seems silly that we "need" to generate &lt;em&gt;two&lt;/em&gt; Gumbel RVs to get one sample from the sigmoid. With a little bit of Googling, we discover that the difference of $\textrm{Gumbel}(0,1)$ RVs is a &lt;a href="https://en.wikipedia.org/wiki/Logistic_distribution"&gt;logistic&lt;/a&gt; RV (specifically $\textrm{Logistic}(0,1)$).&lt;/p&gt;
&lt;p&gt;It turns out that $\textrm{logit}(\textrm{Uniform}(0,1))$ is a $\textrm{Logistic}(0,1)$ RV.&lt;/p&gt;
&lt;p&gt;Voila! Our fast sampling trick and the Gumbel max trick are connected!&lt;/p&gt;
&lt;h2 id="Related-tricks"&gt;Related tricks&lt;a class="anchor-link" href="#Related-tricks"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Another trick is Justin Domke's &lt;a href="https://justindomke.wordpress.com/2014/01/08/reducing-sigmoid-computations-by-at-least-88-0797077977882/"&gt;trick&lt;/a&gt; to reduce calls to $\exp$ by $\approx 88\%$. The &lt;em&gt;disadvantage&lt;/em&gt; of this approach is that it's harder to implement with vectorization. The &lt;em&gt;advantage&lt;/em&gt; is that we don't need to precompute any expensive things.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Appendix"&gt;Appendix&lt;a class="anchor-link" href="#Appendix"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logit-plot"&gt;Logit plot&lt;a class="anchor-link" href="#Logit-plot"&gt;&amp;#182;&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logistic-random-variable"&gt;Logistic random variable&lt;a class="anchor-link" href="#Logistic-random-variable"&gt;&amp;#182;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Check that our sampling method is equivalent to sampling from a logistic distribution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;
&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;&lt;/p&gt;</summary><category term="math"></category><category term="sampling"></category><category term="tricks"></category><category term="Gumbel"></category></entry><entry><title>Sqrt-biased sampling</title><link href="http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/" rel="alternate"></link><published>2016-06-28T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-06-28:blog/post/2016/06/28/sqrt-biased-sampling/</id><summary type="html">&lt;p&gt;The following post is about instance of "sampling in proportion to &lt;span class="math"&gt;\(p\)&lt;/span&gt; is not
optimal, but you probably think it is." It's surprising how few people seem to
know this trick. Myself included! It was brought to my attention recently by
&lt;a href="http://lowrank.net/nikos/"&gt;Nikos Karampatziakis&lt;/a&gt;. (Thanks, Nikos!)&lt;/p&gt;
&lt;p&gt;The paper credited for this trick is
&lt;a href="http://www.pnas.org/content/106/6/1716.full.pdf"&gt;Press (2008)&lt;/a&gt;. I'm borrowing
heavily from that paper as well as an email exchange from Nikos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Suppose you're an aspiring chef with a severe head injury affecting
  your long- and short- term memory trying to find a special recipe from a
  cookbook that you made one time but just can't remember exactly which recipe
  it was. So, based on the ingredients of each recipe, you come up with a prior
  probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; that recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the one you're looking for. In total, the
  cookbook has &lt;span class="math"&gt;\(n\)&lt;/span&gt; recipes and &lt;span class="math"&gt;\(\sum_{i=1}^n p_i = 1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A good strategy would be to sort recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; and cook the most promising
ones first. Unfortunately, you're not a great chef so there is some probability
that you'll mess-up the recipe. So, it's a good idea to try recipes multiple
times. Also, you have no short term memory...&lt;/p&gt;
&lt;p&gt;This suggests a &lt;em&gt;sampling with replacement&lt;/em&gt; strategy, where we sample a recipe
from the cookbook to try &lt;em&gt;independently&lt;/em&gt; of whether we've tried it before
(called a &lt;em&gt;memoryless&lt;/em&gt; strategy). Let's give this strategy the name
&lt;span class="math"&gt;\(\boldsymbol{q}.\)&lt;/span&gt; Note that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is a probability distribution over
the recipes in the cookbook, just like &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many recipes until we find the special one?&lt;/strong&gt; To start, suppose the
special recipe is &lt;span class="math"&gt;\(j.\)&lt;/span&gt; Then, the expected number of recipes we have to make
until we find &lt;span class="math"&gt;\(j\)&lt;/span&gt; under the strategy &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j} = 1/q_{j}.
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation" style="display:none;" class="derivation"&gt;
&lt;strong&gt;Derivation&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We start with
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j},
$$&lt;/div&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(a = (1-q_j)\)&lt;/span&gt;, to clean up notation.
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty t \cdot a^{t-1}
$$&lt;/div&gt;
&lt;p&gt;Use the identity &lt;span class="math"&gt;\(\nabla_a [ a^t ] = t \cdot a^{t-1}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty \nabla_a[ a^{t} ].
$$&lt;/div&gt;
&lt;p&gt;Fish the gradient out of the sum and tweak summation index,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ \sum_{t=1}^\infty a^{t} \right]
= q_{j} \nabla_a\left[ -1 + \sum_{t=0}^\infty a^{t}\right]
$$&lt;/div&gt;
&lt;p&gt;Plugin in the solution to the geometric series,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ -1 + \frac{1}{1-a} \right].
$$&lt;/div&gt;
&lt;p&gt;Take derivative, expand &lt;span class="math"&gt;\(a\)&lt;/span&gt; and simplify,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \frac{1}{(1-a)^2}
= \frac{1}{q_j}
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The equation says that expected time it takes to sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; for &lt;em&gt;the first time&lt;/em&gt;
is the probability we didn't sample for &lt;span class="math"&gt;\((t-1)\)&lt;/span&gt; steps times the probability we
sample it at time &lt;span class="math"&gt;\(t.\)&lt;/span&gt; We multiply this probability by the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; to get the
&lt;em&gt;expected&lt;/em&gt; time.&lt;/p&gt;
&lt;p&gt;Note that this equation assumes that we known &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the special recipe &lt;em&gt;with
certainty&lt;/em&gt; when we sample it. We'll revisit this assumption later when we
consider potential errors in executing the recipe.&lt;/p&gt;
&lt;p&gt;Since we don't known which &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the right one, we take an expectation over it
according to the prior distribution, which yields the following equation,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{q_i}.
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The first surprising thing&lt;/strong&gt;: Uniform is just as good as &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt;,
  yikes! &lt;span class="math"&gt;\(f(\boldsymbol{p}) = \sum_{i=1}^n \frac{p_i}{p_i} = n\)&lt;/span&gt; and
  &lt;span class="math"&gt;\(f(\text{uniform}(n)) = \sum_{i=1}^n \frac{p_i }{ 1/n } = n.\)&lt;/span&gt; (Assume, without
  loss of generality, that &lt;span class="math"&gt;\(p_i &amp;gt; 0\)&lt;/span&gt; since we can just drop these elements from
  &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the &lt;em&gt;optimal&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;?&lt;/strong&gt; We can address this question by
solving the following optimization (which will have a nice closed form
solution),&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \boldsymbol{q}^* = \underset{\boldsymbol{q}}{\operatorname{argmin}} \sum_{i=1}^n \frac{p_i}{q_i} \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \text{ s.t. } \sum_{i=1}^n q_i = 1 \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \ \ \ \ \, q_1 \ldots q_n \ge 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;The optimization problem says minimize the expected time to find the special
recipe. The constraints enforce that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; be a valid probability
distribution.&lt;/p&gt;
&lt;p&gt;The optimal strategy, which we get via Lagrange multipliers, turns out to be,
&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;p&gt;&lt;button onclick="toggle('#Lagrange')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="Lagrange" style="display:none;" class="derivation"&gt;
To solve this constrained optimization problem, we form the
Lagrangian,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\boldsymbol{q}, \lambda) = \sum_{i=1}^n \frac{p_i}{q_i} - \lambda\cdot \left(1 - \sum_{i=1}^n q_i\right),$$&lt;/div&gt;
&lt;p&gt;and solve for &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; and multiplier &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; such that partial
derivatives are all equal to zero. This gives us the following system of
nonlinear equations,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \lambda - \frac{p_i}{q_i^2} = 0 \ \ \ \text{for } 1 \le i \le n \\
&amp;amp;&amp;amp; \lambda \cdot \left(1 - \sum_{i=1}^n q_i \right) = 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We see that &lt;span class="math"&gt;\(q_i = \pm \sqrt{\frac{p_i}{\lambda}}\)&lt;/span&gt; works for the first set of
equations, but since we need &lt;span class="math"&gt;\(q_i \ge 0\)&lt;/span&gt;, we take the positive one. Solving for
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and plugging it in, we get a normalized distribution,&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How much better is &lt;span class="math"&gt;\(q^*\)&lt;/span&gt;?&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(q^*) = \sum_i \frac{p_i}{q^*_i}
= \sum_i \frac{p_i}{ \frac{\sqrt{p_i} }{ \sum_j \sqrt{p_j}} }
= \left( \sum_i \frac{p_i}{ \sqrt{p_i} } \right) \left( \sum_j \sqrt{p_j} \right)
= \left( \sum_i \sqrt{p_i} \right)^2
$$&lt;/div&gt;
&lt;p&gt;which sometimes equals &lt;span class="math"&gt;\(n\)&lt;/span&gt;, e.g., when &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; is uniform, but is never
bigger than &lt;span class="math"&gt;\(n.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the intuition?&lt;/strong&gt; The reason why the &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-scheme is preferred is
because we save on &lt;em&gt;additional&lt;/em&gt; cooking experiments. For example, if a recipe
has &lt;span class="math"&gt;\(k\)&lt;/span&gt; times higher prior probability than the average recipe, then we will try
that recipe &lt;span class="math"&gt;\(\sqrt{k}\)&lt;/span&gt; times more often; compared to &lt;span class="math"&gt;\(k\)&lt;/span&gt;, which we'd get under
&lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt; Additional cooking experiments are not so advantageous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Allowing for noise in the cooking process&lt;/strong&gt;: Suppose that for each recipe we
  had a prior belief about how hard that recipe is for us to cook. Denote that
  belief &lt;span class="math"&gt;\(s_i\)&lt;/span&gt;, these belief are between zero (never get it right) and one
  (perfect every time) and do not sum to one over the cookbook.&lt;/p&gt;
&lt;p&gt;Following a similar derivation to before, the time to cook the special recipe
&lt;span class="math"&gt;\(j\)&lt;/span&gt; and cook it correctly is,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - \color{red}{s_j} q_j)^{t-1} q_{j} \color{red}{s_j} = \frac{1}{s_j \cdot q_j}
$$&lt;/div&gt;
&lt;p&gt;
That gives rise to a modified objective,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{\color{red}{s_i} \cdot q_i}
$$&lt;/div&gt;
&lt;p&gt;This is exactly the same as the previous objective, except we've replaced &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;
with &lt;span class="math"&gt;\(p_i/s_i.\)&lt;/span&gt; Thus, we can reuse our previous derivation to get the optimal
strategy, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i / s_i}.\)&lt;/span&gt; If noise is constant, then we
recover the original solution, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension to finding multiple tasty recipes&lt;/strong&gt;: Suppose we're trying to find
  several tasty recipes, not just a single special one. Now, &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is our prior
  belief that we'll like the recipe at all. How do we minimize the time until we
  find a tasty one? It turns out the same trick works without modification
  because all derivations apply to each recipe independently. The same trick
  works if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; does not sums to one over &lt;span class="math"&gt;\(n.\)&lt;/span&gt; For example, if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the
  independent probability that you'll like recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; at all, not the
  probability that it's the special one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond memoryless policies&lt;/strong&gt;: Clearly, our choice of a memoryless policy can
  be beat by a policy family that balances exploration (trying new recipes) and
  exploitation (trying our best guess).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Overall, the problem we've posed is similar to a
    &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;multi-armed bandit&lt;/a&gt;. In
    our case, the arms are the recipes, pulling the arm is trying the recipe and
    the reward is whether or not we liked the recipe (possibly noisy). The key
    difference between our setup and multi-armed bandits is that we trust our
    prior distribution &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; and noise model &lt;span class="math"&gt;\(\boldsymbol{s}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the amount of noise &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; is known and we trust the prior &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; then
    there is an optimal deterministic (without-replacement) strategy that we can
    get by sorting the recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; accounting for the error rates
    &lt;span class="math"&gt;\(s_i.\)&lt;/span&gt; This approach is described in the original paper.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A more realistic application&lt;/strong&gt;: In certain language modeling applications, we
  avoid computing normalization constants (which require summing over a massive
  vocabulary) by using importance sampling, negative sampling or noise
  contrastive estimation techniques (e.g.,
  &lt;a href="https://arxiv.org/pdf/1511.06909.pdf"&gt;Ji+,16&lt;/a&gt;;
  &lt;a href="http://www.aclweb.org/anthology/Q15-1016"&gt;Levy+,15&lt;/a&gt;). These techniques depend
  on a proposal distribution, which folks often take to be the unigram
  distribution. Unfortunately, this gives too many samples of stop words (e.g.,
  "the", "an", "a"), so practitioners "anneal" the unigram distribution (to
  increase the entropy), that is sample from &lt;span class="math"&gt;\(q_i \propto
  p_{\text{unigram},i}^\alpha.\)&lt;/span&gt; Typically, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is set by grid search and
  (no surprise) &lt;span class="math"&gt;\(\alpha \approx 1/2\)&lt;/span&gt; tends to work best! The &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-sampling
  trick is possibly a reverse-engineered justification in favor of annealing as
  "the right thing to do" (e.g., why not do additive smoothing?) and it even
  tells us how to set the annealing parameter &lt;span class="math"&gt;\(\alpha.\)&lt;/span&gt; The key assumption is
  that we want to sample the actual word at a given position as often as
  possible while still being diverse thanks to the coverage of unigram
  prior. (Furthermore, memoryless sampling leads to simpler algorithms.)&lt;/p&gt;
&lt;!--
Actually, many word2vec papers use $\alpha=3/4$, which was suggested in
[Levy+,15](http://www.aclweb.org/anthology/Q15-1016), including the default
value in
[gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L462). So,
[Ryan Cotterell](https://ryancotterell.github.io/) ran a quick experiment with
gensim, which confirmed the suspicion that $1/2$ may be better than $3/4.$

    Word similarity accuracy (avg of 10 runs)
    | alpha | accuracy |
    +==================+
    |  0.00 |    0.354 |
    |  0.25 |    0.403 |
    |  0.50 |    0.414 |
    |  0.75 |    0.395 |
    |  1.00 |    0.345 |
--&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="sampling"></category><category term="decision-making"></category></entry><entry><title>The optimal proposal distribution is not p</title><link href="http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/" rel="alternate"></link><published>2016-05-28T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-05-28:blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/</id><summary type="html">&lt;p&gt;The following is a quick rant about
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;
(see that post for notation).&lt;/p&gt;
&lt;p&gt;I've heard the following &lt;strong&gt;incorrect&lt;/strong&gt; statement one too many times,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We chose &lt;span class="math"&gt;\(q \approx p\)&lt;/span&gt; because &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is the "optimal" proposal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While it is certainly a good idea to pick &lt;span class="math"&gt;\(q\)&lt;/span&gt; to be as similar as possible to
&lt;span class="math"&gt;\(p\)&lt;/span&gt;, it is by no means &lt;em&gt;optimal&lt;/em&gt; because it is oblivious to &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;With importance sampling, it is possible to achieve a variance reduction over
Monte Carlo estimation. The optimal proposal distribution, assuming &lt;span class="math"&gt;\(f(x) \ge 0\)&lt;/span&gt;
for all &lt;span class="math"&gt;\(x\)&lt;/span&gt;, is &lt;span class="math"&gt;\(q(x) \propto p(x) f(x).\)&lt;/span&gt; This choice of &lt;span class="math"&gt;\(q\)&lt;/span&gt; gives us a &lt;em&gt;zero
variance&lt;/em&gt; estimate &lt;em&gt;with a single sample&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Of course, this is an unreasonable distribution to use because the normalizing
constant &lt;em&gt;is the thing you are trying to estimate&lt;/em&gt;, but it is proof that &lt;em&gt;better
proposal distributions exist&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The key to doing better than &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is to take &lt;span class="math"&gt;\(f\)&lt;/span&gt; into account. Look up
"importance sampling for variance reduction" to learn more.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="statistics"></category><category term="sampling"></category><category term="importance-sampling"></category></entry><entry><title>Dimensional analysis of gradient ascent</title><link href="http://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/" rel="alternate"></link><published>2016-05-27T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-05-27:blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/</id><summary type="html">&lt;p&gt;In physical sciences, numbers are paired with units and called quantities. In
this augmented number system, dimensional analysis provides a crucial sanity
check, much like type checking in a programming language. There are simple rules
for building up units and constraints on what operations are allowed. For
example, you can't multiply quantities which are not conformable or add
quantities with different units. Also, we generally know the units of the input
and desired output, which allows us to check that our computations at least
produce the right units.&lt;/p&gt;
&lt;p&gt;In this post, we'll discuss the dimensional analysis of gradient ascent, which
will hopefully help us understand why the "step size" is parameter so finicky
and why it even exists.&lt;/p&gt;
&lt;p&gt;Gradient ascent is an iterative procedure for (locally) maximizing a function,
&lt;span class="math"&gt;\(f: \mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
x_{t+1} = x_t + \alpha \frac{\partial f(x_t)}{\partial x}
$$&lt;/div&gt;
&lt;p&gt;In general, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a &lt;span class="math"&gt;\(d \times d\)&lt;/span&gt; matrix, but often we constrain the matrix
to be simple, e.g., &lt;span class="math"&gt;\(a\cdot I\)&lt;/span&gt; for some scalar &lt;span class="math"&gt;\(a\)&lt;/span&gt; or &lt;span class="math"&gt;\(\text{diag}(a)\)&lt;/span&gt; for some
vector &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at the units of the change in &lt;span class="math"&gt;\(\Delta x=x_{t+1} - x_t\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\Delta x) = \left(\textbf{units }\alpha\cdot \frac{\partial f(x_t)}{\partial x}\right) = (\textbf{units }\alpha) \frac{(\textbf{units }f)}{(\textbf{units }x)}.
$$&lt;/div&gt;
&lt;p&gt;The units of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; must be &lt;span class="math"&gt;\((\textbf{units }x)\)&lt;/span&gt;. However, if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt;
is unit free, we're happy with &lt;span class="math"&gt;\((\textbf{units }x) / (\textbf{units }f)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Solving for the units of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; we get,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\alpha) = \frac{(\textbf{units }x)^2}{(\textbf{units }f)}.
$$&lt;/div&gt;
&lt;p&gt;This gives us an idea for what &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; should be.&lt;/p&gt;
&lt;p&gt;For example, the inverse Hessian passes the unit check (if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt; unit
free). The disadvantages of the Hessian is that it needs to be positive-definite
(or at least invertible) in order to be a valid "step size" (i.e., we need
step sizes to be &lt;span class="math"&gt;\(&amp;gt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Another method for handling step sizes is line search. However, line search
won't let us run online. Furthermore, line search would be too slow in the case
where we want a step size for each dimension.&lt;/p&gt;
&lt;p&gt;In machine learning, we've become fond of online methods, which adapt the step
size as they go. The general idea is to estimate a step size matrix that passes
the unit check (for each dimension of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). Furthermore, we want do as little
extra work as possible to get this estimate (e.g., we want to avoid computing a
Hessian because that would be extra work). So, the step size should be based
only iterates and gradients up to time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf"&gt;AdaGrad&lt;/a&gt; doesn't doesn't
  pass the unit check. This motivated AdaDelta.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1212.5701"&gt;AdaDelta&lt;/a&gt; uses the ratio of (running
  estimates of) the root-mean-squares of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\partial f / \partial
  x\)&lt;/span&gt;. The mean is taken using an exponentially weighted moving average. See
  paper for actual implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; came later and made some tweaks to
  remove (unintended) bias in the AdaDelta estimates of the numerator and
  denominator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, it's important/useful to analyze the units of numerical algorithms
in order to get a sanity check (i.e., catch mistakes) as well as to develop an
understanding of why certain parameters exist and how properties of a problem
affect the values we should use for them.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="optimization"></category></entry><entry><title>Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</title><link href="http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/" rel="alternate"></link><published>2016-03-05T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-03-05:blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/</id><summary type="html">&lt;p&gt;The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization---the "inner" optimization optimizes training loss (wrt &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;),
while the "outer" optimizes hyperparameters (&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$&lt;/div&gt;
&lt;p&gt;Can we estimate &lt;span class="math"&gt;\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)&lt;/span&gt; so
that we can run gradient-based optimization over &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Well, what does it mean to have an &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; inside a function?&lt;/p&gt;
&lt;p&gt;Well, it means that there is a &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that gets passed to
&lt;span class="math"&gt;\(\mathcal{L}_{\text{dev}}\)&lt;/span&gt;. And, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, denoted
&lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt;. Furthermore, &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt; is the solution to,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$&lt;/div&gt;
&lt;p&gt;Now how does &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; change as the result of an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The constraint on the derivative implies a type of "equilibrium"---the inner
optimization process will continue to optimize regardless of how we change
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Assuming we don't change &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; too much, then the inner
optimization shouldn't change &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; too much and it will change in a
predictable way.&lt;/p&gt;
&lt;p&gt;To do this, we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are related through a
function &lt;span class="math"&gt;\(g\)&lt;/span&gt; as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$g(x,y) = 0.$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;span class="math"&gt;\(g\)&lt;/span&gt; is a smooth function in &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we can perturb either
argument, say &lt;span class="math"&gt;\(x\)&lt;/span&gt; by a small amount &lt;span class="math"&gt;\(\Delta_x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; by &lt;span class="math"&gt;\(\Delta_y\)&lt;/span&gt;. Because
system preserves the constraint, i.e.,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$&lt;/div&gt;
&lt;p&gt;We can solve for the change of &lt;span class="math"&gt;\(x\)&lt;/span&gt; as a result of an infinitesimal change in
&lt;span class="math"&gt;\(y\)&lt;/span&gt;. We take the first-order expansion,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(g(x,y)\)&lt;/span&gt; is already zero,&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Next, we solve for &lt;span class="math"&gt;\(\frac{\Delta_x}{\Delta_y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$&lt;/div&gt;
&lt;p&gt;Back to the original problem: Now we can use the implicit function theorem to
estimate how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; varies in &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; by plugging in &lt;span class="math"&gt;\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)&lt;/span&gt;, &lt;span class="math"&gt;\(x \mapsto \theta\)&lt;/span&gt;
and &lt;span class="math"&gt;\(y \mapsto \lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$&lt;/div&gt;
&lt;p&gt;This tells us how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; changes with respect to an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$&lt;/div&gt;
&lt;p&gt;Since we don't like (explicit) matrix inverses, we compute &lt;span class="math"&gt;\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)&lt;/span&gt; as the solution to &lt;span class="math"&gt;\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)&lt;/span&gt;. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products---i.e., you never have to materialize the Hessian. (Apparently,
&lt;a href="https://en.wikipedia.org/wiki/Matrix-free_methods"&gt;matrix-free linear algebra&lt;/a&gt;
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
&lt;a href="/blog/post/2014/02/10/gradient-vector-product/"&gt;earlier post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.&lt;/p&gt;
&lt;p&gt;In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/"&gt;Truncated Bi-Level Optimization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf"&gt;Efficient multiple hyperparameter learning for log-linear models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1502.03492"&gt;Gradient-based Hyperparameter Optimization through Reversible Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="calculus"></category></entry><entry><title>Multidimensional array index</title><link href="http://timvieira.github.io/blog/post/2016/01/17/multidimensional-array-index/" rel="alternate"></link><published>2016-01-17T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-01-17:blog/post/2016/01/17/multidimensional-array-index/</id><summary type="html">&lt;p&gt;This is a simple note on how to compute a bijective mapping between the indices
of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional array and a flat, one-dimensional array. We'll look at
both directions of the mapping: &lt;code&gt;(tuple-&amp;gt;int)&lt;/code&gt; and &lt;code&gt;(int -&amp;gt; tuple)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We'll assume each dimension &lt;span class="math"&gt;\(a, b, c, \ldots\)&lt;/span&gt; is a positive integer and bounded
&lt;span class="math"&gt;\(a \le A, b \le B, c \le C, \ldots\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Start small&lt;/h3&gt;
&lt;p&gt;Let's start by looking at &lt;span class="math"&gt;\(n = 3\)&lt;/span&gt; and generalize from there.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's our test case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: This is not the only bijective mapping from &lt;code&gt;tuple&lt;/code&gt; to &lt;code&gt;int&lt;/code&gt; that we
could have come up with. The one we chose corresponds to the particular layout,
which is apparent in the test case.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(n=4\)&lt;/span&gt; the pattern is &lt;span class="math"&gt;\(((a \cdot B + b) \cdot C + d) \cdot D + d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Sidenote: We don't actually need the bound &lt;span class="math"&gt;\(a \le A\)&lt;/span&gt; in either &lt;code&gt;index&lt;/code&gt; or
&lt;code&gt;inverse&lt;/code&gt;. This gives us a little extra flexibility because our first
dimension can be infinite/unknown.&lt;/p&gt;
&lt;h3&gt;General case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Map tuple ``a`` to index with known bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# the pattern:&lt;/span&gt;
    &lt;span class="c1"&gt;# ((i*J + j)*K + k)*L + l&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Find key given index ``ix`` and bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Testing the general case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Test that `index` produces the layout we expect.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;got&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;got&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="misc"></category></entry><entry><title>Gradient of a product</title><link href="http://timvieira.github.io/blog/post/2015/07/29/gradient-of-a-product/" rel="alternate"></link><published>2015-07-29T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-07-29:blog/post/2015/07/29/gradient-of-a-product/</id><summary type="html">&lt;div class="math"&gt;$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$&lt;/div&gt;
&lt;p&gt;In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).&lt;/p&gt;
&lt;p&gt;Given a collection of functions with a common argument &lt;span class="math"&gt;\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define their product &lt;span class="math"&gt;\(p(x) = \prod_{i=1}^n f_i(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."&lt;/p&gt;
&lt;p&gt;A naive method for computing the gradient computes the leave-one-out products
from scratch for each &lt;span class="math"&gt;\(i\)&lt;/span&gt; (outer loop)---resulting in a overall runtime of
&lt;span class="math"&gt;\(O(n^2)\)&lt;/span&gt; to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Division trick&lt;/strong&gt;: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f \ne 0\)&lt;/span&gt;. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requires multiplicative inverse operator (division) &lt;em&gt;and&lt;/em&gt;
   associative-commutative multiplication, which means it's not applicable to
   matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Log trick&lt;/strong&gt;: Suppose &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;). It's common practice (especially in machine learning) to replace
&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log f_i\)&lt;/span&gt;, which turns products into sums, &lt;span class="math"&gt;\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)&lt;/span&gt;, and tiny numbers (like
&lt;span class="math"&gt;\(\texttt{3.72e-44}\)&lt;/span&gt;) into large ones (like &lt;span class="math"&gt;\(\texttt{-100}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Furthermore, using the identity &lt;span class="math"&gt;\((\nabla g) = g \cdot \nabla \log g\)&lt;/span&gt;, we can
operate exclusively in the "&lt;span class="math"&gt;\(\log\)&lt;/span&gt;-domain".&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Numerically stable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't require multiplicative inverse assuming you can compute &lt;span class="math"&gt;\(\gradx{ \log
   f_i(x) }\)&lt;/span&gt; without it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f &amp;gt; 0\)&lt;/span&gt;. But, we can use
   &lt;a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/"&gt;LogReal number class&lt;/a&gt;
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't easily generalize to other notions of multiplication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic programming trick&lt;/strong&gt;: &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; runtime and &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. &lt;a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf"&gt;Wallach (2004)&lt;/a&gt;,
section 7).&lt;/p&gt;
&lt;p&gt;The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.&lt;/p&gt;
&lt;p&gt;Here are the equations:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\alpha_0(x) &amp;amp;=&amp;amp; 1 \\
\alpha_t(x)
   &amp;amp;=&amp;amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;amp;=&amp;amp; 1 \\
\beta_t(x)
  &amp;amp;=&amp;amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \prod_{j &amp;lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &amp;gt; i} f_j(x) \right) \\
&amp;amp;=&amp;amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Clearly, this requires &lt;span class="math"&gt;\(O(n)\)&lt;/span&gt; additional space.&lt;/p&gt;
&lt;p&gt;Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).&lt;/p&gt;
&lt;p&gt;Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where &lt;span class="math"&gt;\(A B C \ne B C A\)&lt;/span&gt;, even if all matrices have the
conformable dimensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Connections to automatic differentiation&lt;/strong&gt;: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
&lt;em&gt;can&lt;/em&gt; compute it's gradient with the same asymptotic complexity, &lt;em&gt;but&lt;/em&gt; you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with &lt;span class="math"&gt;\(\bigo(n^2)\)&lt;/span&gt; time and &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt; space
(other than the space to store the &lt;span class="math"&gt;\(n\)&lt;/span&gt; functions) and ended up with a &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt;
time &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space algorithm with a little clever thinking. What I'm saying
is autodiff---even if you don't use a magical package---tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sketch&lt;/em&gt;: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). It turns out that the backward pass gives us &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as
adjoints. Lastly, we'd propagate gradients through the &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s to get
&lt;span class="math"&gt;\(\frac{\partial p}{\partial x}\)&lt;/span&gt;. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="calculus"></category><category term="numerical"></category></entry><entry><title>Multiclass logistic regression and conditional random fields are the same thing</title><link href="http://timvieira.github.io/blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/" rel="alternate"></link><published>2015-04-29T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-04-29:blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/</id><summary type="html">&lt;p&gt;A short rant: multiclass logistic regression and conditional random fields (CRF)
are the same thing. This comes to a surprise to many people because CRFs tend to
be surrounded by additional "stuff."&lt;/p&gt;
&lt;p&gt;Multiclass logistic regression is simple. The goal is to predict the correct
label &lt;span class="math"&gt;\(y^*\)&lt;/span&gt; from handful of labels &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt; given the observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; based
on features &lt;span class="math"&gt;\(\phi(x,y)\)&lt;/span&gt;. Training this model typically requires computing the
gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,y^*) - \sum_{y \in \mathcal{Y}} p(y|x) \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
p(y|x) &amp;amp;=&amp;amp; \frac{1}{Z(x)} \exp(\theta^\top \phi(x,y)) &amp;amp; \ \ \ \ \text{and} \ \ \ \ &amp;amp;
Z(x) &amp;amp;=&amp;amp; \sum_{y \in \mathcal{Y}} \exp(\theta^\top \phi(x,y))
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;At test-time, we often take the highest-scoring label under the model.&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{y}(x) = \textbf{argmax}_{y \in \mathcal{Y}} \theta^\top \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;A conditional random field is exactly multiclass logistic regression. The only
difference is that the sum and argmax are inefficient to compute naively (i.e.,
by enumeration). This point is often lost when people first learn about
CRFs. Some people never make this connection.&lt;/p&gt;
&lt;p&gt;Here's some stuff you'll see once we start talking about CRFs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inference algorithms (e.g., Viterbi decoding, forward-backward, Junction
   tree)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graphical models (factor graphs, Bayes nets, Markov random fields)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model templates (i.e., repeated feature functions)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the logistic regression case, we'd never use the term "inference" to describe
the "sum" and "max" over a handful of categories. Once we move to a structured
label space, this term gets throw around. (BTW, this isn't "statistical
inference," just algorithms to compute sum and max over &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;Graphical models establish a notation and structural properties which allow
efficient inference -- things like cycles and treewidth.&lt;/p&gt;
&lt;p&gt;Model templating is the only essential trick to move from logistic regression to
a CRF. Templating "solves" the problem that not all training examples have the
same "size" -- the set of outputs &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; now depends on &lt;span class="math"&gt;\(x\)&lt;/span&gt;. A model
template specifies how to compute the features for an entire output, by looking
at interactions between subsets of variables.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,\boldsymbol{y}) = \sum_{\alpha \in A(x)} \phi_\alpha(x,
\boldsymbol{y}_\alpha)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a labeled subset of variables often called a factor and
&lt;span class="math"&gt;\(\boldsymbol{y}_\alpha\)&lt;/span&gt; is the subvector containing values of variables
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. Basically, the feature function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gets to look at some subset of
the variables being predicted &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the entire input &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The ability to look
at more of &lt;span class="math"&gt;\(y\)&lt;/span&gt; allows the model to make more coherent predictions.&lt;/p&gt;
&lt;p&gt;Anywho, it's often useful to take a step back and think about what you are
trying to compute instead of how you're computing it. In this post, this allowed
us see the similarity between logistic regression and CRFs even though they seem
quite different.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="machine-learning"></category><category term="rant"></category><category term="crf"></category></entry><entry><title>Conditional random fields as Deep learning models?</title><link href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/" rel="alternate"></link><published>2015-02-05T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-02-05:blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/</id><summary type="html">&lt;p&gt;This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.&lt;/p&gt;
&lt;p&gt;In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Limit the expressiveness of the model (i.e., don't use to model you want)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the training objective&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approximate inference&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Approximate inference is tricky. Things can easily go awry.&lt;/p&gt;
&lt;p&gt;For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
&lt;a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf"&gt;(Kulesza &amp;amp; Pereira, 2007)&lt;/a&gt;. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
&lt;a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf"&gt;sometimes&lt;/a&gt; these loopy BP
approximations work fine.)&lt;/p&gt;
&lt;p&gt;It looks like using approximate inference during training changes the training
objective.&lt;/p&gt;
&lt;p&gt;So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.&lt;/p&gt;
&lt;p&gt;Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp;amp; Eisner.&lt;/p&gt;
&lt;p&gt;Unsuprisingly, it works pretty well.&lt;/p&gt;
&lt;p&gt;I first saw this idea in Stoyanov &amp;amp; Eisner (2011). They use loopy belief
propagation as their approximate inference algorithm. At the end of the day,
their model is essentially a deep recurrent network, which came from unrolling
inference in a graphical model. This idea really struck me because it's clearly
right in the middle between graphical models and deep learning.&lt;/p&gt;
&lt;p&gt;You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.&lt;/p&gt;
&lt;p&gt;Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.&lt;/p&gt;</summary><category term="machine-learning"></category><category term="deep-learning"></category><category term="structured-prediction"></category></entry><entry><title>Log-Real number class</title><link href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/" rel="alternate"></link><published>2015-02-01T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2015-02-01:blog/post/2015/02/01/log-real-number-class/</id><summary type="html">&lt;p&gt;Most people know how to avoid numerical underflow in probability computations by
representing intermediate quantities in the log-domain. This trick turns
"multiplication" into "addition", "addition" into "logsumexp", "0" into
&lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; and "1" into &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Most importantly, it turns really small numbers into
reasonable-size numbers.&lt;/p&gt;
&lt;p&gt;Unfortunately, without modification, this trick is limited to positive numbers
because &lt;code&gt;log&lt;/code&gt; of a negative number is &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Well, there is good news! For the cost of an extra bit, we can extend this trick
to the negative reals and furthermore, we get a bonafide ring instead of a mere
semiring.&lt;/p&gt;
&lt;p&gt;I first saw this trick in
&lt;a href="http://www.aclweb.org/anthology/D09-1005"&gt;Li and Eisner (2009)&lt;/a&gt;. The trick is
nicely summarized in Table 3 of that paper, which I've pasted below.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
&lt;img src="/blog/images/logreal.png"/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Why do I care?&lt;/strong&gt; When computing gradients (e.g., gradient of risk),
intermediate values are rarely all positive. Furthermore, we're often
multiplying small things together. I've recently found log-reals to be effective
at squeaking a bit more numerical accuracy.&lt;/p&gt;
&lt;p&gt;This trick is useful for almost all backprop computations because backprop is
essentially:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;adjoint(u) += adjoint(v) * dv/du.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only tricky bit is lifting all &lt;code&gt;du/dv&lt;/code&gt; computations into the log-reals.&lt;/p&gt;
&lt;p&gt;Implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This trick is better suited to programming languages with structs. Using
  objects will probably in an horrible slow down and using parallel arrays to
  store the sign bit and double is probably too tedious and error prone. (Sorry
  java folks.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here's a &lt;a href="https://github.com/andre-martins/TurboParser/blob/master/src/util/logval.h"&gt;C++ implementation&lt;/a&gt;
  with operator overloading from Andre Martins&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that log-real &lt;code&gt;+=&lt;/code&gt; involves calls to &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;, which will
  definitely slow your code down a bit (these functions are much slower than
  addition).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="numerical"></category></entry><entry><title>Importance Sampling</title><link href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/" rel="alternate"></link><published>2014-12-21T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-12-21:blog/post/2014/12/21/importance-sampling/</id><summary type="html">&lt;p&gt;Importance sampling is a powerful and pervasive technique in statistics, machine
learning and randomized algorithms.&lt;/p&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Importance sampling is a technique for estimating the expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of a
random variable &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; under distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; from samples of a different
distribution &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The key observation is that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is can expressed as the expectation of a
different random variable &lt;span class="math"&gt;\(f^*(x)=\frac{p(x)}{q(x)}\! \cdot\! f(x)\)&lt;/span&gt; under &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}_{q}\! \left[ f^*(x) \right] = \mathbb{E}_{q}\! \left[ \frac{p(x)}{q(x)} f(x) \right] = \sum_{x} q(x) \frac{p(x)}{q(x)} f(x) = \sum_{x} p(x) f(x) = \mathbb{E}_{p}\! \left[ f(x) \right] = \mu
$$&lt;/div&gt;
&lt;p&gt;Technical condition: &lt;span class="math"&gt;\(q\)&lt;/span&gt; must have support everywhere &lt;span class="math"&gt;\(p\)&lt;/span&gt; does, &lt;span class="math"&gt;\(p(x) &amp;gt; 0
\Rightarrow q(x) &amp;gt; 0\)&lt;/span&gt;. Without this condition, the equation is biased! Note: &lt;span class="math"&gt;\(q\)&lt;/span&gt;
can support things that &lt;span class="math"&gt;\(p\)&lt;/span&gt; doesn't.&lt;/p&gt;
&lt;p&gt;Terminology: The quantity &lt;span class="math"&gt;\(w(x) = \frac{p(x)}{q(x)}\)&lt;/span&gt; is often referred to as the
"importance weight" or "importance correction". We often refer to &lt;span class="math"&gt;\(p\)&lt;/span&gt; as the
target density and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the proposal density.&lt;/p&gt;
&lt;p&gt;Now, given samples &lt;span class="math"&gt;\(\{ x^{(i)} \}_{i=1}^{n}\)&lt;/span&gt; from &lt;span class="math"&gt;\(q\)&lt;/span&gt;, we can use the Monte
Carlo estimate, &lt;span class="math"&gt;\(\hat{\mu} \approx \frac{1}{n} \sum_{i=1}^n f^{*}(x^{(i)})\)&lt;/span&gt;, as
an unbiased estimator of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;There are a few reasons we might want use importance sampling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convenience&lt;/strong&gt;: It might be trickier to sample directly from &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bias-correction&lt;/strong&gt;: Suppose, we're developing an algorithm which requires
     samples to satisfy some "safety" condition (e.g., a minimum support
     threshold) and be unbiased. Importance sampling can be used to remove bias,
     while satisfying the condition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: It might be the case that sampling directly from
     &lt;span class="math"&gt;\(p\)&lt;/span&gt; would require more samples to estimate &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. Check out these
     &lt;a href="http://www.columbia.edu/~mh2078/MCS04/MCS_var_red2.pdf"&gt;great notes&lt;/a&gt; for
     more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-policy evaluation and learning&lt;/strong&gt;: We might want to collect some
     "exploratory data" from &lt;span class="math"&gt;\(q\)&lt;/span&gt; and evaluate different "policies", &lt;span class="math"&gt;\(p\)&lt;/span&gt; (e.g.,
     to pick the best one). Some cool papers:
     &lt;a href="http://arxiv.org/abs/1209.2355"&gt;counterfactual reasoning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;reinforcement learning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/1103.4601"&gt;contextual bandits&lt;/a&gt;,
     &lt;a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;domain adaptation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few common cases for &lt;span class="math"&gt;\(q\)&lt;/span&gt; worth separate consideration:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: This is the case in experimental design, variance
     reduction, active learning and reinforcement learning. It's often difficult
     to design &lt;span class="math"&gt;\(q\)&lt;/span&gt;, which results in an estimator with "reasonable" variance. A
     very difficult case is in off-policy evaluation because it (essentially)
     requires a good exploratory distribution for every possible policy. (I have
     much more to say on this topic.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Little to no control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: For example, you're given some dataset
     (e.g., new articles) and you want to estimate performance on a different
     dataset (e.g., Twitter).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: In this case, we want to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; (typically referred
     to as the propensity score) and use it in the importance sampling
     estimator. This technique, as far as I can tell, is widely used to remove
     selection bias when estimating effects of different treatments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;: The main drawback of importance sampling is variance. A few bad
samples with large weights can drastically throw off the estimator. Thus, it's
often the case that a biased estimator is preferred, e.g.,
&lt;a href="https://hips.seas.harvard.edu/blog/2013/01/14/unbiased-estimators-of-partition-functions-are-basically-lower-bounds/"&gt;estimating the partition function&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/1209.2355"&gt;clipping weights&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/cs/0204043"&gt;indirect importance sampling&lt;/a&gt;. A secondary
drawback is that both densities must be normalized, which is often intractable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's next?&lt;/strong&gt; I plan to cover "variance reduction" and "off-policy
evaluation" in more detail in future posts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="statistics"></category><category term="randomized"></category></entry><entry><title>Numerically-stable p-norms</title><link href="http://timvieira.github.io/blog/post/2014/11/10/numerically-stable-p-norms/" rel="alternate"></link><published>2014-11-10T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-11-10:blog/post/2014/11/10/numerically-stable-p-norms/</id><summary type="html">&lt;p&gt;Consider the p-norm
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$&lt;/div&gt;
&lt;p&gt;In python this translates to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;First-pass implementation of p-norm.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, suppose &lt;span class="math"&gt;\(|x_i|^p\)&lt;/span&gt; causes overflow (for some &lt;span class="math"&gt;\(i\)&lt;/span&gt;). This will occur for sufficiently large &lt;span class="math"&gt;\(p\)&lt;/span&gt; or sufficiently large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;---even if &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is representable (i.e., not NaN or &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;big&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e300&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c1"&gt;# expected: 1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This fails because we can't square big&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;A little math&lt;/h2&gt;
&lt;p&gt;There is a way to avoid overflow on the account of a few large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here's a little fact about p-norms: for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \alpha \cdot \boldsymbol{x} ||_p = |\alpha| \cdot || \boldsymbol{x}   ||_p
$$&lt;/div&gt;
&lt;p&gt;We'll use the following version (harder to remember)
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p  = |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
$$&lt;/div&gt;
&lt;p&gt;Don't believe it? Here's some algebra:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
|| \boldsymbol{x} ||_p
&amp;amp;=&amp;amp; \left( \sum_i |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; \left( \sum_i \frac{|\alpha|^p}{|\alpha|^p} \cdot |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left( \frac{|x_i| }{|\alpha|} \right)^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left| \frac{x_i }{\alpha} \right|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
\end{eqnarray*}
$$&lt;/div&gt;
&lt;h2&gt;Back to numerical stability&lt;/h2&gt;
&lt;p&gt;Suppose we pick &lt;span class="math"&gt;\(\alpha = \max_i |x_i|\)&lt;/span&gt;. Now, the largest number we have to take
the power of is one --- making it very difficult to overflow on the account of
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. This should remind you of the infamous log-sum-exp trick.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, our example from before works :-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It appears as if &lt;code&gt;scipy.linalg.norm&lt;/code&gt; is robust to overflow, while &lt;code&gt;numpy.linalg.norm&lt;/code&gt; is not. Note that &lt;code&gt;scipy.linalg.norm&lt;/code&gt; appears to be a bit slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;logsumexp&lt;/code&gt; trick is nearly identical, but operates in the log-domain, i.e., &lt;span class="math"&gt;\(\text{logsumexp}(\log(|x|) \cdot p) / p = \log || x ||_p\)&lt;/span&gt;. You can implement both tricks with the same code, if you use different number classes for log-domain and real-domain---a trick you might have seen before.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.math&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="numerical"></category></entry><entry><title>KL-divergence as an objective function</title><link href="http://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/" rel="alternate"></link><published>2014-10-06T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-10-06:blog/post/2014/10/06/kl-divergence-as-an-objective-function/</id><summary type="html">&lt;p&gt;It's well-known that
&lt;a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL-divergence&lt;/a&gt;
is not symmetric, but which direction is right for fitting your model?&lt;/p&gt;
&lt;p&gt;First, which one is which?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cheat sheet&lt;/strong&gt;: If we're fitting &lt;span class="math"&gt;\(q\)&lt;/span&gt; to &lt;span class="math"&gt;\(p\)&lt;/span&gt; using&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(q || p)\)&lt;/span&gt;
  - mode-seeking, exclusive
  - no normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(p || q)\)&lt;/span&gt;
  - mean-seeking, inclusive
  - results in a moment-matching problem when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family
  - requires normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How I remember which is which&lt;/strong&gt;: The KL notation is a little strange with the
"&lt;span class="math"&gt;\(||\)&lt;/span&gt;". I pretend it's a division symbol. This happens to correspond nicely to a
division symbol in the corresponding equation (I'm not sure it's
intentional). It helps to remember that you always normalize wrt the "numerator"
(and &lt;span class="math"&gt;\(q\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Computational perspecive&lt;/h2&gt;
&lt;p&gt;Let's look at what's involved in fitting a model &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; in each
direction. In this section, I'll describe the gradient and pay special attention
to the issue of normalization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;: &lt;span class="math"&gt;\(p,q_\theta\)&lt;/span&gt; are probabilty distributions. &lt;span class="math"&gt;\(p = \bar{p} / Z_p\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is the normalization constant. Similarly for &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The easy direction &lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(q_\theta || p)
&amp;amp;= \sum_d q(d) \log \left( \frac{q(d)}{p(d)} \right) \\
&amp;amp;= \sum_d q(d) \left( \log q(d) - \log p(d) \right) \\
&amp;amp;= \underbrace{\sum_d q(d) \log q(d)}_{-\text{entropy}} - \underbrace{\sum_d q(d) \log p(d)}_{\text{cross-entropy}} \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's look at normalization of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, the entropy term is easy because there is no &lt;span class="math"&gt;\(p\)&lt;/span&gt; in it.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d q(d) \log p(d)
&amp;amp;= \sum_d q(d) \log (\bar{p}(d) / Z_p) \\
&amp;amp;= \sum_d q(d) \left( \log \bar{p}(d) - \log Z_p) \right) \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \sum_d q(d) \log Z_p \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \log Z_p
\end{align*}&lt;/div&gt;
&lt;p&gt;In this case, &lt;span class="math"&gt;\(-\log Z_p\)&lt;/span&gt; is an additive constant, which can be dropped because
we're optimizing.&lt;/p&gt;
&lt;p&gt;This leaves us with the following optimization problem:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \text{argmin}_\theta \textbf{KL}(q_\theta || p) \\
&amp;amp;\qquad = \text{argmin}_\theta \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d)
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's work out the gradient
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \nabla\left[ \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d) \right] \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \log q_\theta(d) \right] - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) \right) - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) - \log \bar{p}(d) \right) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( \log q_\theta(d) - \log \bar{p}(d) \right) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;We killed the one in the last equality because &lt;span class="math"&gt;\(\sum_d \nabla
\left[ q(d) \right] = \nabla \left[ \sum_d q(d) \right] = \nabla
\left[ 1 \right] = 0\)&lt;/span&gt;, for any &lt;span class="math"&gt;\(q\)&lt;/span&gt; which is a probability distribution.&lt;/p&gt;
&lt;p&gt;This direction is convenient because we don't need to normalize
&lt;span class="math"&gt;\(p\)&lt;/span&gt;. Unfortunately, the "easy" direction is nonconvex in general --- unlike the
"hard" direction, which is convex.&lt;/p&gt;
&lt;h3&gt;Harder direction &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(p || q_\theta)
&amp;amp;= \sum_d p(d) \log \left( \frac{p(d)}{q(d)} \right) \\
&amp;amp;= \sum_d p(d) \left( \log p(d) - \log q(d) \right) \\
&amp;amp;= \sum_d p(d) \log p(d) - \sum_d p(d) \log q(d) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Clearly the first term (entropy) won't matter if we're just trying optimize wrt
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. So, let's focus on the second term (cross-entropy).
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d p(d) \log q(d)
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \left( \log \bar{q}(d) - \log Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \frac{1}{Z_p} \sum_d \bar{p}(d) \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \frac{1}{Z_p} \sum_d \bar{p}(d) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q
\end{align*}&lt;/div&gt;
&lt;p&gt;Unfortunately the &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; factor is unavoidable. The usual "approximate inference
story" is that &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is hard to compute, while the approximating distributions
normalization constant &lt;span class="math"&gt;\(Z_q\)&lt;/span&gt; is easy.&lt;/p&gt;
&lt;p&gt;Nonetheless, optimizing KL in this direction is still useful. Examples include,
expectation propagation, variational decoding and maximum likelihood
estimation. In the case of maximum likelihood estimation, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the empirical
distribution, so technically you don't have to compute its normalizing constant,
but you do need to sample from it (which can be just as hard).&lt;/p&gt;
&lt;p&gt;The gradient, when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family, is intuitive:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\nabla \left[ \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \right]
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \nabla \left[ \log \bar{q}(d) \right] - \nabla \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \phi_q(d) - \mathbb{E}_q \left[ \phi_q \right] \\
&amp;amp;= \mathbb{E}_p \left[ \phi_q \right] - \mathbb{E}_q \left[ \phi_q \right]
\end{align*}&lt;/div&gt;
&lt;p&gt;Optimization problem is convex when &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; is an exponential families ---
i.e., &lt;span class="math"&gt;\(p\)&lt;/span&gt; can be arbitrary. You can think of maximum likelihood estimation as a
method which minimizes KL divergence from samples of &lt;span class="math"&gt;\(p\)&lt;/span&gt;. In this case, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is
the true data distribution! The first term in the gradient is based on a sample
instead of an exact estimate (often called "observed feature counts").&lt;/p&gt;
&lt;p&gt;Downside: computing &lt;span class="math"&gt;\(\mathbb{E}_p \left[ \phi_q \right]\)&lt;/span&gt; might not be tractable.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Both directions of KL are special cases of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence. For a unified
  account of both directions consider looking into &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inclusive divergences require &lt;span class="math"&gt;\(q &amp;gt; 0\)&lt;/span&gt; whenever &lt;span class="math"&gt;\(p &amp;gt; 0\)&lt;/span&gt;. No "false negatives".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exclusive divergences will often favor a single mode.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computing the value of KL (not just the gradient) in either direction requires
  normalization. However, in the "easy" direction, using unnormalized &lt;span class="math"&gt;\(p\)&lt;/span&gt;
  results in only an additive constant difference. So, it's still just as
  useful, if all you care about is optimization (fitting the model).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="statistics"></category><category term="machine-learning"></category></entry><entry><title>Complex-step derivative</title><link href="http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/" rel="alternate"></link><published>2014-08-07T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-08-07:blog/post/2014/08/07/complex-step-derivative/</id><summary type="html">&lt;p&gt;Estimate derivatives by simply passing in a complex number to your function!&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;Recall, the centered-difference approximation is a fairly accurate method for
approximating derivatives of a univariate function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, which only requires two
function evaluations. A similar derivation, based on the Taylor series expansion
with a complex perturbation, gives us a similarly-accurate approximation with a
single (complex) function evaluation instead of two (real-valued) function
evaluations. Note: &lt;span class="math"&gt;\(f\)&lt;/span&gt; must support complex inputs (in frameworks, such as numpy
or matlab, this often requires no modification to source code).&lt;/p&gt;
&lt;p&gt;This post is based on
&lt;a href="http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf"&gt;Martins+'03&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;: Start with the Taylor series approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x + i \cdot \varepsilon) =
  \frac{i^0 \varepsilon^0}{0!} f(x)
+ \frac{i^1 \varepsilon^1}{1!} f'(x)
+ \frac{i^2 \varepsilon^2}{2!} f''(x)
+ \frac{i^3 \varepsilon^3}{3!} f'''(x)
+ \cdots
$$&lt;/div&gt;
&lt;p&gt;Take the imaginary part of both sides and solve for &lt;span class="math"&gt;\(f'(x)\)&lt;/span&gt;. Note: the &lt;span class="math"&gt;\(f\)&lt;/span&gt; and
&lt;span class="math"&gt;\(f''\)&lt;/span&gt; term disappear because &lt;span class="math"&gt;\(i^0\)&lt;/span&gt; and &lt;span class="math"&gt;\(i^2\)&lt;/span&gt; are real-valued.&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) = \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big] + \frac{\varepsilon^2}{3!} f'''(x) + \cdots
$$&lt;/div&gt;
&lt;p&gt;As usual, using a small &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; let's us throw out higher-order
terms. And, we arrive at the following approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;If instead, we take the real part and solve for &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, we get an approximation
to the function's value at &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) \approx \text{Re}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;In other words, a single (complex) function evaluations computes both the
function's value and the derivative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Higher-order function takes univariate function which computes a value and&lt;/span&gt;
&lt;span class="sd"&gt;    returns a function which returns value-derivative pair approximation.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;         &lt;span class="c1"&gt;# convert input to complex number&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imag&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# return function value and gradient&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simple test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="c1"&gt;# function&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# gradient&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Other comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using the complex-step method to estimate the gradients of multivariate
  functions requires independent approximations for each dimension of the
  input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although the complex-step approximation only requires a single function
  evaluation, it's unlikely faster than performing two function evaluations
  because operations on complex numbers are generally much slower than on floats
  or doubles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="calculus"></category></entry><entry><title>Gumbel max trick and weighted reservoir sampling</title><link href="http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/" rel="alternate"></link><published>2014-08-01T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-08-01:blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</id><summary type="html">&lt;p&gt;A while back, &lt;a href="http://people.cs.umass.edu/~wallach/"&gt;Hanna&lt;/a&gt; and I stumbled upon
the following blog post:
&lt;a href="http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm"&gt;Algorithms Every Data Scientist Should Know: Reservoir Sampling&lt;/a&gt;,
which got us excited about reservior sampling.&lt;/p&gt;
&lt;p&gt;Around the same time, I attended a talk by
&lt;a href="http://cs.haifa.ac.il/~tamir/"&gt;Tamir Hazan&lt;/a&gt; about some of his work on
perturb-and-MAP
&lt;a href="http://cs.haifa.ac.il/~tamir/papers/mean-width-icml12.pdf"&gt;(Hazan &amp;amp; Jaakkola, 2012)&lt;/a&gt;,
which is inspired by the
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;Gumbel-max-trick&lt;/a&gt;
(see &lt;a href="/blog/post/2014/07/31/gumbel-max-trick/"&gt;previous post&lt;/a&gt;). The apparent
similarity between weighted reservior sampling and the Gumbel max trick lead us
to make some cute connections, which I'll describe in this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We're given a stream of unnormalized probabilities, &lt;span class="math"&gt;\(x_1,
x_2, \cdots\)&lt;/span&gt;. At any point in time &lt;span class="math"&gt;\(t\)&lt;/span&gt; we'd like to have a sampled index &lt;span class="math"&gt;\(i\)&lt;/span&gt;
available, where the probability of &lt;span class="math"&gt;\(i\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(\pi_t(i) = \frac{x_i}{
\sum_{j=1}^t x_j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume, without loss of generality, that &lt;span class="math"&gt;\(x_i &amp;gt; 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. (If any element
has a zero weight we can safely ignore it since it should never be sampled.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Gumbel-max sampler&lt;/strong&gt;: I came up with the following algorithm, which
is a simple "modification" of the Gumbel-max-trick for handling streaming data:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}  \ \ \text{# maximum value and index}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Compute log-unnormalized probabilities&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(w_i = \log(x_i)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(z_i \sim \text{Gumbel}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(k_i = w_i + z_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; (i.e. the argmax)&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If we interrupt this algorithm at any point, we have a sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After convincing myself this algorithm was correct, I sat down to try to
understand the algorithm in the blog post, which is due to Efraimidis and
Spirakis (2005) (&lt;a href="http://dl.acm.org/citation.cfm?id=1138834"&gt;paywall&lt;/a&gt;,
&lt;a href="http://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf"&gt;free summary&lt;/a&gt;). They
looked similar in many ways but used different sorting keys / perturbations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efraimidis and Spirakis (2005)&lt;/strong&gt;: Here is the ES algorithm for weighted
reservior sampling&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate,&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(u_i \sim \text{Uniform}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(e_i = u_i^{(\frac{1}{x_i})}\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Again, if interrupt this algorithm at any point, we have our sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: Let's try to relate these algorithms. At a high level, both
algorithms compute a randomized key and take an argmax. What's the relationship
between the keys?&lt;/p&gt;
&lt;p&gt;First, note that a &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; variate can be generated via
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1)))\)&lt;/span&gt;. This is a straightforward application of
the
&lt;a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling"&gt;inverse transform sampling&lt;/a&gt;
method for random number generation. This means that if we use the same sequence
of uniform random variates then, &lt;span class="math"&gt;\(z_i = -\log(-\log(u_i))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, this does not give use equality between &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;, but it does
turn out that &lt;span class="math"&gt;\(k_i = -\log(-\log(e_i))\)&lt;/span&gt;, which is useful because this is a
monotonic transformation on the interval &lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;. Since monotonic
transformations preserve ordering, the sequences &lt;span class="math"&gt;\(k\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; result in the same
comparison decisions, as well as, the same argmax. In summary, the algorithms
are the same!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extensions&lt;/strong&gt;: After reading a little further along in the ES paper, we see
that the same algorithm can be used to perform &lt;em&gt;sampling without replacement&lt;/em&gt; by
sorting and taking the elements with the highest keys. This same modification is
applicable to the Gumbel-max-trick because the keys have exactly the same
ordering as ES. In practice we don't sort the key, but instead use a bounded
priority queue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing&lt;/strong&gt;: To the best of my knowledge, the connection between the
Gumbel-max-trick and ES is undocumented. Furthermore, the Gumbel-max-trick is
not known as a streaming algorithm, much less known to perform sampling without
replacement! If you know of a reference let me know. Perhaps, we'll finish
turning these connections into a
&lt;a href="https://github.com/timvieira/gumbel"&gt;short tech report&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="sampling"></category><category term="Gumbel"></category></entry><entry><title>Gumbel max trick</title><link href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/" rel="alternate"></link><published>2014-07-31T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-31:blog/post/2014/07/31/gumbel-max-trick/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Sampling from a discrete distribution parametrized by unnormalized
log-probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_k = \frac{1}{z} \exp(x_k)   \ \ \ \text{where } z = \sum_{j=1}^K \exp(x_j)
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The usual way&lt;/strong&gt;: Exponentiate and normalize (using the
&lt;a href="/blog/post/2014/02/11/exp-normalize-trick/"&gt;exp-normalize trick&lt;/a&gt;), then use the
an algorithm for sampling from a discrete distribution (aka categorical):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;usual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;     &lt;span class="c1"&gt;# the exp-normalize trick&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The Gumbel max trick&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \underset{ i \in \{1,\cdots,K\} }{\operatorname{argmax}} x_i + z_i
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_1 \cdots z_K\)&lt;/span&gt; are i.i.d. &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; random variates. It
turns out that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is distributed according to &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. (See the short derivations
in this
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;blog post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Implementing the Gumbel max trick is remarkable easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gumbel_max_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gumbel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't have access to a Gumbel random variate generator, you can use
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gumbel max requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; samples from a uniform. Usual requires only &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gumbel is one-pass because it does not require normalization (or a pass to
     compute the max for use in the exp-normalize trick). More on this in a
     later post!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Gumbel max trick requires &lt;span class="math"&gt;\(2K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt;, whereas ordinary
     requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log\)&lt;/span&gt; are expensive
     function, we'd like to avoid calling them. What gives? Well, Gumbel's calls
     to &lt;span class="math"&gt;\(\log\)&lt;/span&gt; do not depend on the data so they can be precomputed; this is
     handy for implementations which rely on vectorization for efficiency,
     e.g. python+numpy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;: I have a few posts relating to the Gumbel-max trick. Have a
look at &lt;a href="/blog/tag/gumbel.html"&gt;posts tagged with Gumbel&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="sampling"></category><category term="Gumbel"></category></entry><entry><title>Rant against grid search</title><link href="http://timvieira.github.io/blog/post/2014/07/22/rant-against-grid-search/" rel="alternate"></link><published>2014-07-22T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-22:blog/post/2014/07/22/rant-against-grid-search/</id><summary type="html">&lt;p&gt;Grid search is a simple and intuitive algorithm for optimizing and/or exploring
the effects of parameters to a function. However, given its rigid definition
grid search is susceptible to degenerate behavior. One type of unfortunate
behavior occurs in the presence of unimportant parameters, which results in many
(potentially expensive) function evaluations being wasted.&lt;/p&gt;
&lt;p&gt;This is a very simple point, but nonetheless I'll illustrate with a simple
example.&lt;/p&gt;
&lt;p&gt;Consider the following simple example, let's find the argmax of &lt;span class="math"&gt;\(f(x,y) = -x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we search over a &lt;span class="math"&gt;\(10\)&lt;/span&gt;-by-&lt;span class="math"&gt;\(10\)&lt;/span&gt; grid, resulting in a total of &lt;span class="math"&gt;\(100\)&lt;/span&gt;
function evaluations. For this function, we expect precision which proportional
of the number of samples in the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dimension, which is only &lt;span class="math"&gt;\(10\)&lt;/span&gt; samples! On
the other hand, randomly sampling points over the same space results in &lt;span class="math"&gt;\(100\)&lt;/span&gt;
samples in every dimension.&lt;/p&gt;
&lt;p&gt;In other words, randomly sample instead of using a rigid grid. If you have
points, which are not uniformly spaced, I'm willing to bet that an appropriate
probability distribution exists.&lt;/p&gt;
&lt;p&gt;This type of problem is common on hyperparameter optimizations. For futher
reading see
&lt;a href="http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;Bergstra &amp;amp; Bengio (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other thoughts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Local search is often much more effective. For example, gradient-based
   optimization, Nelder-Mead, stochastic local search, coordinate ascent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grid search tends to produce nicer-looking plots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What about variance in the results? Two things: (a) This is a concern for
   replicability, but is easily remedied by making sampled parameters
   available. (b) There is always some probability that the sampling gives you a
   terrible set of points. This shouldn't be a problem if you use enough
   samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="misc"></category></entry><entry><title>Expected value of a quadratic and the Delta method</title><link href="http://timvieira.github.io/blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/" rel="alternate"></link><published>2014-07-21T00:00:00-04:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-21:blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Expected value of a quadratic&lt;/strong&gt;: Suppose we'd like to compute the expectation
of a quadratic function, i.e.,
&lt;span class="math"&gt;\(\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]\)&lt;/span&gt; , where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is
a random vector and &lt;span class="math"&gt;\(A\)&lt;/span&gt; is deterministic &lt;em&gt;symmetric&lt;/em&gt; matrix. Let &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; be the mean and variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. It turns out the expected value of a
quadratic has the following simple form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]
=
\text{trace}\left( A \Sigma \right) + \mu^{\top}\negthinspace A \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Delta Method&lt;/strong&gt;: Suppose we'd like to compute expected value of a nonlinear
function &lt;span class="math"&gt;\(f\)&lt;/span&gt; applied our random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\mathbb{E}\left[ f(x) \right]\)&lt;/span&gt;. The Delta method approximates this expection by
replacing &lt;span class="math"&gt;\(f\)&lt;/span&gt; by it's second-order Talylor approximation &lt;span class="math"&gt;\(\hat{f_{a}}\)&lt;/span&gt; taken at
some point &lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{f_{a}}(x) = f(a) + \nabla f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^\top H(a) (x - a)
$$&lt;/div&gt;
&lt;p&gt;The expectation of this Talyor approximation is a quadratic function! Let's try
to apply our new equation for the expected value of quadratic. We can use the
trick from above with &lt;span class="math"&gt;\(A=H(a)\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = (x-a)\)&lt;/span&gt;. Note, covariance matrix is shift
invariant and the Hessian is a symmetric matrix!&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\mathbb{E}\left[ \hat{f_{a}}(x) \right]
 &amp;amp; = \mathbb{E} \left[ f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) + \frac{1}{2} \mathbb{E} \left[ (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) +
   \frac{1}{2}\left( \text{trace}\left( H(a) \, \Sigma \right) + (\mu - a)^{\top} H(a)\, (\mu - a) \right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;It's common to take the Taylor expansion around &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. This simplifies the equation&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\mathbb{E}\left[ \hat{f_{\mu}} (x) \right]
&amp;amp;= \mathbb{E}\left[ f(\mu) + \nabla\negthinspace f(\mu) (x - \mu) + \frac{1}{2} (x - \mu)^{\top} H(\mu)\, (x - \mu) \right] \\\
&amp;amp;= f(\mu) + \frac{1}{2} \, \text{trace}\Big( H(\mu) \, \Sigma \Big)
\end{aligned}&lt;/div&gt;
&lt;p&gt;That looks much more tractable! Error bounds are possible to derive, but outside
to scope of this post. For a nice use of the delta method in machine learning
see &lt;a href="http://arxiv.org/pdf/1307.1493v2.pdf"&gt;(Wager+,'13)&lt;/a&gt; and
&lt;a href="http://cs.jhu.edu/~jason/papers/smith+eisner.acl06-risk.pdf"&gt;(Smith &amp;amp; Eisner,'06)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category></entry><entry><title>Visualizing high-dimensional functions with cross-sections</title><link href="http://timvieira.github.io/blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/" rel="alternate"></link><published>2014-02-12T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-02-12:blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</id><summary type="html">&lt;p&gt;Last September, I gave a talk which included a bunch of two-dimensional plots of
a high-dimensional objective I was developing specialized algorithms for
optimizing. A month later, at least three of my colleagues told me that my plots
had inspired them to make similar plots. The plotting trick is really simple and
not original, but nonetheless I'll still write it up for all to enjoy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example plot&lt;/strong&gt;: This image shows cross-sections of two related functions: a
non-smooth (black) and a smooth approximating function (blue). The plot shows
that the approximation is faithful to the overall shape, but sometimes
over-smooths. In this case, we miss the maximum, which happens near the middle
of the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/blog/images/cross-section.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Let &lt;span class="math"&gt;\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)&lt;/span&gt; be a high-dimensional
function (&lt;span class="math"&gt;\(d \gg 2\)&lt;/span&gt;), which you'd like to visualize. Unfortunately, you are like
me and can't see in high-dimensions what do you do?&lt;/p&gt;
&lt;p&gt;One simple thing to do is take a nonzero vector &lt;span class="math"&gt;\(\boldsymbol{d} \in
\mathbb{R}^d\)&lt;/span&gt;, take a point of interest &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;, and build a local
picture of &lt;span class="math"&gt;\(f\)&lt;/span&gt; by evaluating it at various intervals along the chosen direction
as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$
f_i = f(\boldsymbol{x} + \alpha_i \ \boldsymbol{d}) \ \ \text{for } \alpha_i \in [\alpha_\min, \alpha_\max]
$$&lt;/div&gt;
&lt;p&gt;Of course, you'll have to pick a reasonable range and discretize it. Note,
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; are fixed for all &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;. Now, you can
plot &lt;span class="math"&gt;\((\alpha_i,f_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Picking directions&lt;/strong&gt;: There are many alternatives for picking
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;, my favorites are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gradient (if it exists), this direction is guaranteed to show a local
    increase/decrease in the objective, unless it's zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Coordinate vectors. Varying one dimension per plot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random. I recommend directions drawn from a spherical
    Gaussian.&lt;sup id="fnref:sphericalgaussian"&gt;&lt;a class="footnote-ref" href="#fn:sphericalgaussian" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; The reason being that such a vector is
    uniformly distributed across all unit-length directions (i.e., the angle of
    the vector, not it's length). We will vary the length ourselves via
    &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. It's probably best that our plots don't randomly vary in scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Extension to 3d&lt;/strong&gt;: It's pretty easy to extend this generating 3d plots by
using 2 vectors, &lt;span class="math"&gt;\(\boldsymbol{d_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d_2}\)&lt;/span&gt;, and varying two
parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{x} + \alpha \ \boldsymbol{d_1} + \beta \ \boldsymbol{d_2})
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Closing remarks&lt;/strong&gt;: These types of plots are probably best used to: empirically
verify/explore properties of an objective function, compare approximations, test
sensitivity to certain parameters/hyperparameters, visually debug optimization
algorithms.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:sphericalgaussian"&gt;
&lt;p&gt;More formally, vectors drawn from a spherical Gaussian are
points uniformly distributed on the surface of a &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional unit sphere,
&lt;span class="math"&gt;\(\mathbb{S}^d\)&lt;/span&gt;. Sampling a vector from a spherical Gaussian is straightforward:
sample &lt;span class="math"&gt;\(\boldsymbol{d'} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\boldsymbol{d} = \boldsymbol{d'} / \| \boldsymbol{d'} \|_2\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:sphericalgaussian" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="visualization"></category></entry><entry><title>Exp-normalize trick</title><link href="http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/" rel="alternate"></link><published>2014-02-11T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-02-11:blog/post/2014/02/11/exp-normalize-trick/</id><summary type="html">&lt;p&gt;This trick is the very close cousin of the infamous log-sum-exp trick
(&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html"&gt;scipy.misc.logsumexp&lt;/a&gt;),&lt;/p&gt;
&lt;p&gt;Supposed you'd like to evaluate a probability distribution &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt;
parametrized by a vector &lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}^n\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i = \frac{ \exp(x_i) }{ \sum_{j=1}^n \exp(x_j) }
$$&lt;/div&gt;
&lt;p&gt;The exp-normalize trick leverages the following identity to avoid numerical
overflow. For any &lt;span class="math"&gt;\(b \in \mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i
= \frac{ \exp(x_i - b) \exp(b) }{ \sum_{j=1}^n \exp(x_j - b) \exp(b) }
= \frac{ \exp(x_i - b) }{ \sum_{j=1}^n \exp(x_j - b) }
$$&lt;/div&gt;
&lt;p&gt;In other words, the &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is shift-invariant. A reasonable choice
is &lt;span class="math"&gt;\(b = \max_{i=1}^n x_i\)&lt;/span&gt;. With this choice, overflow due to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; is
impossible&lt;span class="math"&gt;\(-\)&lt;/span&gt;the largest number exponentiated after shifting is &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exp-normalize v. log-sum-exp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If what you want to remain in log-space, that is, compute
&lt;span class="math"&gt;\(\log(\boldsymbol{\pi})\)&lt;/span&gt;, you should use logsumexp. However, if
&lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is your goal, then exp-normalize trick is for you! Since it
avoids additional calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;, which would be required if using log-sum-exp
and more importantly exp-normalize is more numerically stable!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log-sum-exp for computing the log-distibution&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log \pi_i = x_i - \mathrm{logsumexp}(\boldsymbol{x})
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{logsumexp}(\boldsymbol{x}) = b + \log \sum_{j=1}^n \exp(x_j - b)
$$&lt;/div&gt;
&lt;p&gt;Typically with the same choice for &lt;span class="math"&gt;\(b\)&lt;/span&gt; as above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numerically-stable sigmoid function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sigmoid function can be computed with the exp-normalize trick in order to
avoid numerical overflow. In the case of &lt;span class="math"&gt;\(\text{sigmoid}(x)\)&lt;/span&gt;, we have a
distribution with unnormalized log probabilities &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt;, where we are only
interested in the probability of the first event. From the exp-normalize
identity, we know that the distributions &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt; and &lt;span class="math"&gt;\([0,-x]\)&lt;/span&gt; are equivalent (to
see why, plug in &lt;span class="math"&gt;\(b=\max(0,x)\)&lt;/span&gt;). This is why sigmoid is often expressed in one
of two equivalent ways:&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{sigmoid}(x) = 1/(1+\exp(-x)) = \exp(x) / (\exp(x) + 1)
$$&lt;/div&gt;
&lt;p&gt;Interestingly, each version covers an extreme case: &lt;span class="math"&gt;\(x=\infty\)&lt;/span&gt; and &lt;span class="math"&gt;\(x=-\infty\)&lt;/span&gt;,
respectively. Below is some python code which implements the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Numerically-stable sigmoid function.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# if x is less than zero then z will be small, denom can&amp;#39;t be&lt;/span&gt;
        &lt;span class="c1"&gt;# zero because it&amp;#39;s 1+z.&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="numerical"></category></entry><entry><title>Gradient-vector product</title><link href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/" rel="alternate"></link><published>2014-02-10T00:00:00-05:00</published><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-02-10:blog/post/2014/02/10/gradient-vector-product/</id><summary type="html">&lt;p&gt;We've all written the following test for our gradient code (known as the
finite-difference approximation).&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial x_i} f(\boldsymbol{x}) \approx
 \frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{e_i})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{e_i})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt; is a vector of zeros except at
&lt;span class="math"&gt;\(i\)&lt;/span&gt; where it is &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This approximation is exact in the limit, and accurate to
&lt;span class="math"&gt;\(o(\varepsilon^2)\)&lt;/span&gt; additive error.&lt;/p&gt;
&lt;p&gt;This is a specific instance of a more general approximation! The dot product of
the gradient and any (conformable) vector &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; can be approximated
with the following formula,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla f(\boldsymbol{x})^{\top} \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;We get the special case above when &lt;span class="math"&gt;\(\boldsymbol{d}=\boldsymbol{e_i}\)&lt;/span&gt;. This also
exact in the limit and just as accurate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Runtime?&lt;/strong&gt; Finite-difference approximation is probably too slow for
  approximating a high-dimensional gradient because the number of function
  evaluations required is &lt;span class="math"&gt;\(2 n\)&lt;/span&gt; where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the dimensionality of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. However,
  if the end goal is to approximate a gradient-vector product, a mere &lt;span class="math"&gt;\(2\)&lt;/span&gt;
  function evaluations is probably faster than specialized code for computing
  the gradient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to set &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;?&lt;/strong&gt; The second approach is more sensitive to
  &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; because &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; is arbitrary, unlike
  &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt;, which is a simple unit-norm vector. Luckily some guidance
  is available. Andrei (2009) reccommends&lt;/p&gt;
&lt;div class="math"&gt;$$
\varepsilon = \sqrt{\epsilon_{\text{mach}}} (1 + \|\boldsymbol{x} \|_{\infty}) / \| \boldsymbol{d} \|_{\infty}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\epsilon_{\text{mach}}\)&lt;/span&gt; is
&lt;a href="http://en.wikipedia.org/wiki/Machine_epsilon"&gt;machine epsilon&lt;/a&gt;. (Numpy users:
&lt;code&gt;numpy.finfo(x.dtype).eps&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Why do I care?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Well, I tend to work on sparse, but high-dimensional problems where
   finite-difference would be too slow. Thus, my usual solution is to only test
   several randomly selected dimensions&lt;span class="math"&gt;\(-\)&lt;/span&gt;biasing samples toward dimensions
   which should be nonzero. With the new trick, I can effectively test more
   dimensions at once by taking random vectors &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;. I recommend
   sampling &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; from a spherical Gaussian so that we're uniform on
   the angle of the vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sometimes the gradient-vector dot product is the end goal. This is the case
   with Hessian-vector products, which arises in many optimization algorithms,
   such as stochastic meta descent. Hessian-vector products are an instance of
   the gradient-vector dot product because the Hessian is just the gradient of
   the gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Hessian-vector product&lt;/h2&gt;
&lt;p&gt;Hessian-vector products are an instance of the gradient-vector dot product
because since the Hessian is just the gradient of the gradient! Now you only
need to remember one formula!&lt;/p&gt;
&lt;div class="math"&gt;$$
H(\boldsymbol{x})\, \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
  \nabla f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
- \nabla f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
\Big)
$$&lt;/div&gt;
&lt;p&gt;With this trick you never have to actually compute the gnarly Hessian! More on
&lt;a href="http://justindomke.wordpress.com/2009/01/17/hessian-vector-products/"&gt;Justin Domke's blog&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="math"></category><category term="calculus"></category></entry></feed>