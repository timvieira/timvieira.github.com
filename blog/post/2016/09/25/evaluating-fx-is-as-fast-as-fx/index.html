<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Evaluating ∇f(x) is as fast as f(x) &mdash; Graduate Descent</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="../../../../../favicon.png" rel="icon">

  <link href="../../../../../theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="../../../../../">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/index.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Evaluating ∇f(x) is as fast as f(x)</h1>
    <p class="meta">
<time datetime="2016-09-25T00:00:00-04:00" pubdate>Sep 25, 2016</time>    </p>
</header>

  <div class="entry-content"><p>Automatic differentiation ('autodiff' or 'backprop') is great&mdash;not just
because it makes it easy to rapidly prototype deep networks with plenty of
doodads and geegaws, but because it means that evaluating the gradient <span class="math">\(\nabla
f(x)\)</span> is as fast of computing <span class="math">\(f(x)\)</span>. In fact, the gradient provably requires at
most a <em>small</em> constant factor more arithmetic operations than the function
itself.  Furthermore, autodiff tells us how to derive and implement the gradient
efficiently. This is a fascinating result that is perhaps not emphasized enough
in machine learning.</p>
<p><strong>The gradient should never be asymptotically slower than the function.</strong> In my
recent <a href="/doc/2016-emnlp-vocrf.pdf">EMNLP'16 paper</a>, my coauthors and I found a
line of work on variable-order CRFs
(<a href="https://papers.nips.cc/paper/3815-conditional-random-fields-with-high-order-features-for-sequence-labeling.pdf">Ye+'09</a>;
<a href="http://www.jmlr.org/papers/volume15/cuong14a/cuong14a.pdf">Cuong+'14</a>), which
had an unnecessarily slow and complicated algorithm for computing gradients,
which was asymptotically (and practically) slower than their forward
algorithm. Without breaking a sweat, we derived a simpler and more efficient
gradient algorithm by simply applying backprop to the forward algorithm (and
made some other contributions).</p>
<p><strong>Many algorithms are just backprop.</strong> For example, forward-backward and
inside-outside, are actually just instances of automatic differentiation
(<a href="https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf">Eisner,'16</a>) (i.e.,
outside is just backprop on inside). This shouldn't be a surprise because these
algorithms are used to compute gradients. Basically, if you know backprop and
the inside algorithm, then you can derive the outside algorithm by applying the
backprop transform manually. I find it easier to understand the outside
algorithm via its connection to backprop, then via
<a href="https://www.cs.jhu.edu/~jason/465/iobasics.pdf">the usual presentation</a>. Note
that inside-outside and forward-backward pre-date backpropagation and have
additional uses beyond computing gradients.</p>
<p><strong>Once you've grokked backprop, the world is your oyster!</strong> You can backprop
through many approximate inference algorithms, e.g.,
<a href="http://www.jmlr.org/proceedings/papers/v15/stoyanov11a/stoyanov11a.pdf">Stoyanov+'11</a>
and many of Justin Domke's papers, to avoid issues I've mentioned
<a href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/">before</a>. You
can even backprop through optimization algorithms to get gradients of dev loss wrt
hyperparameters, e.g.,
<a href="http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf">Domke'12</a> and
<a href="https://arxiv.org/abs/1502.03492">Maclaurin+'15</a>.</p>
<p><strong>There's at least one catch!</strong> Although the <em>time</em> complexity of computing the
gradient is as good as the function, the <em>space</em> complexity may be much larger
because the autodiff recipe (at least the default reverse-mode one) requires memoizing
all intermediate quantities (e.g., the quantities you overwrite in a
loop). There are generic methods for balancing the time-space tradeoff in
autodiff, since you can (at least in theory) reconstruct the intermediate
quantities by playing the forward computation again from intermediate
checkpoints (at a cost to runtime, of course). A recent example is
<a href="https://arxiv.org/abs/1606.03401">Gruslys+'16</a>.</p>
<p><strong>A final remark</strong>. Despite the name "automatic" differentiation, there is no
need to rely on software to "automatically" give you gradient routines. Applying
the backprop transformation is generally easy to do manually and sometimes more
efficient than using a library. Many autodiff libraries lack good support for
dynamic computation graph, i.e., when the structure depends on quantities that
vary with the input (e.g., sentence length).</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Tim Vieira
    </span>
  </span>
<time datetime="2016-09-25T00:00:00-04:00" pubdate>Sep 25, 2016</time>  <span class="categories">
    <a class='category' href='../../../../../category/misc.html'>misc</a>
  </span>
  <span class="categories">
    <a class="category" href="../../../../../tag/calculus.html">calculus</a>,    <a class="category" href="../../../../../tag/automatic-differentiation.html">automatic-differentiation</a>,    <a class="category" href="../../../../../tag/rant.html">rant</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="../../../../../post/2019/06/11/faster-reservoir-sampling-by-waiting/">Faster reservoir sampling by waiting</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2019/04/20/the-likelihood-ratio-gradient/">The likelihood-ratio gradient</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2019/04/19/steepest-ascent/">Steepest ascent</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2018/03/16/black-box-optimization/">Black-box optimization</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="../../../../../tag/sampling.html">sampling</a>,    <a href="../../../../../tag/reservoir-sampling.html">reservoir-sampling</a>,    <a href="../../../../../tag/gumbel.html">Gumbel</a>,    <a href="../../../../../tag/sampling-without-replacement.html">sampling-without-replacement</a>,    <a href="../../../../../tag/optimization.html">optimization</a>,    <a href="../../../../../tag/rl.html">rl</a>,    <a href="../../../../../tag/machine-learning.html">machine-learning</a>,    <a href="../../../../../tag/notebook.html">notebook</a>,    <a href="../../../../../tag/calculus.html">calculus</a>,    <a href="../../../../../tag/automatic-differentiation.html">automatic-differentiation</a>,    <a href="../../../../../tag/implicit-function-theorem.html">implicit-function-theorem</a>,    <a href="../../../../../tag/lagrange-multipliers.html">Lagrange-multipliers</a>,    <a href="../../../../../tag/statistics.html">statistics</a>,    <a href="../../../../../tag/testing.html">testing</a>,    <a href="../../../../../tag/counterfactual-reasoning.html">counterfactual-reasoning</a>,    <a href="../../../../../tag/importance-sampling.html">importance-sampling</a>,    <a href="../../../../../tag/datastructures.html">datastructures</a>,    <a href="../../../../../tag/incremental-computation.html">incremental-computation</a>,    <a href="../../../../../tag/algorithms.html">algorithms</a>,    <a href="../../../../../tag/data-structures.html">data-structures</a>,    <a href="../../../../../tag/rant.html">rant</a>,    <a href="../../../../../tag/decision-making.html">decision-making</a>,    <a href="../../../../../tag/hyperparameter-optimization.html">hyperparameter-optimization</a>,    <a href="../../../../../tag/numerical.html">numerical</a>,    <a href="../../../../../tag/crf.html">crf</a>,    <a href="../../../../../tag/deep-learning.html">deep-learning</a>,    <a href="../../../../../tag/structured-prediction.html">structured-prediction</a>,    <a href="../../../../../tag/visualization.html">visualization</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014&ndash;2019  Tim Vieira &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="../../../../../theme/js/modernizr-2.0.js"></script>
  <script src="../../../../../theme/js/ender.js"></script>
  <script src="../../../../../theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'graduatedescent';
    var disqus_identifier = '/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/';
    var disqus_url = '../../../../../post/2016/09/25/evaluating-fx-is-as-fast-as-fx/';
    var disqus_title = 'Evaluating ∇f(x) is as fast as f(x)';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-8781169-1', 'auto');
  ga('send', 'pageview');
</script>

  
</body>
</html>