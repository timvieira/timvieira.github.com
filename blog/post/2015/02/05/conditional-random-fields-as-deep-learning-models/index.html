<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Conditional random fields as deep learning models? &mdash; Graduate Descent</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="../../../../../favicon.png" rel="icon">

  <link href="../../../../../theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="../../../../../">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/archives.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Conditional random fields as deep learning models?</h1>
    <p class="meta">
<time datetime="2015-02-05T00:00:00-05:00" pubdate>Feb 05, 2015</time>    </p>
</header>

  <div class="entry-content"><p>This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.</p>
<p>In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:</p>
<ol>
<li>
<p>Limit the expressiveness of the model (i.e., don't use to model you want)</p>
</li>
<li>
<p>Change the training objective</p>
</li>
<li>
<p>Approximate inference</p>
</li>
</ol>
<p>Approximate inference is tricky. Things can easily go awry.</p>
<p>For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
<a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf">(Kulesza &amp; Pereira, 2007)</a>. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
<a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf">sometimes</a> these loopy BP
approximations work fine.)</p>
<p>It looks like using approximate inference during training changes the training
objective.</p>
<p>So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.</p>
<p>Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp; Eisner.</p>
<p>Unsuprisingly, it works pretty well.</p>
<p>I first saw this idea in
<a href="http://proceedings.mlr.press/v15/stoyanov11a/stoyanov11a.pdf">Stoyanov &amp; Eisner (2011)</a>. They
use loopy belief propagation as their approximate inference algorithm. At the
end of the day, their model is essentially a deep recurrent network, which came
from unrolling inference in a graphical model. This idea really struck me
because it's clearly right in the middle between graphical models and deep
learning.</p>
<p>You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.</p>
<p>Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.</p></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Tim Vieira
    </span>
  </span>
<time datetime="2015-02-05T00:00:00-05:00" pubdate>Feb 05, 2015</time>  <span class="categories">
    <a class='category' href='../../../../../category/misc.html'>misc</a>
  </span>
  <span class="categories">
    <a class="category" href="../../../../../tag/machine-learning.html">machine-learning</a>,    <a class="category" href="../../../../../tag/deep-learning.html">deep-learning</a>,    <a class="category" href="../../../../../tag/structured-prediction.html">structured-prediction</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="../../../../../post/2019/04/20/the-likelihood-ratio-gradient/">The likelihood-ratio gradient</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2019/04/19/steepest-ascent/">Steepest ascent</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2018/03/16/black-box-optimization/">Black-box optimization</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a>
      </li>
      <li class="post">
          <a href="../../../../../post/2017/07/03/estimating-means-in-a-finite-universe/">Estimating means in a finite universe</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="../../../../../tag/optimization.html">optimization</a>,    <a href="../../../../../tag/rl.html">rl</a>,    <a href="../../../../../tag/machine-learning.html">machine-learning</a>,    <a href="../../../../../tag/notebook.html">notebook</a>,    <a href="../../../../../tag/calculus.html">calculus</a>,    <a href="../../../../../tag/automatic-differentiation.html">automatic-differentiation</a>,    <a href="../../../../../tag/sampling.html">sampling</a>,    <a href="../../../../../tag/statistics.html">statistics</a>,    <a href="../../../../../tag/reservoir-sampling.html">reservoir-sampling</a>,    <a href="../../../../../tag/testing.html">testing</a>,    <a href="../../../../../tag/counterfactual-reasoning.html">counterfactual-reasoning</a>,    <a href="../../../../../tag/importance-sampling.html">importance-sampling</a>,    <a href="../../../../../tag/datastructures.html">datastructures</a>,    <a href="../../../../../tag/algorithms.html">algorithms</a>,    <a href="../../../../../tag/rant.html">rant</a>,    <a href="../../../../../tag/gumbel.html">Gumbel</a>,    <a href="../../../../../tag/decision-making.html">decision-making</a>,    <a href="../../../../../tag/hyperparameter-optimization.html">hyperparameter-optimization</a>,    <a href="../../../../../tag/misc.html">misc</a>,    <a href="../../../../../tag/numerical.html">numerical</a>,    <a href="../../../../../tag/crf.html">crf</a>,    <a href="../../../../../tag/deep-learning.html">deep-learning</a>,    <a href="../../../../../tag/structured-prediction.html">structured-prediction</a>,    <a href="../../../../../tag/visualization.html">visualization</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014&ndash;2019  Tim Vieira &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="../../../../../theme/js/modernizr-2.0.js"></script>
  <script src="../../../../../theme/js/ender.js"></script>
  <script src="../../../../../theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'graduatedescent';
    var disqus_identifier = '/post/2015/02/05/conditional-random-fields-as-deep-learning-models/';
    var disqus_url = '../../../../../post/2015/02/05/conditional-random-fields-as-deep-learning-models/';
    var disqus_title = 'Conditional random fields as deep learning models?';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-8781169-1', 'auto');
  ga('send', 'pageview');
</script>

  
</body>
</html>