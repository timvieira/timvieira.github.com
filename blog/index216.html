<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Graduate Descent</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="./favicon.png" rel="icon">

  <link href="./theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="./">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/index.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div class="blog-index">
  		<article>
<header>
      <h1 class="entry-title">
        <a href="./post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/">Gradient-based hyperparameter optimization and the implicit function theorem</a>
      </h1>
    <p class="meta">
<time datetime="2016-03-05T00:00:00-05:00" pubdate>Mar 05, 2016</time>    </p>
</header>

  <div class="entry-content"><p>The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization&mdash;the "inner" optimization optimizes training loss (wrt <span class="math">\(\theta\)</span>),
while the "outer" optimizes hyperparameters (<span class="math">\(\lambda\)</span>).</p>
<div class="math">$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$</div>
<p>Can we estimate <span class="math">\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)</span> so
that we can run gradient-based optimization over <span class="math">\(\lambda\)</span>?</p>
<p>Well, what does it mean to have an <span class="math">\(\textbf{argmin}\)</span> inside a function?</p>
<p>Well, it means that there is a <span class="math">\(\theta^*\)</span> that gets passed to
<span class="math">\(\mathcal{L}_{\text{dev}}\)</span>. And, <span class="math">\(\theta^*\)</span> is a function of <span class="math">\(\lambda\)</span>, denoted
<span class="math">\(\theta(\lambda)\)</span>. Furthermore, <span class="math">\(\textbf{argmin}\)</span> must set the derivative of the
inner optimization to zero in order to be a local optimum of the inner
function. So we can rephrase the problem as</p>
<div class="math">$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$</div>
<p>
where <span class="math">\(\theta(\lambda)\)</span> is the solution to,
</p>
<div class="math">$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$</div>
<p>Now how does <span class="math">\(\theta\)</span> change as the result of an infinitesimal change to
<span class="math">\(\lambda\)</span>?</p>
<p>The constraint on the derivative implies a type of "equilibrium"&mdash;the inner
optimization process will continue to optimize regardless of how we change
<span class="math">\(\lambda\)</span>. Assuming we don't change <span class="math">\(\lambda\)</span> too much, then the inner
optimization shouldn't change <span class="math">\(\theta\)</span> too much and it will change in a
predictable way.</p>
<p>To do this, we'll appeal to the implicit function theorem. Let's look at the
general case to simplify notation. Suppose <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are related through a
function <span class="math">\(g\)</span> as follows,</p>
<div class="math">$$g(x,y) = 0.$$</div>
<p>Assuming <span class="math">\(g\)</span> is a smooth function in <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, we can perturb either
argument, say <span class="math">\(x\)</span> by a small amount <span class="math">\(\Delta_x\)</span> and <span class="math">\(y\)</span> by <span class="math">\(\Delta_y\)</span>. Because
system preserves the constraint, i.e.,</p>
<div class="math">$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$</div>
<p>We can solve for the change of <span class="math">\(x\)</span> as a result of an infinitesimal change in
<span class="math">\(y\)</span>. We take the first-order expansion,</p>
<div class="math">$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$</div>
<p>Since <span class="math">\(g(x,y)\)</span> is already zero,</p>
<div class="math">$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$</div>
<p>Next, we solve for <span class="math">\(\frac{\Delta_x}{\Delta_y}\)</span>.</p>
<div class="math">$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$</div>
<div class="math">$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$</div>
<p>Back to the original problem: Now we can use the implicit function theorem to
estimate how <span class="math">\(\theta\)</span> varies in <span class="math">\(\lambda\)</span> by plugging in <span class="math">\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)</span>, <span class="math">\(x \mapsto \theta\)</span>
and <span class="math">\(y \mapsto \lambda\)</span>:</p>
<div class="math">$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$</div>
<p>This tells us how <span class="math">\(\theta\)</span> changes with respect to an infinitesimal change to
<span class="math">\(\lambda\)</span>. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt <span class="math">\(\lambda\)</span>,</p>
<div class="math">$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$</div>
<p>Since we don't like (explicit) matrix inverses, we compute <span class="math">\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)</span> as the solution to <span class="math">\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)</span>. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products&mdash;i.e., you never have to materialize the Hessian. (Apparently,
<a href="https://en.wikipedia.org/wiki/Matrix-free_methods">matrix-free linear algebra</a>
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
<a href="/blog/post/2014/02/10/gradient-vector-product/">earlier post</a>).</p>
<p>At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.</p>
<p>In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.</p>
<p><strong>Further reading</strong>:</p>
<ul>
<li>
<p><a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/">Truncated Bi-Level Optimization</a></p>
</li>
<li>
<p><a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf">Efficient multiple hyperparameter learning for log-linear models</a></p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1502.03492">Gradient-based Hyperparameter Optimization through Reversible Learning</a></p>
</li>
<li>
<p><a href="http://fa.bianp.net/blog/2016/hyperparameter-optimization-with-approximate-gradient/">Hyperparameter optimization with approximate gradient</a>
   (<a href="https://arxiv.org/pdf/1602.02355.pdf">paper</a>): This paper looks at the implicit
   differentiation approach where you have an <em>approximate</em>
   solution to the inner optimization problem. They are able to provide error bounds and
   convergence guarantees under some reasonable conditions.</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
  		</article>
<div class="pagination">
    <a class="prev" href="./index217.html">&larr; Older</a>

    <a class="next" href="./index215.html">Newer &rarr;</a>
  <br />
</div></div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="./post/2019/06/11/faster-reservoir-sampling-by-waiting/">Faster reservoir sampling by waiting</a>
      </li>
      <li class="post">
          <a href="./post/2019/04/20/the-likelihood-ratio-gradient/">The likelihood-ratio gradient</a>
      </li>
      <li class="post">
          <a href="./post/2019/04/19/steepest-ascent/">Steepest ascent</a>
      </li>
      <li class="post">
          <a href="./post/2018/03/16/black-box-optimization/">Black-box optimization</a>
      </li>
      <li class="post">
          <a href="./post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="./tag/sampling.html">sampling</a>,    <a href="./tag/reservoir-sampling.html">reservoir-sampling</a>,    <a href="./tag/gumbel.html">Gumbel</a>,    <a href="./tag/sampling-without-replacement.html">sampling-without-replacement</a>,    <a href="./tag/optimization.html">optimization</a>,    <a href="./tag/rl.html">rl</a>,    <a href="./tag/machine-learning.html">machine-learning</a>,    <a href="./tag/notebook.html">notebook</a>,    <a href="./tag/calculus.html">calculus</a>,    <a href="./tag/automatic-differentiation.html">automatic-differentiation</a>,    <a href="./tag/implicit-function-theorem.html">implicit-function-theorem</a>,    <a href="./tag/lagrange-multipliers.html">Lagrange-multipliers</a>,    <a href="./tag/statistics.html">statistics</a>,    <a href="./tag/testing.html">testing</a>,    <a href="./tag/counterfactual-reasoning.html">counterfactual-reasoning</a>,    <a href="./tag/importance-sampling.html">importance-sampling</a>,    <a href="./tag/datastructures.html">datastructures</a>,    <a href="./tag/incremental-computation.html">incremental-computation</a>,    <a href="./tag/algorithms.html">algorithms</a>,    <a href="./tag/data-structures.html">data-structures</a>,    <a href="./tag/rant.html">rant</a>,    <a href="./tag/decision-making.html">decision-making</a>,    <a href="./tag/hyperparameter-optimization.html">hyperparameter-optimization</a>,    <a href="./tag/numerical.html">numerical</a>,    <a href="./tag/crf.html">crf</a>,    <a href="./tag/deep-learning.html">deep-learning</a>,    <a href="./tag/structured-prediction.html">structured-prediction</a>,    <a href="./tag/visualization.html">visualization</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014&ndash;2019  Tim Vieira &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="./theme/js/modernizr-2.0.js"></script>
  <script src="./theme/js/ender.js"></script>
  <script src="./theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'graduatedescent';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-8781169-1', 'auto');
  ga('send', 'pageview');
</script>

  
</body>
</html>