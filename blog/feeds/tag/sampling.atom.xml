<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Graduate Descent - sampling</title><link href="https://timvieira.github.io/blog/" rel="alternate"></link><link href="/blog/feeds/tag/sampling.atom.xml" rel="self"></link><id>https://timvieira.github.io/blog/</id><updated>2020-06-30T00:00:00-04:00</updated><entry><title>Animation of the inverse transform method</title><link href="https://timvieira.github.io/blog/post/2020/06/30/animation-of-the-inverse-transform-method/" rel="alternate"></link><published>2020-06-30T00:00:00-04:00</published><updated>2020-06-30T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2020-06-30:/blog/post/2020/06/30/animation-of-the-inverse-transform-method/</id><content type="html">&lt;p&gt;{% notebook inverse-transform-method.ipynb cells[1:] %}&lt;/p&gt;</content><category term="misc"></category><category term="notebook"></category><category term="sampling"></category><category term="statistics"></category></entry><entry><title>Generating truncated random variates</title><link href="https://timvieira.github.io/blog/post/2020/06/30/generating-truncated-random-variates/" rel="alternate"></link><published>2020-06-30T00:00:00-04:00</published><updated>2020-06-30T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2020-06-30:/blog/post/2020/06/30/generating-truncated-random-variates/</id><content type="html">&lt;p&gt;{% notebook truncated-random-variates.ipynb cells[1:] %}&lt;/p&gt;</content><category term="misc"></category><category term="notebook"></category><category term="sampling"></category><category term="statistics"></category></entry><entry><title>Algorithms for sampling without replacement</title><link href="https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/" rel="alternate"></link><published>2019-09-16T00:00:00-04:00</published><updated>2019-09-16T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2019-09-16:/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/</id><content type="html">&lt;p&gt;{% notebook swor-algs.ipynb cells[1:] %}&lt;/p&gt;</content><category term="misc"></category><category term="notebook"></category><category term="sampling"></category><category term="algorithms"></category><category term="sampling-without-replacement"></category><category term="Gumbel"></category></entry><entry><title>Faster reservoir sampling by waiting</title><link href="https://timvieira.github.io/blog/post/2019/06/11/faster-reservoir-sampling-by-waiting/" rel="alternate"></link><published>2019-06-11T00:00:00-04:00</published><updated>2019-06-11T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2019-06-11:/blog/post/2019/06/11/faster-reservoir-sampling-by-waiting/</id><content type="html">&lt;p&gt;We are interested in designing an efficient algorithm for sampling from a categorical distribution over &lt;span class="math"&gt;\(n\)&lt;/span&gt; items with weights &lt;span class="math"&gt;\(w_i &amp;gt; 0\)&lt;/span&gt;.  Define target sampling distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; as
&lt;/p&gt;
&lt;div class="math"&gt;$$
p = \mathrm{Categorical}\left( \frac{1}{W} \cdot \vec{w} \right)
\quad\text{where}\quad W = \sum_j w_j
$$&lt;/div&gt;
&lt;p&gt;The following is a very simple and relatively famous algorithm due to &lt;a href="https://www.sciencedirect.com/science/article/pii/S002001900500298X"&gt;Efraimidis and Spirakis (2006)&lt;/a&gt;.  It has several useful properties (e.g., it is a one-pass "streaming" algorithm, separates data from noise, can be easily extended for streaming sampling without replacement).  It is also very closely related to the Gumbel-max trick (&lt;a href="http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/"&gt;Vieira,  2014&lt;/a&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weighted_reservoir_sampling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;Exponential&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Some cosmetic differences from E&amp;amp;S'06: We use exponential random variates and &lt;span class="math"&gt;\(\min\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(\max\)&lt;/span&gt;. E&amp;amp;S'06 use a less-elegant and rather-mysterious (IMO) random key &lt;span class="math"&gt;\(u_i^{1/w_i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why does it work?&lt;/strong&gt; The weighted-reservoir sampling algorithm exploits the following well-known properties of exponential random variates:
When &lt;span class="math"&gt;\(X_i \sim \mathrm{Exponential}(w_i)\)&lt;/span&gt;, &lt;span class="math"&gt;\(R = {\mathrm{argmin}}_i X_i\)&lt;/span&gt;, and &lt;span class="math"&gt;\(T = \min_i X_i\)&lt;/span&gt; then
&lt;span class="math"&gt;\(R \sim p\)&lt;/span&gt; and &lt;span class="math"&gt;\(T \sim \mathrm{Exponential}\left( \sum_i w_i \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Fewer random variates by waiting&lt;/h2&gt;
&lt;p&gt;One down-side of this one-pass algorithm is that it requires &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt; uniform random variates.  Contrast that with the usual, two-pass methods for sampling from a categorical distribution, which only need &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt; samples.  E&amp;amp;S'06 also present a much less well-known algorithm, called the "Exponential jumps" algorithm, which is a one-pass algorithm that only requires &lt;span class="math"&gt;\(\mathcal{O}(\log(n))\)&lt;/span&gt; random variates (in expectation).  That's &lt;em&gt;way&lt;/em&gt; fewer random variates and a small price to pay if you are trying to avoid paging-in data from disk a second time.&lt;/p&gt;
&lt;p&gt;Here is my take on their algorithm.  There is no substantive difference, but I believe my version is more instructive since it makes the connection to exponential variates and truncated generation explicit (i.e., no mysterious random keys).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;jump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Weighted-reservoir sampling by jumping&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;
    &lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Sample the key for item i, given that it is smaller than the current threshold&lt;/span&gt;
            &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Exponential&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_truncated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# i enters the reservoir&lt;/span&gt;
            &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
            &lt;span class="c1"&gt;# sample the waiting time (size of the jump)&lt;/span&gt;
            &lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Exponential&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Why does exponential jumps work?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let me first write the &lt;code&gt;weighted_reservoir_sampling&lt;/code&gt; algorithm to be much more similar to the &lt;code&gt;jump&lt;/code&gt; algorithm.  For fun, I'm going to refer to it as the &lt;code&gt;walk&lt;/code&gt; algorithm.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;walk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Weighted-reservoir sampling by walking&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;
    &lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Exponential&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;   &lt;span class="c1"&gt;# i enters the reservoir&lt;/span&gt;
            &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;   &lt;span class="c1"&gt;# threshold to enter the reservoir&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The key idea&lt;/strong&gt; of the exponential jumps algorithm is to sample &lt;em&gt;waiting times&lt;/em&gt; between new minimum events.  In particular, if the algorithm is at step &lt;span class="math"&gt;\(i\)&lt;/span&gt; the probability that it sees its next minimum at steps &lt;span class="math"&gt;\(j \in \{ i+1, \ldots \}\)&lt;/span&gt; can be reasoned about without needing to &lt;em&gt;actually&lt;/em&gt; sample the various &lt;span class="math"&gt;\(X_j\)&lt;/span&gt; variables.&lt;/p&gt;
&lt;p&gt;Rather than going into a full-blown tutorial on waiting times of exponential variates, I will get to the point and show that the &lt;code&gt;jump&lt;/code&gt; algorithm simulates the &lt;code&gt;walk&lt;/code&gt; algorithm.  The key to doing this is showing that the probability of jumping from &lt;span class="math"&gt;\(i\)&lt;/span&gt; to &lt;span class="math"&gt;\(k\)&lt;/span&gt; is the same as "walking" from &lt;span class="math"&gt;\(i\)&lt;/span&gt; to &lt;span class="math"&gt;\(k\)&lt;/span&gt;.  Let &lt;span class="math"&gt;\(W_{i,k} = \sum_{j=i}^k w_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This proof is adapted from the original proof in E&amp;amp;S'06.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
\mathrm{Pr}\left( \text{walk to } k \mid i,T \right)
&amp;amp;=&amp;amp; \mathrm{Pr}\left( X_k &amp;lt; T \right) \prod_{j=i}^{k-1} \mathrm{Pr}\left( X_j \ge T \right) \\
&amp;amp;=&amp;amp; \left(1 - \exp\left( T w_k \right) \right) \prod_{j=i}^{k-1} \exp\left( T w_j \right) \\
&amp;amp;=&amp;amp; \left(1 - \exp\left( T w_k \right) \right) \exp\left( T \sum_{j=i}^{k-1}  w_j \right) \\
&amp;amp;=&amp;amp; \left(1 - \exp\left( T w_k \right) \right) \exp\left( T W_{i,k-1} \right) \\
&amp;amp;=&amp;amp; \exp\left( T W_{i,k-1} \right) - \exp\left( T w_k \right) \exp\left( T W_{i,k-1} \right) \\
&amp;amp;=&amp;amp; \exp\left( T W_{i,k-1} \right) - \exp\left( T W_{i,k} \right) \\
\\
\mathrm{Pr}\left( \text{jump to } k \mid i, T \right)
&amp;amp;=&amp;amp; \mathrm{Pr}\left( W_{i,k-1} &amp;lt; J \le W_{i,k} \right) \\
&amp;amp;=&amp;amp; \mathrm{Pr}\left( W_{i,k-1} &amp;lt; -\frac{\log(U)}{T} \le W_{i,k} \right) \\
&amp;amp;=&amp;amp; \mathrm{Pr}\left( \exp(-T W_{i,k-1}) &amp;gt; U \ge \exp(-T W_{i,k}) \right) \label{foo}\\
&amp;amp;=&amp;amp; \exp(T W_{i,k-1}) - \exp(T W_{i,k} )
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;Given that the waiting time correctly matches the walking algorithm, the remaining detail is to check that &lt;span class="math"&gt;\(X_k\)&lt;/span&gt; is equivalent under the condition that it goes into the reservoir.  This conditioning is why the jumping algorithm must generate a &lt;em&gt;truncated&lt;/em&gt; random variate: a random variate that is guaranteed to less than the previous minimum.  In the &lt;a href="https://cmaddis.github.io/gumbel-machinery"&gt;Gumbel-max world&lt;/a&gt;, this is used in the top-down generative story.&lt;/p&gt;
&lt;h2&gt;Closing thoughts&lt;/h2&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The jump algorithm saves a ton of random variates and gives practical savings
  (at least, in my limited experiments).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The jump algorithm is harder to parallelize or vectorize, but it seems possible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you aren't in a setting that requires a one-pass algorithm or some other
  special properties, you are probably better served by the two-pass algorithms
  since they have lower overhead because it doesn't call expensive functions
  like &lt;span class="math"&gt;\(\log\)&lt;/span&gt; and it uses a single random variate per sample.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I have several posts on the topic of fast sampling algorithms
(&lt;a href="http://timvieira.github.io/blog/post/2016/11/21/heaps-for-incremental-computation/"&gt;1&lt;/a&gt;,
&lt;a href="http://timvieira.github.io/blog/post/2016/07/04/fast-sigmoid-sampling/"&gt;2&lt;/a&gt;,
&lt;a href="http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/"&gt;3&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jake VanderPlas (2018) &lt;a href="http://jakevdp.github.io/blog/2018/09/13/waiting-time-paradox/"&gt;The Waiting Time Paradox, or, Why Is My Bus Always Late?&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Interactive Notebook&lt;/h2&gt;
&lt;script src="https://gist.github.com/timvieira/44edfaf97cb2e191e4618f0d25401bf4.js"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="sampling"></category><category term="reservoir-sampling"></category><category term="Gumbel"></category><category term="sampling-without-replacement"></category></entry><entry><title>Estimating means in a finite universe</title><link href="https://timvieira.github.io/blog/post/2017/07/03/estimating-means-in-a-finite-universe/" rel="alternate"></link><published>2017-07-03T00:00:00-04:00</published><updated>2017-07-03T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2017-07-03:/blog/post/2017/07/03/estimating-means-in-a-finite-universe/</id><content type="html">&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;: In this post, I'm going to describe some efficient approaches
to estimating the mean of a random variable that takes on only finitely many
values. Despite the ubiquity of Monte Carlo estimation, it is really inefficient
for finite domains. I'll describe some lesser-known algorithms based on sampling
without replacement that can be adapted to estimating means.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we want to estimate an expectation of a derministic function
&lt;span class="math"&gt;\(f\)&lt;/span&gt; over a (large) finite universe of &lt;span class="math"&gt;\(n\)&lt;/span&gt; elements where each element &lt;span class="math"&gt;\(i\)&lt;/span&gt; has
probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu \overset{\tiny{\text{def}}}{=} \sum_{i=1}^n p_i f(i)
$$&lt;/div&gt;
&lt;p&gt;However, &lt;span class="math"&gt;\(f\)&lt;/span&gt; is too expensive to evaluate &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. So let's say that we have
&lt;span class="math"&gt;\(m \le n\)&lt;/span&gt; evaluations to form our estimate. (Obviously, if we're happy
evaluating &lt;span class="math"&gt;\(f\)&lt;/span&gt; a total of &lt;span class="math"&gt;\(n\)&lt;/span&gt; times, then we should just compute &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; exactly
with the definition above.)&lt;/p&gt;
&lt;!--
**Why I'm writing this post**: Monte Carlo is often used in designing algorithms
as a means to cheaply approximate intermediate expectations, think of stochastic
gradient descent as a prime example. However, in many cases, we have a *finite*
universe, i.e., we *could* enumerate all elements, but it's just inefficient to
do so. In other words, sampling is merely a choice made by the algorithm
designer, not a fundamental property of the environment, as it is typically in
statistics. What can we do to improve estimation in this special setting? I
won't get into bigger questions of how to design these algorithms, instead I'll
focus on this specific type of estimation problem.
--&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo:&lt;/strong&gt; The most well-known approach to this type of problem is Monte
Carlo (MC) estimation: sample &lt;span class="math"&gt;\(x^{(1)}, \ldots, x^{(m)}
\overset{\tiny\text{i.i.d.}}{\sim} p\)&lt;/span&gt;, return &lt;span class="math"&gt;\(\widehat{\mu}_{\text{MC}} =
\frac{1}{m} \sum_{i = 1}^m f(x^{(i)})\)&lt;/span&gt;. &lt;em&gt;Remarks&lt;/em&gt;: (1) Monte Carlo can be very
inefficient because it resamples high-probability items over and over again. (2)
We can improve efficiency&amp;mdash;measured in &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluations&amp;mdash;somewhat by
caching past evaluations of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. However, this introduces a serious &lt;em&gt;runtime&lt;/em&gt;
inefficiency and requires modifying the method to account for the fact that &lt;span class="math"&gt;\(m\)&lt;/span&gt;
is not fixed ahead of time. (3) Even in our simple setting, MC never reaches
&lt;em&gt;zero&lt;/em&gt; error; it only converges in an &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-&lt;span class="math"&gt;\(\delta\)&lt;/span&gt; sense.&lt;/p&gt;
&lt;!--
Remarks

 - We saw a similar problem where we kept sampling the same individuals over and
   over again in my
   [sqrt-biased sampling post](http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/).
--&gt;

&lt;p&gt;&lt;strong&gt;Sampling without replacement:&lt;/strong&gt; We can get around the problem of resampling
the same elements multiple times by sampling &lt;span class="math"&gt;\(m\)&lt;/span&gt; distinct elements. This is
called a sampling &lt;em&gt;without replacement&lt;/em&gt; (SWOR) scheme. Note that there is no
unique sampling without replacement scheme; although, there does seem to be a
&lt;em&gt;de facto&lt;/em&gt; method (more on that later). There are lots of ways to do sampling
without replacement, e.g., any point process over the universe will do as long
as we can control the size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An alternative formulation:&lt;/strong&gt; We can also formulate our estimation problem as
seeking a sparse, unbiased approximation to a vector &lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}_{&amp;gt;0}^n\)&lt;/span&gt;. We want
our approximation, &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; to satisfy &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}] =
\boldsymbol{x}\)&lt;/span&gt; and while &lt;span class="math"&gt;\(|| \boldsymbol{s} ||_0 \le m\)&lt;/span&gt;. This will suffice for
estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (above) when &lt;span class="math"&gt;\(\boldsymbol{x}=\boldsymbol{p}\)&lt;/span&gt;, the vector of
probabillties, because &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}^\top\! \boldsymbol{f}] =
\mathbb{E}[\boldsymbol{s}]^\top\! \boldsymbol{f} = \boldsymbol{p}^\top\!
\boldsymbol{f} = \mu\)&lt;/span&gt; where &lt;span class="math"&gt;\(\boldsymbol{f}\)&lt;/span&gt; is a vector of all &lt;span class="math"&gt;\(n\)&lt;/span&gt; values of
the function &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Obviously, we don't need to evaluate &lt;span class="math"&gt;\(f\)&lt;/span&gt; in places where
&lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; is zero so it works for our budgeted estimation task. Of
course, unbiased estimation of all probabillties is not &lt;em&gt;necessary&lt;/em&gt; for unbiased
estimation of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; alone. However, this characterization is a good model for
when we have zero knowledge of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Additionally, this formulation might be of
independent interest, since a sparse, unbiased representation of a vector might
be useful in some applications (e.g., replacing a dense vector with a sparse
vector can lead to more efficient computations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priority sampling&lt;/strong&gt;: Priority sampling (Duffield et al., 2005;
&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., 2007&lt;/a&gt;)
is a remarkably simple algorithm, which is essentially optimal for our task, if
we assume no prior knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Here is pseudocode for priority sampling
(PS), based on the &lt;em&gt;alternative formulation&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;\textbf{procedure } \textrm{PrioritySample} \\
&amp;amp;\textbf{inputs: } \text{vector } \boldsymbol{x} \in \mathbb{R}_{&amp;gt;0}^n, \text{budget } m \in \{1, \ldots, n\}\\
&amp;amp;\textbf{output: } \text{sparse and unbiased representation of $\boldsymbol{x}$} \\
&amp;amp;\quad u_i, \ldots, u_n \overset{\tiny\text{i.i.d.}} \sim \textrm{Uniform}(0,1] \\
&amp;amp;\quad  k_i \leftarrow u_i/x_i \text{ for each $i$} \quad\color{grey}{\text{# random sort key }} \\
&amp;amp;\quad S \leftarrow \{ \text{$m$-smallest elements according to $k_i$} \} \\
&amp;amp;\quad \tau \leftarrow (m+1)^{\text{th}}\text{ smallest }k_i \\
&amp;amp;\quad  s_i \gets \begin{cases}
  \max\left( x_i, 1/\tau \right)  &amp;amp; \text{ if } i \in S \\
  0                               &amp;amp; \text{ otherwise}
\end{cases} \\
&amp;amp;\quad \textbf{return }\boldsymbol{s}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textrm{PrioritySample}\)&lt;/span&gt; can be applied to obtain a sparse and unbiased
representation of any vector in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;. We make use of such a
representation for our original problem of budgeted mean estimation (&lt;span class="math"&gt;\(\mu\)&lt;/span&gt;) as
follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \boldsymbol{s} \gets \textrm{PrioritySample}(\boldsymbol{p}, m) \\
&amp;amp; \widehat{\mu}_{\text{PS}} = \sum_{i \in S} s_i \!\cdot\! f(i)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Explanation: The definition of &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; might look a little mysterious. In the &lt;span class="math"&gt;\((i
\in S)\)&lt;/span&gt; case, it comes from &lt;span class="math"&gt;\(s_i = \frac{p_i}{p(i \in S | \tau)} =
\frac{p_i}{\min(1, x_i \cdot \tau)} = \max(x_i,\ 1/\tau)\)&lt;/span&gt;. The factor &lt;span class="math"&gt;\(p(i \in S
| \tau)\)&lt;/span&gt; is an importance-weighting correction that comes from the
&lt;a href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator"&gt;Horvitz-Thompson estimator&lt;/a&gt;
(modified slightly from its usual presentation to estimate means),
&lt;span class="math"&gt;\(\sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S]\)&lt;/span&gt;, where
&lt;span class="math"&gt;\(S\)&lt;/span&gt; is sampled according to some process with inclusion probabilities &lt;span class="math"&gt;\(q_i = p(i
\in S)\)&lt;/span&gt;. In the case of priority sampling, we have an auxiliary variable for
&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; that makes computing &lt;span class="math"&gt;\(q_i\)&lt;/span&gt; easy. Thus, for priority sampling, we can use
&lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;. This auxillary variable adds a tiny bit extra noise
in our estimator, which is tantamount to one extra sample.&lt;/p&gt;
&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#ps-unbiased');"&gt;Show proof of
unbiasedness&lt;/button&gt; &lt;div id="ps-unbiased" class="derivation"
style="display:none;"&gt; &lt;strong&gt;Proof of unbiasedness&lt;/strong&gt;. The following proof is a
little different from that in the priority sampling papers. I think it's more
straightforward. More importantly, it shows how we can extend the method to
sample from slightly different without-replacement distributions (as long as we
can compute &lt;span class="math"&gt;\(q_i(\tau) = \mathrm{Pr}(i \in S \mid \tau) = \mathrm{Pr}(k_i \le \tau)\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
\mathbb{E}\left[ \widehat{\mu}_{\text{PS}} \right]
&amp;amp;=&amp;amp; \mathbb{E}_{\tau, k_1, \ldots k_n}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i(\tau)} \cdot f(i) \cdot \boldsymbol{1}[ k_i \le \tau] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i(\tau)} \cdot f(i) \cdot \mathbb{E}_{k_i | \tau}\!\Big[ \boldsymbol{1}[ k_i \le \tau  ] \Big] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[
   \sum_{i=1}^n \frac{p_i}{q_i(\tau)} \cdot f(i) \cdot
   \mathrm{Pr}( k_i \le \tau )
   \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[
   \sum_{i=1}^n \frac{p_i}{q_i(\tau)} \cdot f(i) \cdot
   q_i(\tau)
   \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[
   \sum_{i=1}^n p_i \cdot f(i)
   \right] \\
&amp;amp;=&amp;amp; \sum_{i=1}^n p_i \cdot f(i) \\
&amp;amp;=&amp;amp; \mu
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling satisfies our task criteria: it is both unbiased and sparse
   (i.e., under the evaluation budget).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling can be straighforwardly generalized to support streaming
   &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, since the keys and threshold can be computed as we run, which means it
   can be stopped at any time, in principle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling was designed for estimating subset sums, i.e., estimating
   &lt;span class="math"&gt;\(\sum_{i \in I} x_i\)&lt;/span&gt; for some &lt;span class="math"&gt;\(I \subseteq \{1,\ldots,n\}\)&lt;/span&gt;. In this setting,
   the set of sampled items &lt;span class="math"&gt;\(S\)&lt;/span&gt; is chosen to be "representative" of the
   population, albeit much smaller. In the subset sum setting, priority sampling
   has been shown to have near-optimal variance
   &lt;a href="https://www.cs.rutgers.edu/~szegedy/PUBLICATIONS/full1.pdf"&gt;(Szegedy, 2005)&lt;/a&gt;.
   Specifically, priority sampling with &lt;span class="math"&gt;\(m\)&lt;/span&gt; samples is no worse than the best
   possible &lt;span class="math"&gt;\((m-1)\)&lt;/span&gt;-sparse estimator in terms of variance. Of course,
   if we have some knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we may be able to beat
   PS. &lt;!-- We can relate subset sums to estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; by interpreting
   &lt;span class="math"&gt;\(\boldsymbol{x} = \alpha\!\cdot\! \boldsymbol{p}\)&lt;/span&gt; for some &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, scaling
   &lt;span class="math"&gt;\(f\)&lt;/span&gt; appropriately by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, and encoding the subset via indicators in
   &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s dimensions. --&gt;
   &lt;!-- (e.g.,. via
   &lt;a href="http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/"&gt;importance sampling&lt;/a&gt;
   or by modifying PS to sample proportional to &lt;span class="math"&gt;\(x_i = p_i \!\cdot\! |f_i|\)&lt;/span&gt; (as
   well as other straightforward modifications), but presumably with a surrogate
   for &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; because we don't want to evaluate it). --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Components of the estimate &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; are uncorrelated, i.e.,
   &lt;span class="math"&gt;\(\textrm{Cov}[s_i, s_j] = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(i \ne j\)&lt;/span&gt; and &lt;span class="math"&gt;\(m \ge 2\)&lt;/span&gt;. This is surprising
   since &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; are related via the threshold &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we instead sample &lt;span class="math"&gt;\(u_1, \ldots, u_n \overset{\text{i.i.d.}}{\sim}
   -\textrm{Exponential}(1)\)&lt;/span&gt;, then &lt;span class="math"&gt;\(S\)&lt;/span&gt; will be sampled according to the &lt;em&gt;de facto&lt;/em&gt;
   sampling without replacement scheme (e.g., &lt;code&gt;numpy.random.sample(..., replace=False)&lt;/code&gt;),
   known as probability proportional to size without replacement (PPSWOR).
   To we can then adjust our estimator
   &lt;div class="math"&gt;$$
   \widehat{\mu}_{\text{PPSWOR}} = \sum_{i \in S} \frac{p_i}{q_i} f(i)
   $$&lt;/div&gt;
   where &lt;span class="math"&gt;\(q_i = p(i \in S|\tau) = p(k_i &amp;gt; \tau) = 1-\exp(-x_i \!\cdot\!
   \tau)\)&lt;/span&gt;. This estimator performs about as well as priority sampling. It
   inherits my proof of unbiasedness (above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; is an auxiliary variable that is introduced to break complex
   dependencies between keys. Computing &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;'s distribution is complicated
   because it is an order statistic of non-identically distributed random
   variates; this means we can't rely on symmetry to make summing over
   permutations efficient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
 - The one downside of this method is that sampling seems to require looking at
   all $n$ items.
--&gt;

&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;You can get the Jupyter notebook for replicating this experiment
&lt;a href="https://github.com/timvieira/blog/blob/master/content/notebook/Priority%20Sampling.ipynb"&gt;here&lt;/a&gt;.
So download the notebook and play with it!&lt;/p&gt;
&lt;p&gt;The improvement of priority sampling (PS) over Monte Carlo (MC) is pretty
nice. I've also included PPSWOR, which seems pretty indistinguishable from PS so
I won't really bother to discuss it. Check out the results!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Priority sampling vs. Monte Carlo" src="http://timvieira.github.io/blog/images/ps-mc.png"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The shaded region indicates the 10% and 90% percentiles over 20,000
replications, which gives a sense of the variability of each estimator. The
x-axis is the sampling budget, &lt;span class="math"&gt;\(m \le n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The plot shows a small example with &lt;span class="math"&gt;\(n=50\)&lt;/span&gt;. We see that PS's variability
actually goes to zero, unlike Monte Carlo, which is still pretty inaccurate even
at &lt;span class="math"&gt;\(m=n\)&lt;/span&gt;. (Note that MC's x-axis measures raw evaluations, not distinct ones.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt; If you liked this post, you might like my other posts
tagged with &lt;a href="http://timvieira.github.io/blog/tag/sampling.html"&gt;sampling&lt;/a&gt; and
&lt;a href="http://timvieira.github.io/blog/tag/reservoir-sampling.html"&gt;reservoir sampling&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Edith Cohen, "The Magic of Random Sampling"
   (&lt;a href="http://www.cohenwang.com/edith/Talks/MagicSampling201611.pdf"&gt;slides&lt;/a&gt;,
   &lt;a href="https://www.youtube.com/watch?v=jp83HyDs8fs"&gt;talk&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., (2007)&lt;/a&gt;
   has plenty good stuff that I didn't cover.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alex Smola's &lt;a href="http://blog.smola.org/post/1078486350/priority-sampling"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suresh Venkatasubramanian's
   &lt;a href="http://blog.geomblog.org/2005/10/priority-sampling.html"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="sampling"></category><category term="statistics"></category><category term="reservoir-sampling"></category><category term="sampling-without-replacement"></category></entry><entry><title>Heaps for incremental computation</title><link href="https://timvieira.github.io/blog/post/2016/11/21/heaps-for-incremental-computation/" rel="alternate"></link><published>2016-11-21T00:00:00-05:00</published><updated>2016-11-21T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-11-21:/blog/post/2016/11/21/heaps-for-incremental-computation/</id><content type="html">&lt;p&gt;In this post, I'll describe a neat trick for maintaining a summary quantity
(e.g., sum, product, max, log-sum-exp, concatenation, cross-product) under
changes to its inputs. The trick and it's implementation are inspired by the
well-known max-heap datastructure. I'll also describe a really elegant
application to fast sampling under an evolving categorical distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we'd like to efficiently compute a summary quantity under
changes to its &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional input vector &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;. The particular
form of the quantity we're going to compute is &lt;span class="math"&gt;\(z = \bigoplus_{i=1}^n w_i\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; is some associative binary operator with identity element
&lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#operator-mathy');"&gt;more formally...&lt;/button&gt;
&lt;div id="operator-mathy" class="derivation" style="display:none"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w} \in \boldsymbol{K}^n\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\oplus: \boldsymbol{K} \times \boldsymbol{K} \mapsto \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associative: &lt;span class="math"&gt;\((a \oplus b) \oplus c = a \oplus (b \oplus c)\)&lt;/span&gt; for all &lt;span class="math"&gt;\(a,b,c
  \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identity element: &lt;span class="math"&gt;\(\boldsymbol{0} \in \boldsymbol{K}\)&lt;/span&gt; such that &lt;span class="math"&gt;\(k \oplus
  \boldsymbol{0} = \boldsymbol{0} \oplus k = k\)&lt;/span&gt;, for all &lt;span class="math"&gt;\(k \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;The trick&lt;/strong&gt;: Essentially, the trick boils down to &lt;em&gt;parenthesis placement&lt;/em&gt; in
the expression which computes &lt;span class="math"&gt;\(z\)&lt;/span&gt;. A freedom we assumed via the associative
property.&lt;/p&gt;
&lt;p&gt;I'll demonstrate by example with &lt;span class="math"&gt;\(n=8\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Linear structure: We generally compute something like &lt;span class="math"&gt;\(z\)&lt;/span&gt; with a simple
loop. This looks like a right-branching binary tree when we think about the
order of operations,&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((((((w_1 \oplus w_2) \oplus w_3) \oplus w_4) \oplus w_5) \oplus w_6) \oplus w_7) \oplus w_8).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; Heap structure: Here the parentheses form a balanced tree, which looks
much more like a recursive implementation that computes the left and right
halves and &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;s the results (divide-and-conquer style),&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((w_1 \oplus w_2) \oplus (w_3 \oplus w_4)) \oplus ((w_5 \oplus w_6) \oplus (w_7 \oplus w_8))).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;
The benefit of the heap structure is that there are &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;
intermediate quantities that depend on any input, whereas the linear structure
has &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt;. The intermediate quantities correspond to the values of each of the
parenthesized expressions.&lt;/p&gt;
&lt;p&gt;Since fewer intermediate quantities depend on a given input, fewer intermediates
need to be adjusted upon a change to the input. Therefore, we get faster
algorithms for &lt;em&gt;maintaining&lt;/em&gt; the output quantity &lt;span class="math"&gt;\(z\)&lt;/span&gt; as the inputs change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Heap datastructure&lt;/strong&gt; (aka
&lt;a href="https://en.wikipedia.org/wiki/Fenwick_tree"&gt;binary index tree or Fenwick tree&lt;/a&gt;):
We're going to store the values of the intermediates quantities and inputs in a
heap datastructure, which is a &lt;em&gt;complete&lt;/em&gt; binary tree. In our case, the tree has
depth &lt;span class="math"&gt;\(1 + \lceil \log_2 n \rceil\)&lt;/span&gt;, with the values of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at it's
leaves (aligned left) and padding with &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt; for remaining
leaves. Thus, the array's length is &lt;span class="math"&gt;\(&amp;lt; 4 n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This structure makes our implementation really nice and efficient because we
don't need pointers to find the parent or children of a node (i.e., no need to
wrap elements into a "node" class like in a general tree data structure). So, we
can pack everything into an array, which means our implementation has great
memory/cache locality and low storage overhead.&lt;/p&gt;
&lt;p&gt;Traversing the tree is pretty simple: Let &lt;span class="math"&gt;\(d\)&lt;/span&gt; be the number of internal nodes,
nodes &lt;span class="math"&gt;\(1 \le i \le d\)&lt;/span&gt; are internal. For node &lt;span class="math"&gt;\(i\)&lt;/span&gt;, left child &lt;span class="math"&gt;\(\rightarrow {2
\cdot i},\)&lt;/span&gt; right child &lt;span class="math"&gt;\(\rightarrow {2 \cdot i + 1},\)&lt;/span&gt; parent &lt;span class="math"&gt;\(\rightarrow
\lfloor i / 2 \rfloor.\)&lt;/span&gt; (Note that these operations assume the array's indices
start at &lt;span class="math"&gt;\(1\)&lt;/span&gt;. We generally fake this by adding a dummy node at position &lt;span class="math"&gt;\(0\)&lt;/span&gt;,
which makes implementation simpler.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initializing the heap&lt;/strong&gt;: Here's code that initializes the heap structure we
  just described.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sumheap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Create sumheap from weights `w` in O(n) time.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  &lt;span class="c1"&gt;# number of intermediates&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                &lt;span class="c1"&gt;# intermediates + leaves&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;                     &lt;span class="c1"&gt;# store `w` at leaves.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Updating &lt;span class="math"&gt;\(w_k\)&lt;/span&gt;&lt;/strong&gt; boils down to fixing intermediate sums that (transitively)
  depend on &lt;span class="math"&gt;\(w_k.\)&lt;/span&gt; I won't go into all of the details here, instead I'll give
  code (below). I'd like to quickly point out that the term "parents" is not
  great for our purposes because they are actually the &lt;em&gt;dependents&lt;/em&gt;: when an
  input changes the value the parents, grand parents, great grand parents, etc,
  become stale and need to be recomputed bottom up (from the leaves). The code
  below implements the update method for changing the value of &lt;span class="math"&gt;\(w_k\)&lt;/span&gt; and runs in
  &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Update w[k] = v` in time O(log n).&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# fix parents in the tree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;//=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical stability&lt;/strong&gt;: If the operations are noisy (e.g., floating point
   operator), then the heap version may be better behaved. For example, if
   operations have an independent, additive noise rate &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; then noise
   of &lt;span class="math"&gt;\(z_{\text{heap}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot \log n)\)&lt;/span&gt;, whereas
   &lt;span class="math"&gt;\(z_{\text{linear}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot n)\)&lt;/span&gt;. (Without further
   assumptions about the underlying operator, I don't believe you can do better
   than that.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relationship to max-heap&lt;/strong&gt;: In the case of a max or min heap, we can avoid
   allocating extra space for intermediate quantities because all intermediates
   values are equal to exactly one element of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change propagation&lt;/strong&gt;: The general idea of &lt;em&gt;adjusting&lt;/em&gt; cached intermediate
   quantities is a neat idea. In fact, we encounter it each time we type
   &lt;code&gt;make&lt;/code&gt; at the command line! The general technique goes by many
   names&amp;mdash;including change propagation, incremental maintenance, and
   functional reactive programming&amp;mdash;and applies to basically &lt;em&gt;any&lt;/em&gt;
   side-effect-free computation. However, it's most effective when the
   dependency structure of the computation is sparse and requires little
   overhead to find and refresh stale values. In our example of computing &lt;span class="math"&gt;\(z\)&lt;/span&gt;,
   these considerations manifest themselves as the heap vs linear structures and
   our fast array implementation instead of a generic tree datastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Generalizations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No zero? No problem. We don't &lt;em&gt;actually&lt;/em&gt; require a zero element. So, it's
   fair to augment &lt;span class="math"&gt;\(\boldsymbol{K} \cup \{ \textsf{null} \}\)&lt;/span&gt; where
   &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; is distinguished value (i.e., &lt;span class="math"&gt;\(\textsf{null} \notin
   \boldsymbol{K}\)&lt;/span&gt;) that &lt;em&gt;acts&lt;/em&gt; just like a zero after we overload &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; to
   satisfy the definition of a zero (e.g., by adding an if-statement).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to an arbitrary maps instead of fixed vectors is possible with
   a "locator" map, which a bijective map from elements to indices in a dense
   array.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for growing and shrinking: We support &lt;strong&gt;growing&lt;/strong&gt; by maintaining an
   underlying array that is always slightly larger than we need&amp;mdash;which
   we're &lt;em&gt;already&lt;/em&gt; doing in the heap datastructure. Doubling the size of the
   underlying array (i.e., rounding up to the next power of two) has the added
   benefit of allowing us to grow &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at no asymptotic cost!  This
   is because the resize operation, which requires an &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt; time to
   allocate a new array and copying old values, happens so infrequently that
   they can be completely amortized. We get of effect of &lt;strong&gt;shrinking&lt;/strong&gt; by
   replacing the old value with &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; (or &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;). We can
   shrink the underlying array when the fraction of nonzeros dips below
   &lt;span class="math"&gt;\(25\%\)&lt;/span&gt;. This prevents "thrashing" between shrinking and growing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Application&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sampling from an evolving distribution&lt;/strong&gt;: Suppose that &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;
corresponds to a categorical distributions over &lt;span class="math"&gt;\(\{1, \ldots, n\}\)&lt;/span&gt; and that we'd
like to sample elements from in proportion to this (unnormalized) distribution.&lt;/p&gt;
&lt;p&gt;Other methods like the &lt;a href="http://www.keithschwarz.com/darts-dice-coins/"&gt;alias&lt;/a&gt; or
inverse CDF methods are efficient after a somewhat costly initialization
step. But! they are not as efficient as the heap sampler when the distribution
is being updated. (I'm not sure about whether variants of alias that support
updates exist.)&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Init&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;alias&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;td&gt;O(n)?&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;i-CDF&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heap&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Use cases include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Gibbs_sampling"&gt;Gibbs sampling&lt;/a&gt;, where
  distributions are constantly modified and sampled from (changes may not be
  sparse so YMMV). The heap sampler is used in
  &lt;a href="https://arxiv.org/abs/1412.4986"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/"&gt;EXP3&lt;/a&gt;
  (&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;mutli-armed bandit algorithm&lt;/a&gt;)
  is an excellent example of an algorithm that samples and modifies a single
  weight in the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Stochastic priority queues&lt;/em&gt; where we sample proportional to priority and the
  weights on items in the queue may change, elements are possibly removed after
  they are sampled (i.e., sampling without replacement), and elements are added.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, I won't spell out all of the details of these algorithms. Instead, I'll
just give the code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inverse CDF sampling&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Ordinary sampling method, O(n) init, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;            &lt;span class="c1"&gt;# build cdf, O(n)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;     &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# binary search, O(log n)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Heap sampling&lt;/strong&gt; is essentially the same, except the cdf is stored as heap,
which is perfect for binary search!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hsample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Sample from sumheap, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="c1"&gt;# number of internal nodes.&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="c1"&gt;# Use binary search to find the index of the largest CDF (represented as a&lt;/span&gt;
    &lt;span class="c1"&gt;# heap) value that is less than a random probe.&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Determine if the value is in the left or right subtree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;         &lt;span class="c1"&gt;# Point at left child&lt;/span&gt;
        &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="c1"&gt;# Probability mass under left subtree.&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# Value is in right subtree.&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;  &lt;span class="c1"&gt;# Subtract mass from left subtree&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;     &lt;span class="c1"&gt;# Point at right child&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Complete code and test cases for heap sampling are available in this
&lt;a href="https://gist.github.com/timvieira/da31b56436045a3122f5adf5aafec515"&gt;gist&lt;/a&gt;.  A fast Cython &lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/maths/sumheap.pyx"&gt;implementation&lt;/a&gt; is available in my Python &lt;a href="https://github.com/timvieira/arsenal/"&gt;arsenal&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="sampling"></category><category term="datastructures"></category><category term="incremental-computation"></category></entry><entry><title>Fast sigmoid sampling</title><link href="https://timvieira.github.io/blog/post/2016/07/04/fast-sigmoid-sampling/" rel="alternate"></link><published>2016-07-04T00:00:00-04:00</published><updated>2016-07-04T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-07-04:/blog/post/2016/07/04/fast-sigmoid-sampling/</id><content type="html">&lt;p&gt;{% notebook notebook/Fast-sigmoid-sampling.ipynb cells[1:] %}&lt;/p&gt;</content><category term="misc"></category><category term="sampling"></category><category term="Gumbel"></category></entry><entry><title>Sqrt-biased sampling</title><link href="https://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/" rel="alternate"></link><published>2016-06-28T00:00:00-04:00</published><updated>2016-06-28T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-06-28:/blog/post/2016/06/28/sqrt-biased-sampling/</id><content type="html">&lt;p&gt;The following post is about instance of "sampling in proportion to &lt;span class="math"&gt;\(p\)&lt;/span&gt; is not
optimal, but you probably think it is." It's surprising how few people seem to
know this trick. Myself included! It was brought to my attention recently by
&lt;a href="http://lowrank.net/nikos/"&gt;Nikos Karampatziakis&lt;/a&gt;. (Thanks, Nikos!)&lt;/p&gt;
&lt;p&gt;The paper credited for this trick is
&lt;a href="http://www.pnas.org/content/106/6/1716.full.pdf"&gt;Press (2008)&lt;/a&gt;. I'm borrowing
heavily from that paper as well as an email exchange from Nikos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Suppose you're an aspiring chef with a severe head injury affecting
  your long- and short- term memory trying to find a special recipe from a
  cookbook that you made one time but just can't remember exactly which recipe
  it was. So, based on the ingredients of each recipe, you come up with a prior
  probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; that recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the one you're looking for. In total, the
  cookbook has &lt;span class="math"&gt;\(n\)&lt;/span&gt; recipes and &lt;span class="math"&gt;\(\sum_{i=1}^n p_i = 1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A good strategy would be to sort recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; and cook the most promising
ones first. Unfortunately, you're not a great chef so there is some probability
that you'll mess-up the recipe. So, it's a good idea to try recipes multiple
times. Also, you have no short term memory...&lt;/p&gt;
&lt;p&gt;This suggests a &lt;em&gt;sampling with replacement&lt;/em&gt; strategy, where we sample a recipe
from the cookbook to try &lt;em&gt;independently&lt;/em&gt; of whether we've tried it before
(called a &lt;em&gt;memoryless&lt;/em&gt; strategy). Let's give this strategy the name
&lt;span class="math"&gt;\(\boldsymbol{q}.\)&lt;/span&gt; Note that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is a probability distribution over
the recipes in the cookbook, just like &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many recipes until we find the special one?&lt;/strong&gt; To start, suppose the
special recipe is &lt;span class="math"&gt;\(j.\)&lt;/span&gt; Then, the expected number of recipes we have to make
until we find &lt;span class="math"&gt;\(j\)&lt;/span&gt; under the strategy &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j} = 1/q_{j}.
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-series')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-series" style="display:none;" class="derivation"&gt;
&lt;strong&gt;Derivation&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We start with
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j},
$$&lt;/div&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(a = (1-q_j)\)&lt;/span&gt;, to clean up notation.
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty t \cdot a^{t-1}
$$&lt;/div&gt;
&lt;p&gt;Use the identity &lt;span class="math"&gt;\(\nabla_a [ a^t ] = t \cdot a^{t-1}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty \nabla_a[ a^{t} ].
$$&lt;/div&gt;
&lt;p&gt;Fish the gradient out of the sum and tweak summation index,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ \sum_{t=1}^\infty a^{t} \right]
= q_{j} \nabla_a\left[ -1 + \sum_{t=0}^\infty a^{t}\right]
$$&lt;/div&gt;
&lt;p&gt;Plugin in the solution to the geometric series,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ -1 + \frac{1}{1-a} \right].
$$&lt;/div&gt;
&lt;p&gt;Take derivative, expand &lt;span class="math"&gt;\(a\)&lt;/span&gt; and simplify,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \frac{1}{(1-a)^2}
= \frac{1}{q_j}
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The equation says that expected time it takes to sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; for &lt;em&gt;the first time&lt;/em&gt;
is the probability we didn't sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; for &lt;span class="math"&gt;\((t-1)\)&lt;/span&gt; steps times the probability we
sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t.\)&lt;/span&gt;  We multiply this probability by the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; to get the
&lt;em&gt;expected&lt;/em&gt; time.&lt;/p&gt;
&lt;p&gt;Note that this equation assumes that we know &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the special recipe &lt;em&gt;with
certainty&lt;/em&gt; when we sample it. We'll revisit this assumption later when we
consider potential errors in executing the recipe.&lt;/p&gt;
&lt;p&gt;Since we don't known which &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the right one, we take an expectation over it
according to the prior distribution, which yields the following equation,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{q_i}.
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The first surprising thing&lt;/strong&gt;: Uniform is just as good as &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt;,
  yikes! &lt;span class="math"&gt;\(f(\boldsymbol{p}) = \sum_{i=1}^n \frac{p_i}{p_i} = n\)&lt;/span&gt; and
  &lt;span class="math"&gt;\(f(\text{uniform}(n)) = \sum_{i=1}^n \frac{p_i }{ 1/n } = n.\)&lt;/span&gt; (Assume, without
  loss of generality, that &lt;span class="math"&gt;\(p_i &amp;gt; 0\)&lt;/span&gt; since we can just drop these elements from
  &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the &lt;em&gt;optimal&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;?&lt;/strong&gt; We can address this question by
solving the following optimization (which will have a nice closed form
solution),&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \boldsymbol{q}^* = \underset{\boldsymbol{q}}{\operatorname{argmin}} \sum_{i=1}^n \frac{p_i}{q_i} \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \text{ s.t. } \sum_{i=1}^n q_i = 1 \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \ \ \ \ \, q_1 \ldots q_n \ge 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;The optimization problem says minimize the expected time to find the special
recipe. The constraints enforce that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is a valid probability
distribution.&lt;/p&gt;
&lt;p&gt;The optimal strategy, which we get via Lagrange multipliers, turns out to be,
&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;p&gt;&lt;button onclick="toggle('#Lagrange')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="Lagrange" style="display:none;" class="derivation"&gt;
To solve this constrained optimization problem, we form the
Lagrangian,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\boldsymbol{q}, \lambda) = \sum_{i=1}^n \frac{p_i}{q_i} - \lambda\cdot \left(1 - \sum_{i=1}^n q_i\right),$$&lt;/div&gt;
&lt;p&gt;and solve for &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; and multiplier &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; such that partial
derivatives are all equal to zero. This gives us the following system of
nonlinear equations,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \lambda - \frac{p_i}{q_i^2} = 0 \ \ \ \text{for } 1 \le i \le n \\
&amp;amp;&amp;amp; \lambda \cdot \left(1 - \sum_{i=1}^n q_i \right) = 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We see that &lt;span class="math"&gt;\(q_i = \pm \sqrt{\frac{p_i}{\lambda}}\)&lt;/span&gt; works for the first set of
equations, but since we need &lt;span class="math"&gt;\(q_i \ge 0\)&lt;/span&gt;, we take the positive one. Solving for
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and plugging it in, we get a normalized distribution,&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How much better is &lt;span class="math"&gt;\(q^*\)&lt;/span&gt;?&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(q^*) = \sum_i \frac{p_i}{q^*_i}
= \sum_i \frac{p_i}{ \frac{\sqrt{p_i} }{ \sum_j \sqrt{p_j}} }
= \left( \sum_i \frac{p_i}{ \sqrt{p_i} } \right) \left( \sum_j \sqrt{p_j} \right)
= \left( \sum_i \sqrt{p_i} \right)^2
$$&lt;/div&gt;
&lt;p&gt;which sometimes equals &lt;span class="math"&gt;\(n\)&lt;/span&gt;, e.g., when &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; is uniform, but is never
bigger than &lt;span class="math"&gt;\(n.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the intuition?&lt;/strong&gt; The reason why the &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-scheme is preferred is
because we save on &lt;em&gt;additional&lt;/em&gt; cooking experiments. For example, if a recipe
has &lt;span class="math"&gt;\(k\)&lt;/span&gt; times higher prior probability than the average recipe, then we will try
that recipe &lt;span class="math"&gt;\(\sqrt{k}\)&lt;/span&gt; times more often; compared to &lt;span class="math"&gt;\(k\)&lt;/span&gt;, which we'd get under
&lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt; Additional cooking experiments are not so advantageous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Allowing for noise in the cooking process&lt;/strong&gt;: Suppose that for each recipe we
  had a prior belief about how hard that recipe is for us to cook. Denote that
  belief &lt;span class="math"&gt;\(s_i\)&lt;/span&gt;, these beliefs are between zero (never get it right) and one
  (perfect every time) and do not necessarily sum to one over the cookbook.&lt;/p&gt;
&lt;p&gt;Following a similar derivation to before, the time to cook the special recipe
&lt;span class="math"&gt;\(j\)&lt;/span&gt; and cook it correctly is,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - \color{red}{s_j} q_j)^{t-1} q_{j} \color{red}{s_j} = \frac{1}{s_j \cdot q_j}
$$&lt;/div&gt;
&lt;p&gt;
That gives rise to a modified objective,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{\color{red}{s_i} \cdot q_i}
$$&lt;/div&gt;
&lt;p&gt;This is exactly the same as the previous objective, except we've replaced &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;
with &lt;span class="math"&gt;\(p_i/s_i.\)&lt;/span&gt; Thus, we can reuse our previous derivation to get the optimal
strategy, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i / s_i}.\)&lt;/span&gt; If noise is constant, then we
recover the original solution, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension to finding multiple tasty recipes&lt;/strong&gt;: Suppose we're trying to find
  several tasty recipes, not just a single special one. Now, &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is our prior
  belief that we'll like the recipe at all. How do we minimize the time until we
  find a tasty one? It turns out the same trick works without modification
  because all derivations apply to each recipe independently. The same trick
  works if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; does not sums to one over &lt;span class="math"&gt;\(n.\)&lt;/span&gt; For example, if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the
  independent probability that you'll like recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; at all, not the
  probability that it's the special one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond memoryless policies&lt;/strong&gt;: Clearly, our choice of a memoryless policy can
  be beat by a policy family that balances exploration (trying new recipes) and
  exploitation (trying our best guess).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Overall, the problem we've posed is similar to a
    &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;multi-armed bandit&lt;/a&gt;. In
    our case, the arms are the recipes, pulling the arm is trying the recipe and
    the reward is whether or not we liked the recipe (possibly noisy). The key
    difference between our setup and multi-armed bandits is that we trust our
    prior distribution &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; and noise model &lt;span class="math"&gt;\(\boldsymbol{s}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the amount of noise &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; is known and we trust the prior &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; then
    there is an optimal deterministic (without-replacement) strategy that we can
    get by sorting the recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; accounting for the error rates
    &lt;span class="math"&gt;\(s_i.\)&lt;/span&gt; This approach is described in the original paper.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A more realistic application&lt;/strong&gt;: In certain language modeling applications, we
  avoid computing normalization constants (which require summing over a massive
  vocabulary) by using importance sampling, negative sampling or noise
  contrastive estimation techniques (e.g.,
  &lt;a href="https://arxiv.org/pdf/1511.06909.pdf"&gt;Ji+,16&lt;/a&gt;;
  &lt;a href="http://www.aclweb.org/anthology/Q15-1016"&gt;Levy+,15&lt;/a&gt;). These techniques depend
  on a proposal distribution, which folks often take to be the unigram
  distribution. Unfortunately, this gives too many samples of stop words (e.g.,
  "the", "an", "a"), so practitioners "anneal" the unigram distribution (to
  increase the entropy), that is sample from &lt;span class="math"&gt;\(q_i \propto
  p_{\text{unigram},i}^\alpha.\)&lt;/span&gt; Typically, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is set by grid search and
  (no surprise) &lt;span class="math"&gt;\(\alpha \approx 1/2\)&lt;/span&gt; tends to work best! The &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-sampling
  trick is possibly a reverse-engineered justification in favor of annealing as
  "the right thing to do" (e.g., why not do additive smoothing?) and it even
  tells us how to set the annealing parameter &lt;span class="math"&gt;\(\alpha.\)&lt;/span&gt; The key assumption is
  that we want to sample the actual word at a given position as often as
  possible while still being diverse thanks to the coverage of unigram
  prior. (Furthermore, memoryless sampling leads to simpler algorithms.)&lt;/p&gt;
&lt;!--
Actually, many word2vec papers use $\alpha=3/4$, which was suggested in
[Levy+,15](http://www.aclweb.org/anthology/Q15-1016), including the default
value in
[gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L462). So,
[Ryan Cotterell](https://ryancotterell.github.io/) ran a quick experiment with
gensim, which confirmed the suspicion that $1/2$ may be better than $3/4.$

    Word similarity accuracy (avg of 10 runs)
    | alpha | accuracy |
    +==================+
    |  0.00 |    0.354 |
    |  0.25 |    0.403 |
    |  0.50 |    0.414 |
    |  0.75 |    0.395 |
    |  1.00 |    0.345 |
--&gt;

&lt;p&gt;&lt;strong&gt;Final remarks&lt;/strong&gt;: I have uploaded a &lt;a href="https://github.com/timvieira/blog/blob/master/content/notebook/Sqrt-biased-sampling.ipynb"&gt;Jupyter notebook&lt;/a&gt; with test cases that illustrate the ideas in this article.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="sampling"></category><category term="decision-making"></category></entry><entry><title>The optimal proposal distribution is not p</title><link href="https://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/" rel="alternate"></link><published>2016-05-28T00:00:00-04:00</published><updated>2016-05-28T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-05-28:/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/</id><content type="html">&lt;p&gt;The following is a quick rant about
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;
(see that post for notation).&lt;/p&gt;
&lt;p&gt;I've heard the following &lt;strong&gt;incorrect&lt;/strong&gt; statement one too many times,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We chose &lt;span class="math"&gt;\(q \approx p\)&lt;/span&gt; because &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is the "optimal" proposal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While it is certainly a good idea to pick &lt;span class="math"&gt;\(q\)&lt;/span&gt; to be as similar as possible to
&lt;span class="math"&gt;\(p\)&lt;/span&gt;, it is by no means &lt;em&gt;optimal&lt;/em&gt; because it is oblivious to &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;With importance sampling, it is possible to achieve a variance reduction over
Monte Carlo estimation. The optimal proposal distribution, assuming &lt;span class="math"&gt;\(f(x) \ge 0\)&lt;/span&gt;
for all &lt;span class="math"&gt;\(x\)&lt;/span&gt;, is &lt;span class="math"&gt;\(q(x) \propto p(x) f(x).\)&lt;/span&gt; This choice of &lt;span class="math"&gt;\(q\)&lt;/span&gt; gives us a &lt;em&gt;zero
variance&lt;/em&gt; estimate &lt;em&gt;with a single sample&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Of course, this is an unreasonable distribution to use because the normalizing
constant &lt;em&gt;is the thing you are trying to estimate&lt;/em&gt;, but it is proof that &lt;em&gt;better
proposal distributions exist&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The key to doing better than &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is to take &lt;span class="math"&gt;\(f\)&lt;/span&gt; into account. Look up
"importance sampling for variance reduction" to learn more.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="statistics"></category><category term="sampling"></category><category term="importance-sampling"></category></entry><entry><title>Importance sampling</title><link href="https://timvieira.github.io/blog/post/2014/12/21/importance-sampling/" rel="alternate"></link><published>2014-12-21T00:00:00-05:00</published><updated>2014-12-21T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-12-21:/blog/post/2014/12/21/importance-sampling/</id><content type="html">&lt;p&gt;Importance sampling is a powerful and pervasive technique in statistics, machine learning, and randomized algorithms.&lt;/p&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Importance sampling is a technique for estimating the expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of a random variable &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; under distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; from samples of a different distribution &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The key observation is that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; can be expressed as the expectation of a different random variable &lt;span class="math"&gt;\(f^*(x)=\frac{p(x)}{q(x)}\! \cdot\! f(x)\)&lt;/span&gt; under &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}_{q}\! \left[ f^*(x) \right] = \mathbb{E}_{q}\! \left[ \frac{p(x)}{q(x)} f(x) \right] = \sum_{x} q(x) \frac{p(x)}{q(x)} f(x) = \sum_{x} p(x) f(x) = \mathbb{E}_{p}\! \left[ f(x) \right] = \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Technical condition: &lt;span class="math"&gt;\(q\)&lt;/span&gt; must have support everywhere &lt;span class="math"&gt;\(p\)&lt;/span&gt; does, &lt;span class="math"&gt;\(f(x) p(x) &amp;gt; 0 \Rightarrow q(x) &amp;gt; 0.\)&lt;/span&gt;  Without this condition, the equation is biased!  Note: &lt;span class="math"&gt;\(q\)&lt;/span&gt; can support things that &lt;span class="math"&gt;\(p\)&lt;/span&gt; doesn't.&lt;/p&gt;
&lt;p&gt;Terminology: The quantity &lt;span class="math"&gt;\(w(x) = \frac{p(x)}{q(x)}\)&lt;/span&gt; is often referred to as the &lt;em&gt;importance weight&lt;/em&gt; or &lt;em&gt;importance correction&lt;/em&gt;. We often refer to &lt;span class="math"&gt;\(p\)&lt;/span&gt; as the target density and &lt;span class="math"&gt;\(q\)&lt;/span&gt; as the proposal density.&lt;/p&gt;
&lt;p&gt;Now, given samples &lt;span class="math"&gt;\(\{ x^{(i)} \}_{i=1}^{n}\)&lt;/span&gt; from &lt;span class="math"&gt;\(q,\)&lt;/span&gt; we can use the Monte Carlo estimate, &lt;span class="math"&gt;\(\hat{\mu} \approx \frac{1}{n} \sum_{i=1}^n f^{*}(x^{(i)}),\)&lt;/span&gt; as an unbiased estimator of &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;There are a few reasons we might want to use importance sampling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convenience&lt;/strong&gt;: It might be trickier to sample directly from &lt;span class="math"&gt;\(p.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bias-correction&lt;/strong&gt;: Suppose we're developing an algorithm that requires samples to satisfy some safety condition (e.g., a minimum support threshold) and be unbiased. Importance sampling can be used to remove bias while satisfying the condition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: It might be the case that sampling directly from &lt;span class="math"&gt;\(p\)&lt;/span&gt; would require more samples to estimate &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt; Check out these &lt;a href="http://www.columbia.edu/~mh2078/MCS04/MCS_var_red2.pdf"&gt;great notes&lt;/a&gt; for more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-policy evaluation and learning&lt;/strong&gt;: We might want to collect some exploratory data from &lt;span class="math"&gt;\(q\)&lt;/span&gt; and evaluate different &lt;em&gt;policies&lt;/em&gt; &lt;span class="math"&gt;\(p\)&lt;/span&gt; (e.g., to pick the best one). Here's a link to a future post on [off-policy evaluation and counterfactual reasoning (https://timvieira.github.io/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/) and some cool papers:
     &lt;a href="http://arxiv.org/abs/1209.2355"&gt;counterfactual reasoning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;reinforcement learning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/1103.4601"&gt;contextual bandits&lt;/a&gt;,
     &lt;a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;domain adaptation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few common cases for &lt;span class="math"&gt;\(q\)&lt;/span&gt; worth separate consideration:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: This is the case in experimental design, variance reduction, active learning, and reinforcement learning. It's often difficult to design &lt;span class="math"&gt;\(q,\)&lt;/span&gt; which results in an estimator with reasonable variance. A very difficult case is in off-policy evaluation because it (essentially) requires a good exploratory distribution for every possible policy. (I have much more to say on this topic.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Little to no control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: For example, you're given some dataset (e.g., new articles), and you want to estimate performance on a different dataset (e.g., Twitter).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: In this case, we want to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; (typically referred to as the propensity score) and use it in the importance sampling estimator. As far as I can tell, this technique is widely used to remove selection bias when estimating the effects of different treatments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;: The main drawback of importance sampling is variance. A few bad samples with large weights can drastically throw off the estimator. Thus, it's often the case that a biased estimator is preferred, e.g., &lt;a href="https://hips.seas.harvard.edu/blog/2013/01/14/unbiased-estimators-of-partition-functions-are-basically-lower-bounds/"&gt;estimating the partition function&lt;/a&gt;, &lt;a href="http://arxiv.org/abs/1209.2355"&gt;clipping weights&lt;/a&gt;, &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;indirect importance sampling&lt;/a&gt;. A secondary drawback is that both densities must be normalized, which is often intractable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's next?&lt;/strong&gt; I plan to cover variance reduction and
&lt;a href="https://timvieira.github.io/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation&lt;/a&gt; in more detail in future posts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="statistics"></category><category term="importance-sampling"></category><category term="sampling"></category></entry><entry><title>Gumbel-max trick and weighted reservoir sampling</title><link href="https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/" rel="alternate"></link><published>2014-08-01T00:00:00-04:00</published><updated>2014-08-01T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-08-01:/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</id><content type="html">&lt;p&gt;A while back, &lt;a href="http://people.cs.umass.edu/~wallach/"&gt;Hanna&lt;/a&gt; and I stumbled upon
the following blog post:
&lt;a href="http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm"&gt;Algorithms Every Data Scientist Should Know: Reservoir Sampling&lt;/a&gt;,
which got us excited about reservoir sampling.&lt;/p&gt;
&lt;p&gt;Around the same time, I attended a talk by
&lt;a href="http://cs.haifa.ac.il/~tamir/"&gt;Tamir Hazan&lt;/a&gt; about some of his work on
perturb-and-MAP
&lt;a href="http://cs.haifa.ac.il/~tamir/papers/mean-width-icml12.pdf"&gt;(Hazan &amp;amp; Jaakkola, 2012)&lt;/a&gt;,
which is inspired by the
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;Gumbel-max-trick&lt;/a&gt;
(see &lt;a href="/blog/post/2014/07/31/gumbel-max-trick/"&gt;previous post&lt;/a&gt;). The apparent similarity between weighted reservoir sampling and the Gumbel-max trick lead us to make some cute connections, which I'll describe in this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We're given a stream of unnormalized probabilities, &lt;span class="math"&gt;\(x_1, x_2, \cdots\)&lt;/span&gt;. At any point in time &lt;span class="math"&gt;\(t\)&lt;/span&gt; we'd like to have a sampled index &lt;span class="math"&gt;\(i\)&lt;/span&gt; available, where the probability of &lt;span class="math"&gt;\(i\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(\pi_t(i) = \frac{x_i}{
\sum_{j=1}^t x_j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume, without loss of generality, that &lt;span class="math"&gt;\(x_i &amp;gt; 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. (If any element has a zero weight we can safely ignore it since it should never be sampled.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Gumbel-max sampler&lt;/strong&gt;: I came up with the following algorithm, which is a simple "modification" of the Gumbel-max-trick for handling streaming data:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}  \ \ \text{# maximum value and index}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Compute log-unnormalized probabilities&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(w_i = \log(x_i)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(z_i \sim \text{Gumbel}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(k_i = w_i + z_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; (i.e. the argmax)&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If we interrupt this algorithm at any point, we have a sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After convincing myself this algorithm was correct, I sat down to try to
understand the algorithm in the blog post, which is due to Efraimidis and
Spirakis (2005) (&lt;a href="http://dl.acm.org/citation.cfm?id=1138834"&gt;paywall&lt;/a&gt;,
&lt;a href="http://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf"&gt;free summary&lt;/a&gt;). They
looked similar in many ways but used different sorting keys/perturbations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efraimidis and Spirakis (2005)&lt;/strong&gt;: Here is the ES algorithm for weighted
reservoir sampling&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# compute randomized key&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(u_i \sim \text{Uniform}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(e_i = u_i^{(\frac{1}{x_i})}\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(e_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Again, if we interrupt this algorithm at any point, we have our sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;. Note
that you can simplify &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; so that you don't have to compute &lt;code&gt;pow&lt;/code&gt; (which is nice
because &lt;code&gt;pow&lt;/code&gt; is pretty slow). It's equivalent to use &lt;span class="math"&gt;\(e'_i = \log(e_i) =
\log(u_i)/x_i\)&lt;/span&gt; because &lt;span class="math"&gt;\(\log\)&lt;/span&gt; is monotonic. (Note that &lt;span class="math"&gt;\(-e'_i \sim
\textrm{Exponential}(x_i)\)&lt;/span&gt;.)&lt;/p&gt;
&lt;!--
I find this version of the algorithm more intuitive, since it's well-known that
$\left(\underset{{i=1 \ldots t}}{\min} \textrm{Exponential}(x_i) \right) =
\textrm{Exponential}\left(\sum_{i=1}^t x_i \right)$. This version makes it clear
that minimizing is actually summing. However, we want the argmin, which is
distributed according to $\pi_t$.
--&gt;

&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: At a high level, we can see that both algorithms compute a
randomized key and take an argmax. What's the relationship between the keys?&lt;/p&gt;
&lt;p&gt;First, note that a &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; variate can be generated via
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1)))\)&lt;/span&gt;. This is a straightforward application of
the
&lt;a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling"&gt;inverse transform sampling&lt;/a&gt;
method for random number generation. This means that if we use the same sequence
of uniform random variates then, &lt;span class="math"&gt;\(z_i = -\log(-\log(u_i))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, this does not give use equality between &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;, but it does
turn out that &lt;span class="math"&gt;\(k_i = -\log(-\log(e_i))\)&lt;/span&gt;, which is useful because this is a
monotonic transformation on the interval &lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;. Since monotonic
transformations preserve ordering, the sequences &lt;span class="math"&gt;\(k\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; result in the same
comparison decisions, as well as, the same argmax. In summary, the algorithms
are the same!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extensions&lt;/strong&gt;: After reading a little further along in the ES paper, we see
that the same algorithm can be used to perform &lt;em&gt;sampling without replacement&lt;/em&gt; by
sorting and taking the elements with the highest keys. This same modification applies to the Gumbel-max-trick because the keys have precisely the same ordering as ES. In practice, we don't sort the key, but instead, use a bounded priority queue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing&lt;/strong&gt;: To the best of my knowledge, the connection between the
Gumbel-max trick and ES is undocumented. Furthermore, the Gumbel-max-trick is not known as a streaming algorithm, much less known to perform sampling without replacement! If you know of a reference, let me know.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to cite this article&lt;/strong&gt;: If you found this article useful, please cite it as&lt;/p&gt;
&lt;pre style="background-color: white; color: black; border: #333;"&gt;
@misc{vieira2014gumbel,
    title = {Gumbel-max trick and weighted reservoir sampling},
    author = {Tim Vieira},
    url = {http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/},
    year = {2014}
}
&lt;/pre&gt;

&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;p&gt;I have a few other articles that are related to
&lt;a href="http://timvieira.github.io/blog/tag/sampling-without-replacement.html"&gt;sampling without replacement&lt;/a&gt;,
&lt;a href="http://timvieira.github.io/blog/tag/reservoir-sampling.html"&gt;reservoir sampling&lt;/a&gt;,
and &lt;a href="http://timvieira.github.io/blog/tag/gumbel.html"&gt;Gumbel tricks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are a few interesting papers that build on the ideas in this article.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wouter Kool, Herke van Hoof, and Max Welling. 2019.
&lt;a href="https://arxiv.org/abs/1903.06059"&gt;Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sang Michael Xie and Stefano Ermon. 2019. &lt;a href="https://arxiv.org/abs/1901.10517"&gt;Reparameterizable Subset Sampling via Continuous Relaxations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="sampling"></category><category term="Gumbel"></category><category term="reservoir-sampling"></category><category term="sampling-without-replacement"></category></entry><entry><title>Gumbel-max trick</title><link href="https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/" rel="alternate"></link><published>2014-07-31T00:00:00-04:00</published><updated>2014-07-31T00:00:00-04:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2014-07-31:/blog/post/2014/07/31/gumbel-max-trick/</id><content type="html">&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Sampling from a discrete distribution parametrized by unnormalized
log-probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_k = \frac{1}{z} \exp(x_k)   \ \ \ \text{where } z = \sum_{j=1}^K \exp(x_j)
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The usual way&lt;/strong&gt;: Exponentiate and normalize (using the
&lt;a href="/blog/post/2014/02/11/exp-normalize-trick/"&gt;exp-normalize trick&lt;/a&gt;), then use the
an algorithm for sampling from a discrete distribution (aka categorical):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;usual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;     &lt;span class="c1"&gt;# the exp-normalize trick&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The Gumbel-max trick&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \underset{ i \in \{1,\cdots,K\} }{\operatorname{argmax}} x_i + z_i
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_1 \cdots z_K\)&lt;/span&gt; are i.i.d. &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; random variates. It
turns out that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is distributed according to &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. (See the short derivations
in this
&lt;a href="https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/"&gt;blog post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Implementing the Gumbel-max trick is remarkable easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gumbel_max_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gumbel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't have access to a Gumbel random variate generator, you can use
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Number of calls to the random number generator: Gumbel-max requires &lt;span class="math"&gt;\(K\)&lt;/span&gt;
     samples from a uniform, whereas the usual algorithm only requires &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gumbel is a one-pass algorithm: It does not need to see all of the data
     (e.g., to normalize) before it can start partially sampling. Thus,
     Gumbel-max can be used for
     &lt;a href="http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/"&gt;weighted sampling from a stream&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Low-level efficiency: The Gumbel-max trick requires &lt;span class="math"&gt;\(2K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt;,
     whereas ordinary requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log\)&lt;/span&gt; are
     expensive function, we'd like to avoid calling them. What gives? Well,
     Gumbel's calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt; do not depend on the data so they can be
     precomputed; this is handy for implementations which rely on vectorization
     for efficiency, e.g. python+numpy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;: I have a few posts relating to the Gumbel-max trick. Have a
look at &lt;a href="/blog/tag/gumbel.html"&gt;posts tagged with Gumbel&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="sampling"></category><category term="Gumbel"></category></entry></feed>