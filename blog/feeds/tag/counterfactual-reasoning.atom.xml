<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Graduate Descent - counterfactual-reasoning</title><link href="https://timvieira.github.io/blog/" rel="alternate"></link><link href="/blog/feeds/tag/counterfactual-reasoning.atom.xml" rel="self"></link><id>https://timvieira.github.io/blog/</id><updated>2016-12-19T00:00:00-05:00</updated><entry><title>Counterfactual reasoning and learning from logged data</title><link href="https://timvieira.github.io/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/" rel="alternate"></link><published>2016-12-19T00:00:00-05:00</published><updated>2016-12-19T00:00:00-05:00</updated><author><name>Tim Vieira</name></author><id>tag:timvieira.github.io,2016-12-19:/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/</id><content type="html">&lt;style&gt; .toggle-button { background-color: #555555; border: none; color: white;
padding: 10px 15px; border-radius: 6px; text-align: center; text-decoration:
none; display: inline-block; font-size: 16px; cursor: pointer; } .derivation {
background-color: #f2f2f2; border: thin solid #ddd; padding: 10px;
margin-bottom: 10px; } &lt;/style&gt;

&lt;script&gt;
/* workaround for when markdown/mathjax gets confused by the javascript dollar function. */
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;Counterfactual reasoning is &lt;em&gt;reasoning about data that we did not observe&lt;/em&gt;. For
example, reasoning about the expected reward of new policy given data collected
from a older one.&lt;/p&gt;
&lt;p&gt;In this post, I'll discuss some basic techniques for learning from logged
data. For the large part, this post is based on things I learned from
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Peshkin &amp;amp; Shelton (2002)&lt;/a&gt;
and &lt;a href="https://arxiv.org/abs/1209.2355"&gt;Bottou et al. (2013)&lt;/a&gt; (two of all-time
favorite papers).&lt;/p&gt;
&lt;p&gt;After reading, have a look at the
&lt;a href="https://gist.github.com/timvieira/788c2c25c94663c49abada60f2e107e9"&gt;Jupyter notebook&lt;/a&gt;
accompanying this post!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; (&lt;em&gt;off-line off-policy optimization&lt;/em&gt;): We're trying to optimize a
function of the form,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \underset{p_\theta}{\mathbb{E}} \left[ r(x) \right] = \sum_{x \in \mathcal{X}} p_\theta(x) r(x).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; But! We only have a &lt;em&gt;fixed&lt;/em&gt; sample of size &lt;span class="math"&gt;\(m\)&lt;/span&gt; from a data collection
policy &lt;span class="math"&gt;\(q\)&lt;/span&gt;, &lt;span class="math"&gt;\(\{ (r^{(j)}, x^{(j)} ) \}_{j=1}^m \overset{\text{i.i.d.}} \sim q.\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Although, it's not &lt;em&gt;necessarily&lt;/em&gt; the case, you can think of &lt;span class="math"&gt;\(q = p_{\theta'}\)&lt;/span&gt;
  for a &lt;em&gt;fixed&lt;/em&gt; value &lt;span class="math"&gt;\(\theta'.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; is an arbitrary multivariate space, which permits a mix of
  continuous and discrete components, with appropriate densities, &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt;
  and &lt;span class="math"&gt;\(q\)&lt;/span&gt; defined over it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(r: \mathcal{X} \mapsto \mathbb{R}\)&lt;/span&gt; is a black box that outputs a scalar
  score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I've used the notation &lt;span class="math"&gt;\(r^{(j)}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(r(x^{(j)})\)&lt;/span&gt; to emphasize that we
  can't evaluate &lt;span class="math"&gt;\(r\)&lt;/span&gt; at &lt;span class="math"&gt;\(x\)&lt;/span&gt; values other than those in the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We'll assume that &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns positive probability everywhere, &lt;span class="math"&gt;\(q(x) &amp;gt; 0\)&lt;/span&gt; for
  all &lt;span class="math"&gt;\(x \in \mathcal{X}\)&lt;/span&gt;. This means is that the data collection process must
  be randomized and eventually sample all possible configurations. Later, I
  discuss relaxing this assumption.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each distribution is a product of one or more factors of the following types:
&lt;strong&gt;policy factors&lt;/strong&gt; (at least one), which directly depend on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, and
&lt;strong&gt;environment factors&lt;/strong&gt; (possibly none), which do not depend directly on
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Note that environment factors are &lt;em&gt;only&lt;/em&gt; accessible via sampling
(i.e., we don't know the &lt;em&gt;value&lt;/em&gt; they assign to a sample). For example, a
&lt;em&gt;contextual bandit problem&lt;/em&gt;, where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a context-action pair, &lt;span class="math"&gt;\(x =
(s,a)\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(q(x) = q(a|s) p(s)\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{\theta}(x) = p_{\theta}(a|s)
p(s)\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share the environment factor &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;, the
distribution over contexts, and only differ in the action-given-context
factor. For now, assume that we can evaluate all environment factors; later,
I'll discuss how we cleverly work around it.&lt;/p&gt;
&lt;!--
(We can
even extend it to a full-blown MDP or POMDP by taking $x$ to be a sequence of
state-action pairs, often called "trajectories".)
--&gt;

&lt;p&gt;&lt;strong&gt;The main challenge&lt;/strong&gt; of this setting is that we don't have controlled
experiments to learn from because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is not (completely) in our control. This
manifests itself as high variance ("noise") in estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;. Consider
the contextual bandit setting, we receive a context &lt;span class="math"&gt;\(s\)&lt;/span&gt; and execute single
action; we never get to rollback to that precise context and try an alternative
action (to get a paired sample #yolo) because we do not control &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;. This is
an important paradigm for many 'real world' problems, e.g., predicting medical
treatments or ad selection.&lt;/p&gt;
&lt;!--
This is the crucial difference that makes counterfactual learning
more difficult than (fully) supervised learning.
--&gt;

&lt;p&gt;&lt;strong&gt;Estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;&lt;/strong&gt; [V1]: We obtain an unbiased estimator of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;
with
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta)
\approx \hat{J}_{\!\text{IS}}(\theta)
= \frac{1}{m} \sum_{j=1}^m r^{(j)} \!\cdot\! w^{(j)}_{\theta}
\quad \text{ where } w^{(j)}_{\theta} = \frac{p_{\theta}(x^{(j)}) }{ q(x^{(j)}) }.
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; This estimator is remarkable: it uses importance sampling as a function
approximator! We have an &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt; for any value of
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that we like. &lt;em&gt;The catch&lt;/em&gt; is that we have to pick &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; a priori,
i.e., with no knowledge of the sample.&lt;/p&gt;
&lt;!--
We also require that the usual 'support
conditions' for importance sampling conditions ($p_{\theta}(x)&gt;0 \Rightarrow
q(x)&gt;0$ for all $x \in \mathcal{X}$, which is why we made assumption A1.
--&gt;

&lt;p&gt;After we've collected a (large) sample it's possible to optimize
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; using any optimization algorithm (e.g., L-BFGS). Of
course, we risk overfitting to the sample if we evaluate
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt;. Actually, it's a bit worse: this objective tends to
favor regions of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, which are not well represented in the sample because
the importance sampling estimator has high variance in these regions resulting
from large importance weights (when &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; is small and &lt;span class="math"&gt;\(p_{\theta}(x)\)&lt;/span&gt; is
large, &lt;span class="math"&gt;\(w(x)\)&lt;/span&gt; is large and consequently so is &lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; regardless
of whether &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; is high!). Thus, we want some type of "regularization" to keep
the optimizer in regions which are sufficiently well-represented by the sample.&lt;/p&gt;
&lt;!--
**Visual example**: We can visualize this phenomena in a simple example. Let $q
= \mathcal{N}(0, \sigma=5)$, $r(x) = 1 \text{ if } x \in [2, 3], 0.2 \text{
otherwise},$ and $p_\theta = \mathcal{N}(\theta, \sigma=1)$.  This example is
nice because it let's us plot $x$ and $\theta$ in the same space. This is
generally not the case, because $\mathcal{X}$ may have no connection to $\theta$
space, e.g., $\mathcal{X}$ may be discrete.

**TODO** add plot
--&gt;

&lt;p&gt;&lt;strong&gt;Better surrogate&lt;/strong&gt; [V2]: There are many ways to improve the variance of the
estimator and &lt;em&gt;confidently&lt;/em&gt; obtain improvements to the system. One of my
favorites is Bottou et al.'s lower bound on &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;, which we get by
clipping importance weights, replace &lt;span class="math"&gt;\(w^{(j)}_{\theta}\)&lt;/span&gt; with &lt;span class="math"&gt;\(\min(R,
w^{(j)}_{\theta})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Confidence intervals&lt;/strong&gt; [V3]: We can augment the V2 lower bound with confidence
intervals derived from the empirical Bernstein bound (EBB). We'll require that
&lt;span class="math"&gt;\(r\)&lt;/span&gt; is bounded and that we know its max/min values. The EBB &lt;em&gt;penalizes&lt;/em&gt;
hypotheses (values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;) which have higher sample variance. (Note: a
Hoeffding bound wouldn't change the &lt;em&gt;shape&lt;/em&gt; of the objective, but EBB does
thanks to the sample variance penalty. EBB tends to be tighter.). The EBB
introduces an additional "confidence" hyperparameter, &lt;span class="math"&gt;\((1-\delta)\)&lt;/span&gt;. Bottou et
al. recommend maximizing the lower bound as it provides safer improvements. See
the original paper for the derivation.&lt;/p&gt;
&lt;!--
An important benefit of having upper *and* lower is that the bounds tell
us whether or not we should collect more data
--&gt;

&lt;p&gt;Both V2 and V3 are &lt;em&gt;biased&lt;/em&gt; (as they are lower bounds), but we can mitigate the
bias by &lt;em&gt;tuning&lt;/em&gt; the hyperparameter &lt;span class="math"&gt;\(R\)&lt;/span&gt; on a heldout sample (we can even tune
&lt;span class="math"&gt;\(\delta\)&lt;/span&gt;, if desired). Additionally, V2 and V3 are 'valid' when &lt;span class="math"&gt;\(q\)&lt;/span&gt; has limited
support since they prevent the importance weights from exploding (of course, the
bias can be arbitrarily bad, but probably unavoidable given the
learning-from-only-logged data setup).&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;!--
**Be warned**: This may be considered an idealized setting. Much of the research
in counterfactual and causal reasoning targets (often subtle) deviations from
these assumptions (and some different questions, of course). Some extensions and
discussion appear towards the end of the post.
--&gt;

&lt;p&gt;&lt;strong&gt;Unknown environment factors&lt;/strong&gt;: Consider the contextual bandit setting
(mentioned above). Here &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share an &lt;em&gt;unknown&lt;/em&gt; environment factor: the
distribution of contexts. Luckily, we do not need to know the value of this
factor in order to apply any of our estimators because they are all based on
likelihood &lt;em&gt;ratios&lt;/em&gt;, thus the shared unknown factors cancel out!  Some specific
examples are given below. Of course, these factors do influence the estimators
because they are crucial in &lt;em&gt;sampling&lt;/em&gt;, they just aren't necessary in
&lt;em&gt;evaluation&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In contextual bandit example, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a state-action pair, &lt;span class="math"&gt;\(w_{\theta}(x) =
    \frac{p_{\theta}(x)}{q(x)} = \frac{ p_{\theta}(s,a) }{ q(s,a) } =
    \frac{p_{\theta}(a|s) p(s)}{q(a|s) p(s)} = \frac{p_{\theta}(a|s) }{ q(a|s)
    }\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In a Markov decision process, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a sequence of state-action pairs,
    &lt;span class="math"&gt;\(w_{\theta}(x) = \frac{p_{\theta}(x)}{q(x)} = \frac{ p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) p_\theta(a_t|s_t) } { p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) q(a_t|s_t) } = \frac{\prod_{t=0}^T \pi_\theta(a_t|s_t)}
    {\prod_{t=0}^T q(a_t|s_t)}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: These estimators can all be improved with variance
reduction techniques. Probably the most effective technique is using
&lt;a href="https://en.wikipedia.org/wiki/Control_variates"&gt;control variates&lt;/a&gt; (of which
baseline functions are a special case). These are random variables correlated
with &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; for which we know their expectations (or at least they are estimated
separately). A great example is how ad clicks depend strongly on time-of-day
(fewer people are online late at night so we get fewer clicks), thus the
time-of-day covariate explains a large part of the variation in &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimation instead of optimization&lt;/strong&gt;: You can use this general setup for
estimation instead of optimization, in which case it's fine to let &lt;span class="math"&gt;\(r\)&lt;/span&gt; have
real-valued multivariate output. The confidence intervals are probably useful in
that setting too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: Often &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an existing complex system, which does not record
its probabilities. It is possible to use regression to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; from the
samples, which is called the &lt;strong&gt;propensity score&lt;/strong&gt; (PS). PS attempts to account
for &lt;strong&gt;confounding variables&lt;/strong&gt;, which are hidden causes that control variation in
the data. Failing to account for confounding variables may lead to
&lt;a href="https://en.wikipedia.org/wiki/Simpson's_paradox"&gt;incorrect conclusions&lt;/a&gt;. Unfortunately,
PS results in a biased estimator because we're using a 'ratio of expectations'
(we'll divide by the PS estimate) instead of an 'expectation of ratios'. PS is
only statistically consistent in the (unlikely) event that the density estimate
is correctly specified (i.e., we can eventually get &lt;span class="math"&gt;\(q\)&lt;/span&gt; correct). In the unknown
&lt;span class="math"&gt;\(q\)&lt;/span&gt; setting, it's often better to use the &lt;strong&gt;doubly-robust estimator&lt;/strong&gt; (DR) which
combines &lt;em&gt;two&lt;/em&gt; estimators: a density estimator for &lt;span class="math"&gt;\(q\)&lt;/span&gt; and a function
approximation for &lt;span class="math"&gt;\(r\)&lt;/span&gt;. A great explanation for the bandit case is in
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Dudík et al. (2011)&lt;/a&gt;. The DR estimator is also
biased, but it has a better bias-variance tradeoff than PS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; doesn't have support everywhere?&lt;/strong&gt; This is an especially important
setting because it is often the case that data collection policies abide by some
&lt;strong&gt;safety regulations&lt;/strong&gt;, which prevent known bad configurations. In many
situations, evaluating &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; corresponds to executing an action &lt;span class="math"&gt;\(x\)&lt;/span&gt; in the real
world so terrible outcomes could occur, such as, breaking a system, giving a
patient a bad treatment, or losing money. V1 is ok to use as long as we satisfy
the importance sampling support conditions, which might mean rejecting certain
values for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; (might be non-trivial to enforce) and consequently finding a
less-optimal policy. V2 and V3 are ok to use without an explicit constraint, but
additional care may be needed to ensure specific safety constraints are
satisfied by the learned policy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; is deterministic?&lt;/strong&gt; This is related to the point above. This is a
hard problem. Essentially, this trying to learn without any exploration /
experimentation! In general, we need exploration to learn. Randomization isn't
the only way to perform exploration, there are many systematic types of
experimentation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are some cases of systematic experimentation that are ok. For example,
    enumerating all elements of &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; (almost certainly
    infeasible). Another example is a contextual bandit where &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns
    actions to contexts deterministically via a hash function (this setting is
    fine because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is essentially a uniform distribution over actions, which
    is independent of the state). In other special cases, we &lt;em&gt;may&lt;/em&gt; be able to
    characterize systematic exploration as
    &lt;a href="https://en.wikipedia.org/wiki/Stratified_sampling"&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A generic solution might be to apply the doubly-robust estimator, which
    "smooths out" deterministic components (by pretending they are random) and
    accounting for confounds (by explicitly modeling them in the propensity
    score).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What if we control data collection (&lt;span class="math"&gt;\(q\)&lt;/span&gt;)?&lt;/strong&gt; This is an interesting
setting. Essentially, it asks "how do we explore/experiment optimally (and
safely)?". In general, this is an open question and depends on many
considerations, such as, how much control, exploration cost (safety constraints)
and prior knowledge (of &lt;span class="math"&gt;\(r\)&lt;/span&gt; and unknown factors in the environment). I've seen
some papers cleverly design &lt;span class="math"&gt;\(q\)&lt;/span&gt;. The first that comes to mind is
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Levine &amp;amp; Koltun (2013)&lt;/a&gt;. Another
setting is &lt;em&gt;online&lt;/em&gt; contextual bandits, in which algorithms like
&lt;a href="http://jmlr.org/proceedings/papers/v15/beygelzimer11a/beygelzimer11a.pdf"&gt;EXP4&lt;/a&gt;
and
&lt;a href="http://www.research.rutgers.edu/~lihong/pub/Chapelle12Empirical.pdf"&gt;Thompson sampling&lt;/a&gt;,
prescribe certain types of exploration and work interactively (i.e., they don't
have a fixed training sample). Lastly, I'll mention that there are many
techniques for variance reduction by importance sampling, which may apply.&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max
Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson.
&lt;a href="https://arxiv.org/abs/1209.2355"&gt;Counterfactual reasoning in learning systems&lt;/a&gt;.
JMLR 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The source for the majority of this post. It includes many other interesting
ideas and goes more in depth into some of the details.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Miroslav Dudík, John Langford, Lihong Li.
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Doubly robust policy evaluation and learning&lt;/a&gt;.
ICML 2011.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discussed in extensions section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Philip S. Thomas.
&lt;a href="http://psthomas.com/papers/Thomas2015c.pdf"&gt;Safe reinforcement learning&lt;/a&gt;.
PhD Thesis 2015.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Covers confidence intervals for policy evaluation similar to Bottou et al., as
well as learning algorithms for RL with safety guarantees (e.g., so we don't
break the robot).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Peshkin and Shelton.
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Learning from scarce experience&lt;/a&gt;.
ICML 2002.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An older RL paper, which covers learning from logged data. This is one of the
earliest papers on learning from logged data that I could find.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Levine and Koltun.
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Guided policy search&lt;/a&gt;.
ICML 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses clever choices for &lt;span class="math"&gt;\(q\)&lt;/span&gt; to better-guide learning in the RL setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corinna Cortes, Yishay Mansour, Mehryar Mohri.
&lt;a href="https://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;Learning bounds for importance weighting&lt;/a&gt;.
NIPS 2010.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses &lt;em&gt;generalization bounds&lt;/em&gt; for the counterfactual objective. Includes an
alternative weighting scheme to keep importance weights from exploding.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="misc"></category><category term="counterfactual-reasoning"></category><category term="importance-sampling"></category><category term="machine-learning"></category></entry></feed>