<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Graduate Descent</title><link>http://timvieira.github.io/blog/</link><description></description><lastBuildDate>Fri, 18 Aug 2017 00:00:00 -0400</lastBuildDate><item><title>Backprop is not just the chain rule</title><link>http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/</link><description>&lt;p&gt;Almost everyone I know says that "backprop is just the chain rule." Although
that's &lt;em&gt;basically true&lt;/em&gt;, there are some subtle and beautiful things about
automatic differentiation techniques (including backprop) that will not be
appreciated with this &lt;em&gt;dismissive&lt;/em&gt; attitude.&lt;/p&gt;
&lt;p&gt;This leads to a poor understanding. As
&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;I have ranted before&lt;/a&gt;:
people do not understand basic facts about autodiff.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt; is provably as fast as evaluating &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Let that sink in. Computing the gradient&amp;mdash;an essential incredient to
efficient optimization&amp;mdash;is no slower to compute than the function
itself. Contrast that with the finite-difference gradient approximation, which
is quite accurate, but its runtime is $\textrm{dim}(x)$ times slower than
evaluating $f$
([discussed here](http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/))!
--&gt;

&lt;ul&gt;
&lt;li&gt;Code for &lt;span class="math"&gt;\(\nabla f(x)\)&lt;/span&gt; can be derived by a rote program transformation, even
  if the code has control flow structures like loops and intermediate variables
  (as long as the control flow is independent of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). You can even do this
  "automatic" transformation by hand!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Autodiff &lt;span class="math"&gt;\(\ne\)&lt;/span&gt; what you learned in calculus&lt;/h3&gt;
&lt;p&gt;Let's try to understand the difference between autodiff and the type of
differentiation that you learned in calculus, which is called &lt;em&gt;symbolic&lt;/em&gt;
differentiation.&lt;/p&gt;
&lt;p&gt;I'm going to use an example from
&lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's notes&lt;/a&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) = \exp(\exp(x) + \exp(x)^2) + \sin(\exp(x) + \exp(x)^2).
$$&lt;/div&gt;
&lt;!--
If we plug-and-chug with the chain rule, we get a correct expression for the
derivative,
$$\small
\frac{\partial f}{\partial x} =
\exp(\exp(x) + \exp(x)^2) (\exp(x) + 2 \exp(x) \exp(x)) \\
\quad\quad\small+ \cos(\exp(x) + \exp(x)^2) (\exp(x) + 2 \exp(x) \exp(x)).
$$

However, this expression leaves something to be desired because it has lot of
repeated evaluations of the same function. This is clearly bad, if we want to
turn it into source code.
--&gt;

&lt;p&gt;If we were writing &lt;em&gt;a program&lt;/em&gt; (e.g., in Python) to compute &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we'd definitely
take advantage of the fact that it has a lot of repeated evaluations for
efficiency.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Symbolic differentiation would have to use the "flat" version of this function,
so no intermediate variable &lt;span class="math"&gt;\(\Rightarrow\)&lt;/span&gt; slow.&lt;/p&gt;
&lt;p&gt;Automatic differentiation let's us differentiate a program with &lt;em&gt;intermediate&lt;/em&gt;
variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The rules for transforming the code for a function into code for the gradient
  are really minimal (fewer things to memorize!). Additionally, the rules are
  more general than in symbolic case because they handle as superset of
  programs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quite &lt;a href="http://conal.net/papers/beautiful-differentiation/"&gt;beautifully&lt;/a&gt;, the
  program for the gradient &lt;em&gt;has exactly the same structure&lt;/em&gt; as the function,
  which implies that we get the same runtime (up to some constants factors).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I won't give the details of how to execute the backpropagation transform to the
program. You can get that from
&lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's notes&lt;/a&gt;
and many other good
resources. &lt;a href="https://gist.github.com/timvieira/39e27756e1226c2dbd6c36e83b648ec2"&gt;Here's some code&lt;/a&gt;
that I wrote that accompanies to the &lt;code&gt;f(x)&lt;/code&gt; example, which has a bunch of
comments describing the manual "automatic" differentiation process on &lt;code&gt;f(x)&lt;/code&gt;.&lt;/p&gt;
&lt;!--
Caveat: You might have seen some *limited* cases where an input variable was
reused, but chances are that it was something really simple like multiplication
or division, e.g., $\nabla\! \left[ f(x) \cdot g(x) \right] = f(x) \cdot g'(x)
+ f'(x) \cdot g(x)$, and you just memorized a rule. The rules of autodiff are
simpler and actually explains why there is a sum in the product rule. You can
also rederive the quotient rule without a hitch. I'm all about having fewer
things to memorize!
--&gt;

&lt;!--
You might hope that something like common subexpression elimination would save
the symbolic approach. Indeed that could be leveraged to improve any chunk of
code, but to match efficiency it's not needed! If we had needed to blow up the
computation to then shrink it down that would be much less efficient! The "flat"
version of a program can be exponentially larger than a version with reuse.
--&gt;

&lt;!-- Only sort of related: think of the exponential blow up in converting a
Boolean expression from conjunctive normal form to and disjunction normal.  --&gt;

&lt;h2&gt;Autodiff by the method of Lagrange multipliers&lt;/h2&gt;
&lt;p&gt;Let's view the intermediate variables in our optimization problem as simple
equality constraints in an equivalent &lt;em&gt;constrained&lt;/em&gt; optimization problem. It
turns out that the de facto method for handling constraints, the method Lagrange
multipliers, recovers &lt;em&gt;exactly&lt;/em&gt; the adjoints (intermediate derivatives) in the
backprop algorithm!&lt;/p&gt;
&lt;p&gt;Here's our example from earlier written in this constraint form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\underset{x}{\text{argmax}}\ &amp;amp; f \\
\text{s.t.} \quad
a &amp;amp;= \exp(x) \\
b &amp;amp;= a^2     \\
c &amp;amp;= a + b   \\
d &amp;amp;= \exp(c) \\
e &amp;amp;= \sin(c) \\
f &amp;amp;= d + e
\end{align*}
$$&lt;/div&gt;
&lt;h4&gt;The general formuation&lt;/h4&gt;
&lt;div class="math"&gt;\begin{align*}
  &amp;amp; \underset{\boldsymbol{x}}{\text{argmax}}\ z_n &amp;amp; \\
  &amp;amp; \text{s.t.}\quad z_i = x_i                          &amp;amp;\text{ for $1 \le i \le d$} \\
  &amp;amp; \phantom{\text{s.t.}}\quad z_i = f_i(z_{\alpha(i)}) &amp;amp;\text{ for $d &amp;lt; i \le n$} \\
  \end{align*}&lt;/div&gt;
&lt;p&gt;The first set of constraint (&lt;span class="math"&gt;\(1, \ldots, d\)&lt;/span&gt;) are a little silly. They are only
there to keep our formulation tidy. The variables in the program fall into three
categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;input variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;): &lt;span class="math"&gt;\(x_1, \ldots, x_d\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;intermediate variables&lt;/strong&gt;: (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;): &lt;span class="math"&gt;\(z_i = f_i(z_{\alpha(i)})\)&lt;/span&gt; for
  &lt;span class="math"&gt;\(1 \le i \le n\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\alpha(i)\)&lt;/span&gt; is a list of indices from &lt;span class="math"&gt;\(\{1, \ldots,
  n-1\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_{\alpha(i)}\)&lt;/span&gt; is the subvector of variables needed to evaluate
  &lt;span class="math"&gt;\(f_i(\cdot)\)&lt;/span&gt;. Minor detail: take &lt;span class="math"&gt;\(f_{1:d}\)&lt;/span&gt; to be the identity function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;output variable&lt;/strong&gt; (&lt;span class="math"&gt;\(z_n\)&lt;/span&gt;): We assume that our programs has a singled scalar
  output variable, &lt;span class="math"&gt;\(z_n\)&lt;/span&gt;, which represents the quantity we'd like to maximize.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- (It is possible to generalize this story to compute Jacobians of functions
with multi-variate outputs by "scalarizing" the objective, e.g., multiply the
outputs by a vector. This Gives an efficient program for computing Jacobian
vector products that can be used to extra Jacobians.)  --&gt;

&lt;p&gt;The relation &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a
&lt;a href="https://en.wikipedia.org/wiki/Dependency_graph"&gt;dependency graph&lt;/a&gt; among
variables. Thus, &lt;span class="math"&gt;\(\alpha(i)\)&lt;/span&gt; is the list of &lt;em&gt;incoming&lt;/em&gt; edges to node &lt;span class="math"&gt;\(i\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\beta(j) = \{ i: j \in \alpha(i) \}\)&lt;/span&gt; is the set of &lt;em&gt;outgoing&lt;/em&gt; edges. For now,
we'll assume that the dependency graph given by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is ① acyclic: no &lt;span class="math"&gt;\(z_i\)&lt;/span&gt;
can transitively depend on itself.  ② single-assignment: each &lt;span class="math"&gt;\(z_i\)&lt;/span&gt; appears on
the left-hand side of &lt;em&gt;exactly one&lt;/em&gt; equation.  We'll discuss relaxing these
assumptions in &lt;a href="#lagrange-backprop-generalization"&gt;§ Generalizations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The standard way to solve a constrained optimization is to use the method
Lagrange multipliers, which converts a &lt;em&gt;constrained&lt;/em&gt; optimization problem into
an &lt;em&gt;unconstrained&lt;/em&gt; problem with a few more variables &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; (one
per &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; constraint), called Lagrange multipliers.&lt;/p&gt;
&lt;h4&gt;The Lagrangian&lt;/h4&gt;
&lt;p&gt;To handle constaints, let's dig up a tool from our calculus class,
&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;the method of Lagrange multipliers&lt;/a&gt;,
which converts a &lt;em&gt;constrained&lt;/em&gt; optimization probelm into an &lt;em&gt;unconstrainted&lt;/em&gt;
one. The unconstrained version is called "the Lagrangian" of the constrained
problem. Here is its form for our task,&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathcal{L}\left(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\lambda}\right)
= z_n - \sum_{i=d+1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right).
$$&lt;/div&gt;
&lt;p&gt;Optimizing the Lagrangian amounts to solving the following nonlinear system of
equations, which give necessary, but not sufficient, conditions for optimality,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla \mathcal{L}\left(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\lambda}\right) = 0.
$$&lt;/div&gt;
&lt;p&gt;Let's look a little closer at the Lagrangian conditions by breaking up the
system of equations into salient parts, corresponding to which variable types
are affected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intermediate variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;): Optimizing the
multipliers&amp;mdash;i.e., setting the gradient of Lagrangian
w.r.t. &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; to zero&amp;mdash;ensures that the constraints on
intermediate variables are satisfied.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\nabla_{\! \lambda_i} \mathcal{L}
= z_i - f_i(z_{\alpha(i)}) = 0
\quad\Leftrightarrow\quad z_i = f_i(z_{\alpha(i)})
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We can use forward propagation to satisfy these equations, which we may regard
as a block-coordinate step in the context of optimizing the &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;.&lt;/p&gt;
&lt;!--GENERALIZATION:
However, if they are cyclic dependencies we may need to
solve a nonlinear system of equations. (TODO: it's unclear what the more general
cyclic setting is. Perhaps I should having a running example of a cyclic program
and an acyclic program.)
--&gt;

&lt;p&gt;&lt;strong&gt;Lagrange multipliers&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt;, excluding &lt;span class="math"&gt;\(\lambda_n\)&lt;/span&gt;):
  Setting the gradient of the &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; w.r.t. the intermediate variables
  equal to zeros tells us what to do with the intermediate multipliers.&lt;/p&gt;
&lt;div class="math"&gt;\begin{eqnarray*}
0 &amp;amp;=&amp;amp; \nabla_{\! z_j} \mathcal{L} \\
&amp;amp;=&amp;amp; \nabla_{\! z_j}\! \left[ z_n - \sum_{i=d+1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right) \right] \\
&amp;amp;=&amp;amp; - \sum_{i=d+1}^n \lambda_i \nabla_{\! z_j}\! \left[ \left( z_i - f_i(z_{\alpha(i)}) \right) \right] \\
&amp;amp;=&amp;amp; - \left( \sum_{i=d+1}^n \lambda_i \nabla_{\! z_j}\! \left[ z_i \right] \right) + \left( \sum_{i=d+1}^n \lambda_i \nabla_{\! z_j}\! \left[ f_i(z_{\alpha(i)}) \right] \right) \\
&amp;amp;=&amp;amp; - \lambda_j + \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} \\
&amp;amp;\Updownarrow&amp;amp; \\
\lambda_j &amp;amp;=&amp;amp; \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} \\
\end{eqnarray*}&lt;/div&gt;
&lt;p&gt;Clearly, &lt;span class="math"&gt;\(\frac{\partial f_i(z_{\alpha(i)})}{\partial z_j} = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(j \notin
\alpha(i)\)&lt;/span&gt;, which is why the &lt;span class="math"&gt;\(\beta(j)\)&lt;/span&gt; notation came in handy. By assumption,
the local derivatives, &lt;span class="math"&gt;\(\frac{\partial f_i(z_{\alpha(i)})}{\partial z_j}\)&lt;/span&gt; for &lt;span class="math"&gt;\(j
\in \alpha(i)\)&lt;/span&gt;, are easy to calculate&amp;mdash;we don't even need the chain rule to
compute them because they are simple function applications without
composition. Similar to the equations for &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;, solving this linear
system is another block-coordinate step.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Key observation&lt;/em&gt;: The last equation for &lt;span class="math"&gt;\(\lambda_j\)&lt;/span&gt; should look very familiar:
It is exactly the equation used in backpropagation! It says that we sum
&lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; of nodes that immediately depend on &lt;span class="math"&gt;\(j\)&lt;/span&gt; where we scaled each
&lt;span class="math"&gt;\(\lambda_i\)&lt;/span&gt; by the derivative of the function that directly relates &lt;span class="math"&gt;\(i\)&lt;/span&gt; and
&lt;span class="math"&gt;\(j\)&lt;/span&gt;. You should think of the scaling as a "unit conversion" from derivatives of
type &lt;span class="math"&gt;\(i\)&lt;/span&gt; to derivatives of type &lt;span class="math"&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output mutliplier&lt;/strong&gt; (&lt;span class="math"&gt;\(\lambda_n\)&lt;/span&gt;): Here we follow the same pattern as for
  intermediate multipliers.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
0 &amp;amp;=&amp;amp; \nabla_{\! z_n}\! \left[ z_n - \sum_{i=d+1}^n \lambda_i \cdot \left( z_i - f_i(z_{\alpha(i)}) \right) \right] &amp;amp;=&amp;amp; 1 - \lambda_n \\
 &amp;amp;\Updownarrow&amp;amp; \\
 \lambda_n &amp;amp;=&amp;amp; 1
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Input multipliers&lt;/strong&gt; &lt;span class="math"&gt;\((\boldsymbol{\lambda}_{1:d})\)&lt;/span&gt;: Our dummy constraints
  gives us &lt;span class="math"&gt;\(\boldsymbol{\lambda}_{1:d}\)&lt;/span&gt;, which are conveniently equal to the
  gradient of the function we're optimizing:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{\!\boldsymbol{x}} f(\boldsymbol{x}) = \boldsymbol{\lambda}_{1:d}.
$$&lt;/div&gt;
&lt;p&gt;Of course, this interpretation is only precise when ① the constraints are
satisfied (&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt; equations) and ② the linear system on multipliers is
satisfied (&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; equations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input variables&lt;/strong&gt; (&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;): Unforunately, the there is no
  closed-form solution to how to set &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. For this we resort to
  something like gradient ascent. Conveniently, &lt;span class="math"&gt;\(\nabla_{\!\boldsymbol{x}}
  f(\boldsymbol{x}) = \boldsymbol{\lambda}_{1:d}\)&lt;/span&gt;, which we can use to optimize
  &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;!&lt;/p&gt;
&lt;div id="lagrange-backprop-generalization"&gt;&lt;/div&gt;

&lt;h3&gt;Generalizations&lt;/h3&gt;
&lt;p&gt;We can think of these equations for &lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; as a simple &lt;em&gt;linear&lt;/em&gt;
system of equations, which we are solving by back-substitution when we use the
backpropagation method. The reason why back-substitution is sufficient for the
linear system (i.e., we don't need a &lt;em&gt;full&lt;/em&gt; linear system solver) is that the
dependency graph induced by the &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; relation is acyclic. If we had needed a
full linear system solver, the solution would take &lt;span class="math"&gt;\(\mathcal{O}(n^3)\)&lt;/span&gt; time
instead of linear time, seriously blowing-up our nice runtime!&lt;/p&gt;
&lt;p&gt;This connection to linear systems is interesting: It tells us that we &lt;em&gt;could&lt;/em&gt;
compute gradients in cyclic graphs. All we'd need is to run a linear system
solver to stich together our gradients! That is exactly what the
&lt;a href="https://en.wikipedia.org/wiki/Implicit_function_theorem"&gt;implicit function theorem&lt;/a&gt;
says!&lt;/p&gt;
&lt;p&gt;Cyclic constraints add some expressive powerful to "constraint language" and
it's interesting that we can still efficiently compute gradients in this
setting. An example of what a general type of cyclic constraint looks like is&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \underset{\boldsymbol{x}}{\text{argmax}}\, z_n \\
&amp;amp; \text{s.t.}\quad g(\boldsymbol{z}) = \boldsymbol{0} \\
&amp;amp; \text{and}\quad \boldsymbol{z}_{1:d} = \boldsymbol{x}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(g\)&lt;/span&gt; can be an any smooth multivariate function of the intermediate
variables! Of course, allowing cyclic constraints come at the cost of a
more-difficult analogue of "the forward pass" to satisfy the &lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;
equations (if we want to keep it a block-coordinate step). The
&lt;span class="math"&gt;\(\boldsymbol{\lambda}\)&lt;/span&gt; equations are now a linear system that requres a linear
solver (e.g., Guassian elimination).&lt;/p&gt;
&lt;p&gt;Example use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bi-level optimization: Solving an optimization problem with another one inside
  it. For example,
  &lt;a href="http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/"&gt;gradient-based hyperparameter optimization&lt;/a&gt;
  in machine learning. The implicit function theorem manages to get gradients of
  hyperparameters without needing to store any of intermediate states of the
  optimization algorithm used in the inner optimzation! This is a &lt;em&gt;huge&lt;/em&gt; memory
  saver since direct backprop on the inner gradient decent algorithm would
  require caching all intermediate states. Yikes!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cyclic constraints are useful in many graph algorithms. For example, computing
  gradients of edge weights in a general finite-state machine or, similarly,
  computing the value function in a Markov decision process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other methods for optimization?&lt;/h3&gt;
&lt;p&gt;The connection to Lagrangians brings tons of algorithms for constrained
optimization into the mix! We can imagine using more general algorithms for
optimizing our function and other ways of enforcing the constraints. We see
immediately that we could run optimization with adjoints set to values other
than those that backprop would set them to (i.e., we can optimize them like we'd
do in other algorithms for optimizing general Langrangians).&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Backprop is does not directly fall out of the the rules for differentiation
that you learned in calculus (e.g., the chain rule).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is because it operates on a more general family of functions: &lt;em&gt;programs&lt;/em&gt;
  which have &lt;em&gt;intermediate variables&lt;/em&gt;. Supporting intermediate variables is
  crucial for implementing both functions and their gradients efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I described how we could use something we did learn from calculus 101, the
method of Lagrange multipliers, to support optimization with intermediate
variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It turned out that backprop is a &lt;em&gt;particular instantiation&lt;/em&gt; of the method of
  Lagrange multipliers, involving block-coordinate steps for solving for the
  intermediates and multipliers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I also described a neat generalization to support &lt;em&gt;cyclic&lt;/em&gt; programs and I
  hinted at ideas for doing optimization a little differently, deviating from
  the de facto block-coordinate strategy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After working out the connection between backprop and the method of Lagrange
  multipliers, I discovered following paper, which beat me to it. I don't think
  my version is too redundant.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Yann LeCun. (1988)
&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf"&gt;A Theoretical Framework from Back-Propagation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The backpropagation algorithm can be cleanly generalized from values to
  functionals!&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Alexander Grubb and J. Andrew Bagnell. (2010)
&lt;a href="https://t.co/5OW5xBT4Y1"&gt;Boosted Backpropagation Learning for Training Deep Modular Networks&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A great blog post that uses the implicit function theorem to &lt;em&gt;derive&lt;/em&gt; the
  method of Lagrange multipliers. He also touches on the connection to
  backpropgation.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ben Recht. (2016)
&lt;a href="http://www.argmin.net/2016/05/31/mechanics-of-lagrangians/"&gt;Mechanics of Lagrangians&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Fri, 18 Aug 2017 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2017-08-18:blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/</guid><category>calculus</category><category>automatic-differentiation</category></item><item><title>Estimating means in a finite universe</title><link>http://timvieira.github.io/blog/post/2017/07/03/estimating-means-in-a-finite-universe/</link><description>&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;: In this post, I'm going to describe some efficient approaches
to estimating the mean of a random variable that takes on only finitely many
values. Despite the ubiquity of Monte Carlo estimation, it is really inefficient
for finite domains. I'll describe some lesser-known algorithms based on sampling
without replacement that can be adapted to estimating means.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we want to estimate an expectation of a derministic function
&lt;span class="math"&gt;\(f\)&lt;/span&gt; over a (large) finite universe of &lt;span class="math"&gt;\(n\)&lt;/span&gt; elements where each element &lt;span class="math"&gt;\(i\)&lt;/span&gt; has
probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu \overset{\tiny{\text{def}}}{=} \sum_{i=1}^n p_i f(i)
$$&lt;/div&gt;
&lt;p&gt;However, &lt;span class="math"&gt;\(f\)&lt;/span&gt; is too expensive to evaluate &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. So let's say that we have
&lt;span class="math"&gt;\(m \le n\)&lt;/span&gt; evaluations to form our estimate. (Obviously, if we're happy
evaluating &lt;span class="math"&gt;\(f\)&lt;/span&gt; a total of &lt;span class="math"&gt;\(n\)&lt;/span&gt; times, then we should just compute &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; exactly
with the definition above.)&lt;/p&gt;
&lt;!--
**Why I'm writing this post**: Monte Carlo is often used in designing algorithms
as a means to cheaply approximate intermediate expectations, think of stochastic
gradient descent as a prime example. However, in many cases, we have a *finite*
universe, i.e., we *could* enumerate all elements, but it's just inefficient to
do so. In other words, sampling is merely a choice made by the algorithm
designer, not a fundamental property of the environment, as it is typically in
statistics. What can we do to improve estimation in this special setting? I
won't get into bigger questions of how to design these algorithms, instead I'll
focus on this specific type of estimation problem.
--&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo:&lt;/strong&gt; The most well-known approach to this type of problem is Monte
Carlo (MC) estimation: sample &lt;span class="math"&gt;\(x^{(1)}, \ldots, x^{(m)}
\overset{\tiny\text{i.i.d.}}{\sim} p\)&lt;/span&gt;, return &lt;span class="math"&gt;\(\widehat{\mu}_{\text{MC}} =
\frac{1}{m} \sum_{i = 1}^m f(x^{(i)})\)&lt;/span&gt;. &lt;em&gt;Remarks&lt;/em&gt;: (1) Monte Carlo can be very
inefficient because it resamples high-probability items over and over again. (2)
We can improve efficiency&amp;mdash;measured in &lt;span class="math"&gt;\(f\)&lt;/span&gt; evaluations&amp;mdash;somewhat by
caching past evaluations of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. However, this introduces a serious &lt;em&gt;runtime&lt;/em&gt;
inefficiency and requires modifying the method to account for the fact that &lt;span class="math"&gt;\(m\)&lt;/span&gt;
is not fixed ahead of time. (3) Even in our simple setting, MC never reaches
&lt;em&gt;zero&lt;/em&gt; error; it only converges in an &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-&lt;span class="math"&gt;\(\delta\)&lt;/span&gt; sense.&lt;/p&gt;
&lt;!---
Remarks

 - We saw a similar problem where we kept sampling the same individuals over and
   over again in my
   [sqrt-biased sampling post](http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/).
--&gt;

&lt;p&gt;&lt;strong&gt;Sampling without replacement:&lt;/strong&gt; We can get around the problem of resampling
the same elements multiple times by sampling &lt;span class="math"&gt;\(m\)&lt;/span&gt; distinct elements. This is
called a sampling &lt;em&gt;without replacement&lt;/em&gt; (SWOR) scheme. Note that there is no
unique sampling without replacement scheme; although, there does seem to be a
&lt;em&gt;de facto&lt;/em&gt; method (more on that later). There are lots of ways to do sampling
without replacement, e.g., any point process over the universe will do as long
as we can control the size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An alternative formulation:&lt;/strong&gt; We can also formulate our estimation problem as
seeking a sparse, unbiased approximation to a vector &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. We want
our approximation, &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; to satisfy &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}] =
\boldsymbol{x}\)&lt;/span&gt; and while &lt;span class="math"&gt;\(|| \boldsymbol{s} ||_0 \le m\)&lt;/span&gt;. This will suffice for
estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (above) when &lt;span class="math"&gt;\(\boldsymbol{x}=\boldsymbol{p}\)&lt;/span&gt;, the vector of
probabillties, because &lt;span class="math"&gt;\(\mathbb{E}[\boldsymbol{s}^\top\! \boldsymbol{f}] =
\mathbb{E}[\boldsymbol{s}]^\top\! \boldsymbol{f} = \boldsymbol{p}^\top\!
\boldsymbol{f} = \mu\)&lt;/span&gt; where &lt;span class="math"&gt;\(\boldsymbol{f}\)&lt;/span&gt; is a vector of all &lt;span class="math"&gt;\(n\)&lt;/span&gt; values of
the function &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Obviously, we don't need to evaluate &lt;span class="math"&gt;\(f\)&lt;/span&gt; in places where
&lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; is zero so it works for our budgeted estimation task. Of
course, unbiased estimation of all probabillties is not &lt;em&gt;necessary&lt;/em&gt; for unbiased
estimation of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; alone. However, this characterization is a good model for
when we have zero knowledge of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Additionally, this formulation might be of
independent interest, since a sparse, unbiased representation of a vector might
be useful in some applications (e.g., replacing a dense vector with a sparse
vector can lead to more efficient computations).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priority sampling&lt;/strong&gt;: Priority sampling (Duffield et al., 2005;
&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., 2007&lt;/a&gt;)
is a remarkably simple algorithm, which is essentially optimal for our task, if
we assume no prior knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;. Here is pseudocode for priority sampling
(PS), based on the &lt;em&gt;alternative formulation&lt;/em&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;\textbf{procedure } \textrm{PrioritySample} \\
&amp;amp;\textbf{inputs: } \text{vector } \boldsymbol{x} \in \mathbb{R}^n, \text{budget } m \in \{1, \ldots, n\}\\
&amp;amp;\textbf{output: } \text{sparse and unbiased representation of $\boldsymbol{x}$} \\
&amp;amp;\quad u_i, \ldots, u_n \overset{\tiny\text{i.i.d.}} \sim \textrm{Uniform}(0,1] \\
&amp;amp;\quad  k_i \leftarrow u_i/x_i \text{ for each $i$} \quad\color{grey}{\text{# random sort key }} \\
&amp;amp;\quad S \leftarrow \{ \text{$m$-smallest elements according to $k_i$} \} \\
&amp;amp;\quad \tau \leftarrow (m+1)^{\text{th}}\text{ smallest }k_i \\
&amp;amp;\quad  s_i \gets \begin{cases}
  \max\left( x_i, 1/\tau \right)  &amp;amp; \text{ if } i \in S \\
  0                               &amp;amp; \text{ otherwise}
\end{cases} \\
&amp;amp;\quad \textbf{return }\boldsymbol{s}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textrm{PrioritySample}\)&lt;/span&gt; can be applied to obtain a sparse and unbiased
representation of any vector in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt;. We make use of such a
representation for our original problem of budgeted mean estimation (&lt;span class="math"&gt;\(\mu\)&lt;/span&gt;) as
follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp; \boldsymbol{s} \gets \textrm{PrioritySample}(\boldsymbol{p}, m) \\
&amp;amp; \widehat{\mu}_{\text{PS}} = \sum_{i \in S} s_i \!\cdot\! f(i)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Explanation: The definition of &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; might look a little mysterious. In the &lt;span class="math"&gt;\((i
\in S)\)&lt;/span&gt; case, it comes from &lt;span class="math"&gt;\(s_i = \frac{p_i}{p(i \in S | \tau)} =
\frac{p_i}{\min(1, x_i \cdot \tau)} = \max(x_i,\ 1/\tau)\)&lt;/span&gt;. The factor &lt;span class="math"&gt;\(p(i \in S
| \tau)\)&lt;/span&gt; is an importance-weighting correction that comes from the
&lt;a href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator"&gt;Horvitz-Thompson estimator&lt;/a&gt;
(modified slightly from its usual presentation to estimate means),
&lt;span class="math"&gt;\(\sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S]\)&lt;/span&gt;, where
&lt;span class="math"&gt;\(S\)&lt;/span&gt; is sampled according to some process with inclusion probabilities &lt;span class="math"&gt;\(q_i = p(i
\in S)\)&lt;/span&gt;. In the case of priority sampling, we have an auxiliary variable for
&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; that makes computing &lt;span class="math"&gt;\(q_i\)&lt;/span&gt; easy. Thus, for priority sampling, we can use
&lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;. This auxillary variable adds a tiny bit extra noise
in our estimator, which is tantamount to one extra sample.&lt;/p&gt;
&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#ps-unbiased');"&gt;Show proof of
unbiasedness&lt;/button&gt; &lt;div id="ps-unbiased" class="derivation"
style="display:none;"&gt; &lt;strong&gt;Proof of unbiasedness&lt;/strong&gt;. The following proof is a
little different from that in the priority sampling papers. I think it's more
straightforward. More importantly, it shows how we can extend the method to
sample from slightly different without replacement distributions
(as long as we can compute &lt;span class="math"&gt;\(q_i = p(i \in S | \tau)\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
\mathbb{E}\left[ \widehat{\mu}_{\text{PS}} \right]
&amp;amp;=&amp;amp; \mathbb{E}_{\tau, u_1, \ldots u_n}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \mathbb{E}_{u_i | \tau}\!\left[ \frac{p_i}{q_i} \cdot f(i) \cdot \boldsymbol{1}[ i \in S] \right] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot \mathbb{E}_{u_i | \tau}\!\Big[ \boldsymbol{1}[ i \in S] \Big] \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n \frac{p_i}{q_i} \cdot f(i) \cdot q_i \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \sum_{i=1}^n p_i \cdot f(i) \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\tau}\! \left[ \mu \right] \\
&amp;amp;=&amp;amp; \mu
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling satisfies our task criteria: it is both unbiased and sparse
   (i.e., under the evaluation budget).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling can be straighforwardly generalized to support streaming
   &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;, since the keys and threshold can be computed as we run, which means it
   can be stopped at any time, in principle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Priority sampling was designed for estimating subset sums, i.e., estimating
   &lt;span class="math"&gt;\(\sum_{i \in I} x_i\)&lt;/span&gt; for some &lt;span class="math"&gt;\(I \subseteq \{1,\ldots,n\}\)&lt;/span&gt;. In this setting,
   the set of sampled items &lt;span class="math"&gt;\(S\)&lt;/span&gt; is chosen to be "representative" of the
   population, albeit much smaller. In the subset sum setting, priority sampling
   has been shown to have near-optimal variance
   &lt;a href="https://www.cs.rutgers.edu/~szegedy/PUBLICATIONS/full1.pdf"&gt;(Szegedy, 2005)&lt;/a&gt;.
   Specifically, priority sampling with &lt;span class="math"&gt;\(m\)&lt;/span&gt; samples is no worse than the best
   possible &lt;span class="math"&gt;\((m-1)\)&lt;/span&gt;-sparse estimator in terms of variance. Of course, when
   estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; some knowledge about &lt;span class="math"&gt;\(f\)&lt;/span&gt;, we can obviously be used to beat
   PS. &lt;!-- We can relate subset sums to estimating &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; by interpreting
   &lt;span class="math"&gt;\(\boldsymbol{x} = \alpha\!\cdot\! \boldsymbol{p}\)&lt;/span&gt; for some &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, scaling
   &lt;span class="math"&gt;\(f\)&lt;/span&gt; appropriately by &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, and encoding the subset via indicators in
   &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s dimensions. --&gt;
   &lt;!-- (e.g.,. via
   &lt;a href="http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/"&gt;importance sampling&lt;/a&gt;
   or by modifying PS to sample proportional to &lt;span class="math"&gt;\(x_i = p_i \!\cdot\! |f_i|\)&lt;/span&gt; (as
   well as other straightforward modifications), but presumably with a surrogate
   for &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; because we don't want to evaluate it). --&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Components of the estimate &lt;span class="math"&gt;\(\boldsymbol{s}\)&lt;/span&gt; are uncorrelated, i.e.,
   &lt;span class="math"&gt;\(\textrm{Cov}[s_i, s_j] = 0\)&lt;/span&gt; for &lt;span class="math"&gt;\(i \ne j\)&lt;/span&gt; and &lt;span class="math"&gt;\(m \ge 2\)&lt;/span&gt;. This is surprising
   since &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(s_j\)&lt;/span&gt; are related via the threshold &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we instead sample &lt;span class="math"&gt;\(u_1, \ldots, u_n \overset{\text{i.i.d.}}{\sim}
   -\textrm{Exponential}(1)\)&lt;/span&gt;, then &lt;span class="math"&gt;\(S\)&lt;/span&gt; will be sampled according to the &lt;em&gt;de facto&lt;/em&gt;
   sampling without replacement scheme (e.g., &lt;code&gt;numpy.random.sample(..., replace=False)&lt;/code&gt;),
   known as probability proportional to size without replacement (PPSWOR).
   To we can then adjust our estimator
   &lt;div class="math"&gt;$$
   \widehat{\mu}_{\text{PPSWOR}} = \sum_{i \in S} \frac{p_i}{q_i} f(i)
   $$&lt;/div&gt;
   where &lt;span class="math"&gt;\(q_i = p(i \in S|\tau) = p(k_i &amp;gt; \tau) = 1-\exp(-x_i \!\cdot\!
   \tau)\)&lt;/span&gt;. This estimator performs about as well as priority sampling. It
   inherits my proof of unbiasedness (above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\tau\)&lt;/span&gt; is an auxiliary variable that is introduced to break complex
   dependencies between keys. Computing &lt;span class="math"&gt;\(\tau\)&lt;/span&gt;'s distribution is complicated
   because it is an order statistic of non-identically distributed random
   variates; this means we can't rely on symmetry to make summing over
   permutations efficient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
 - The one downside of this method is that sampling seems to require looking at
   all $n$ items.
--&gt;

&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;You can get the Jupyter notebook for replicating this experiment
&lt;a href="https://github.com/timvieira/blog/blob/master/content/notebook/Priority%20Sampling.ipynb"&gt;here&lt;/a&gt;.
So download the notebook and play with it!&lt;/p&gt;
&lt;p&gt;The improvement of priority sampling (PS) over Monte Carlo (MC) is pretty
nice. I've also included PPSWOR, which seems pretty indistinguishable from PS so
I won't really bother to discuss it. Check out the results!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Priority sampling vs. Monte Carlo" src="http://timvieira.github.io/blog/images/ps-mc.png" /&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The shaded region indicates the 10% and 90% percentiles over 20,000
replications, which gives a sense of the variability of each estimator. The
x-axis is the sampling budget, &lt;span class="math"&gt;\(m \le n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The plot shows a small example with &lt;span class="math"&gt;\(n=50\)&lt;/span&gt;. We see that PS's variability
actually goes to zero, unlike Monte Carlo, which is still pretty inaccurate even
at &lt;span class="math"&gt;\(m=n\)&lt;/span&gt;. (Note that MC's x-axis measures raw evaluations, not distinct ones.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt; If you liked this post, you might like my other posts
tagged with &lt;a href="http://timvieira.github.io/blog/tag/sampling.html"&gt;sampling&lt;/a&gt; and
&lt;a href="http://timvieira.github.io/blog/tag/reservoir-sampling.html"&gt;reservoir sampling&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Edith Cohen, "The Magic of Random Sampling"
   (&lt;a href="http://www.cohenwang.com/edith/Talks/MagicSampling201611.pdf"&gt;slides&lt;/a&gt;,
   &lt;a href="https://www.youtube.com/watch?v=jp83HyDs8fs"&gt;talk&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nickduffield.net/download/papers/priority.pdf"&gt;Duffield et al., (2007)&lt;/a&gt;
   has plenty good stuff that I didn't cover.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alex Smola's &lt;a href="http://blog.smola.org/post/1078486350/priority-sampling"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suresh Venkatasubramanian's
   &lt;a href="http://blog.geomblog.org/2005/10/priority-sampling.html"&gt;post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 03 Jul 2017 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2017-07-03:blog/post/2017/07/03/estimating-means-in-a-finite-universe/</guid><category>sampling</category><category>statistics</category><category>reservoir-sampling</category></item><item><title>How to test gradient implementations</title><link>http://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/</link><description>&lt;!--
**Who should read this?** Nowadays, you're probably just using automatic
differentiation to compute the gradient of whatever function you're using. If
that's you and you trust your software package wholeheartedly, then you probably
don't need to read this. If you're rolling your own module/Op to put in to an
auto-diff library, then you should read this. I find that none of the available
libraries are any good for differentiating dynamic programs, so I still find
this stuff useful. You'll probably have to write gradient code without the aid
of an autodiff library someday... So either way, knowing this stuff is good for
you. To answer the question, everyone.
--&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we have a function, &lt;span class="math"&gt;\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)&lt;/span&gt;,
and we want to test code that computes &lt;span class="math"&gt;\(\nabla f\)&lt;/span&gt;. (Note that these techniques
also apply when &lt;span class="math"&gt;\(f\)&lt;/span&gt; has multivariate output.)&lt;/p&gt;
&lt;h2&gt;Finite-difference approximation&lt;/h2&gt;
&lt;p&gt;The main way that people test gradient computation is by comparing it against a
finite-difference (FD) approximation to the gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{d}^\top\! \nabla f(\boldsymbol{x}) \approx \frac{1}{2 \varepsilon}(f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d}) - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d}))
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;
where &lt;span class="math"&gt;\(\boldsymbol{d} \in \mathbb{R}^n\)&lt;/span&gt; is an arbitrary "direction" in parameter
space. We will look at many directions when we test. Generally, people take the
&lt;span class="math"&gt;\(n\)&lt;/span&gt; elementary vectors as the directions, but random directions are just as good
(and you can catch bugs in all dimensions with less than &lt;span class="math"&gt;\(n\)&lt;/span&gt; of them).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Always use the two-sided difference formula&lt;/strong&gt;. There is a version which
doesn't add &lt;em&gt;and&lt;/em&gt; subtract, just does one or the other. Do not use it ever.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Make sure you test multiple inputs&lt;/strong&gt; (values of &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;) or any thing
else the function depends on (e.g., the minibatch).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What directions to use&lt;/strong&gt;: When debugging, I tend to use elementary directions
because they tell me something about which dimensions that are wrong... this
doesn't always help though. The random directions are best when you want the
test cases to run really quickly. In that case, you can switch to check a few
random directions using a
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/util.py"&gt;spherical&lt;/a&gt;
distribution&amp;mdash;do &lt;em&gt;not&lt;/em&gt; sample them from a multivariate uniform!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Always test your implementation of &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/strong&gt; It's very easy to &lt;em&gt;correctly&lt;/em&gt;
  compute the gradient of the &lt;em&gt;wrong&lt;/em&gt; function. The FD approximation is a
  "self-consistency" test, it does not validate &lt;span class="math"&gt;\(f\)&lt;/span&gt; only the relationship
  between &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(\nabla\! f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Obviously, how you test &lt;span class="math"&gt;\(f\)&lt;/span&gt; depends strongly on what it's supposed to compute.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: For a conditional random field (CRF), you can also test that your
   implementation of a dynamic program for computing &lt;span class="math"&gt;\(\log Z_\theta(x)\)&lt;/span&gt; is
   correctly by comparing against brute-force enumeration of &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; on
   small examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, you can directly test the gradient code if you know a different way
to compute it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example: In a CRF, we know that the &lt;span class="math"&gt;\(\nabla \log Z_\theta(x)\)&lt;/span&gt; is a feature
   expectation, which you can also test against a brute-force enumeration on
   small examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Why not just use the FD approximation as your gradient?&lt;/h3&gt;
&lt;p&gt;For low-dimensional functions, you can straight-up use the finite-difference
approximation instead of rolling code to compute the gradient. (Take &lt;span class="math"&gt;\(n\)&lt;/span&gt;
axis-aligned unit vectors for &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;.) The FD approximation is very
accurate. Of course, specialized code is probably a little more accurate, but
that's not &lt;em&gt;really&lt;/em&gt; why we bother to do it! The reason why we write specialized
gradient code is &lt;em&gt;not&lt;/em&gt; to improve numerical accuracy, it's to improve
&lt;em&gt;efficiency&lt;/em&gt;. As I've
&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;ranted&lt;/a&gt;
before, automatic differentiation techniques guarantee that evaluating &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; gradient should be as efficient as computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; (with the caveat that
&lt;em&gt;space&lt;/em&gt; complexity may increase substantially - i.e., space-time tradeoffs
exists). FD is &lt;span class="math"&gt;\(\mathcal{O}(n \cdot \textrm{runtime } f(x))\)&lt;/span&gt;, where as autodiff
is &lt;span class="math"&gt;\(\mathcal{O}(\textrm{runtime } f(x))\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;How to compare vectors&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Absolute difference is the devil.&lt;/strong&gt; You should never compare vectors in
absolute difference (this is Lecture 1 of any numerical methods course). In this
case, the problem is that gradients depend strongly on the scale of &lt;span class="math"&gt;\(f\)&lt;/span&gt;. If &lt;span class="math"&gt;\(f\)&lt;/span&gt;
takes tiny values then it's easy for differences to be lower than a tiny
threshold.&lt;/p&gt;
&lt;p&gt;Most people use &lt;strong&gt;relative error&lt;/strong&gt; &lt;span class="math"&gt;\(= \frac{|\textbf{want} -
\textbf{got}|}{|\textbf{want}|}\)&lt;/span&gt;, to get a scale-free error measure, but
unfortunately relative error chokes when &lt;span class="math"&gt;\(\textbf{want}\)&lt;/span&gt; is zero.&lt;/p&gt;
&lt;p&gt;I compute several error measures with a script that you can import from my
github
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/checkgrad.py"&gt;arsenal.math.checkgrad.{fdcheck}&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I use two metrics to test gradients:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Relative error (skipping zeros): If relative error hits a zero, I skip
   it. I'll rely on the other measure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pearson correlation: Checks the &lt;em&gt;direction&lt;/em&gt; of the gradient, but allows a
   scale and shift transformation. This measure doesn't have trouble with zeros,
   but allows scale and shift problems to pass by. &lt;em&gt;Make sure you fix those
   errors!&lt;/em&gt; (e.g. In the CRF example, you might have forgotten to divide by
   &lt;span class="math"&gt;\(Z(x)\)&lt;/span&gt;, which not really a constant... I've made this exact mistake a few
   times.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I also look at some diagnostics, which help me debug stuff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Accuracy of predicting the sign {+,-,0} of each dimension (or dot random product).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Absolute error (just as a diagnostic)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scatter plot: When debugging, I like to scatter plot the elements of FD vs. my
  implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these measurements (and the scatter plot) can be computed with
&lt;a href="https://github.com/timvieira/arsenal/blob/master/arsenal/math/compare.py"&gt;arsenal.math.compare.{compare}&lt;/a&gt;,
which I find super useful when debugging absolutely anything numerical.&lt;/p&gt;
&lt;h2&gt;Bonus tests&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Testing modules&lt;/strong&gt;: You can test the different modules of your code as well
(assuming you have a composable module-based setup). E.g., I test my DP
algorithm independent of how the features and downstream loss are computed. You
can also test feature and downstream loss modules independent of one
another. Note that autodiff (implicitly) computes Jacobian-vector products
because modules are multivariate in general. We can reduce to the scalar case by
taking a dot product of the outputs with a (fixed) random vector.&lt;/p&gt;
&lt;p&gt;Something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spherical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# fixed random vector |output|=|m|&lt;/span&gt;
&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# scalar function for use in fd&lt;/span&gt;

&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# propagate&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adjoint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="c1"&gt;# set output adjoint to r, usually we set adjoint of scalar output=1&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bprop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adjoint&lt;/span&gt; &lt;span class="c1"&gt;# grab the gradient&lt;/span&gt;
&lt;span class="n"&gt;fd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fdgrad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ad&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Integration tests&lt;/strong&gt;: Test that running a gradient-based optimization algorithm
is successful with your gradient implementation. Use smaller versions of your
problem if possible. A related test for machine learning applications is to make
sure that your model and learning procedure can (over)fit small datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test that batch = minibatch&lt;/strong&gt; (if applicable). It's very easy to get this bit
wrong. Broadcasting rules (in numpy, for example) make it easy to hide matrix
conformability mishaps. So make sure you get the same results as manual
minibatching (Of course, you should only do minibatching if are get a speed-up
from vectorization or parallelism. You should probably test that it's actually
faster.)&lt;/p&gt;
&lt;!--
Other common sources of bugs

* Really look over your test cases. I often find that my errors are actually in
  the test case themselves because either (1) I wrote it really quickly with
  less care than the difficult function/gradient, or (2) there is a gap between
  "what I want it to do" and "what I told it to do".

* Random search in the space of programs can result in overfitting! This is a
  general problem with test-driven development that always applies. If you are
  hamfistedly twiddling bits of your code without thinking about why things
  work, you can trick almost any test.
--&gt;

&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I've written about gradient approximations before, you might like these
  articles:
  &lt;a href="http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/"&gt;Gradient-vector products&lt;/a&gt;,
  &lt;a href="http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/"&gt;Complex-step method&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The foundations of backprop:
  &lt;a href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/"&gt;Backprop is not just the chain rule&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/"&gt;I strongly recommend&lt;/a&gt;
  learning how automatic differentiation works, I learned it from
  &lt;a href="https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"&gt;Justin Domke's course notes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2017/04/22/you-deserve-better-than-two-sided-finite-differences/"&gt;Justin Domke post&lt;/a&gt;:
  Explains why we need bespoke finite-difference stencils (i.e., more than
  two-sided differences) to prevent numerical demons from destroying our results!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Fri, 21 Apr 2017 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2017-04-21:blog/post/2017/04/21/how-to-test-gradient-implementations/</guid><category>testing</category><category>calculus</category></item><item><title>Counterfactual reasoning and learning from logged data</title><link>http://timvieira.github.io/blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/</link><description>&lt;style&gt; .toggle-button { background-color: #555555; border: none; color: white;
padding: 10px 15px; border-radius: 6px; text-align: center; text-decoration:
none; display: inline-block; font-size: 16px; cursor: pointer; } .derivation {
background-color: #f2f2f2; border: thin solid #ddd; padding: 10px;
margin-bottom: 10px; } &lt;/style&gt;

&lt;script&gt;
/* workaround for when markdown/mathjax gets confused by the javascript dollar function. */
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;Counterfactual reasoning is &lt;em&gt;reasoning about data that we did not observe&lt;/em&gt;. For
example, reasoning about the expected reward of new policy given data collected
from a older one.&lt;/p&gt;
&lt;p&gt;In this post, I'll discuss some basic techniques for learning from logged
data. For the large part, this post is based on things I learned from
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Peshkin &amp;amp; Shelton (2002)&lt;/a&gt;
and &lt;a href="https://arxiv.org/abs/1209.2355"&gt;Bottou et al. (2013)&lt;/a&gt; (two of all-time
favorite papers).&lt;/p&gt;
&lt;p&gt;After reading, have a look at the
&lt;a href="https://gist.github.com/timvieira/788c2c25c94663c49abada60f2e107e9"&gt;Jupyter notebook&lt;/a&gt;
accompanying this post!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; (&lt;em&gt;off-line off-policy optimization&lt;/em&gt;): We're trying to optimize a
function of the form,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \underset{p_\theta}{\mathbb{E}} \left[ r(x) \right] = \sum_{x \in \mathcal{X}} p_\theta(x) r(x).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; But! We only have a &lt;em&gt;fixed&lt;/em&gt; sample of size &lt;span class="math"&gt;\(m\)&lt;/span&gt; from a data collection
policy &lt;span class="math"&gt;\(q\)&lt;/span&gt;, &lt;span class="math"&gt;\(\{ (r^{(j)}, x^{(j)} ) \}_{j=1}^m \overset{\text{i.i.d.}} \sim q.\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Although, it's not &lt;em&gt;necessarily&lt;/em&gt; the case, you can think of &lt;span class="math"&gt;\(q = p_{\theta'}\)&lt;/span&gt;
  for a &lt;em&gt;fixed&lt;/em&gt; value &lt;span class="math"&gt;\(\theta'.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; is an arbitrary multivariate space, which permits a mix of
  continuous and discrete components, with appropriate densities, &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt;
  and &lt;span class="math"&gt;\(q\)&lt;/span&gt; defined over it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(r: \mathcal{X} \mapsto \mathbb{R}\)&lt;/span&gt; is a black box that outputs a scalar
  score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I've used the notation &lt;span class="math"&gt;\(r^{(j)}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(r(x^{(j)})\)&lt;/span&gt; to emphasize that we
  can't evaluate &lt;span class="math"&gt;\(r\)&lt;/span&gt; at &lt;span class="math"&gt;\(x\)&lt;/span&gt; values other than those in the sample.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We'll assume that &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns positive probability everywhere, &lt;span class="math"&gt;\(q(x) &amp;gt; 0\)&lt;/span&gt; for
  all &lt;span class="math"&gt;\(x \in \mathcal{X}\)&lt;/span&gt;. This means is that the data collection process must
  be randomized and eventually sample all possible configurations. Later, I
  discuss relaxing this assumption.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each distribution is a product of one or more factors of the following types:
&lt;strong&gt;policy factors&lt;/strong&gt; (at least one), which directly depend on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, and
&lt;strong&gt;environment factors&lt;/strong&gt; (possibly none), which do not depend directly on
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Note that environment factors are &lt;em&gt;only&lt;/em&gt; accessible via sampling
(i.e., we don't know the &lt;em&gt;value&lt;/em&gt; they assign to a sample). For example, a
&lt;em&gt;contextual bandit problem&lt;/em&gt;, where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a context-action pair, &lt;span class="math"&gt;\(x =
(s,a)\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(q(x) = q(a|s) p(s)\)&lt;/span&gt; and &lt;span class="math"&gt;\(p_{\theta}(x) = p_{\theta}(a|s)
p(s)\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(p_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share the environment factor &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;, the
distribution over contexts, and only differ in the action-given-context
factor. For now, assume that we can evaluate all environment factors; later,
I'll discuss how we cleverly work around it.&lt;/p&gt;
&lt;!--
(We can
even extend it to a full-blown MDP or POMDP by taking $x$ to be a sequence of
state-action pairs, often called "trajectories".)
--&gt;

&lt;p&gt;&lt;strong&gt;The main challenge&lt;/strong&gt; of this setting is that we don't have controlled
experiments to learn from because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is not (completely) in our control. This
manifests itself as high variance ("noise") in estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;. Consider
the contextual bandit setting, we receive a context &lt;span class="math"&gt;\(s\)&lt;/span&gt; and execute single
action; we never get to rollback to that precise context and try an alternative
action (to get a paired sample #yolo) because we do not control &lt;span class="math"&gt;\(p(s)\)&lt;/span&gt;. This is
an important paradigm for many 'real world' problems, e.g., predicting medical
treatments or ad selection.&lt;/p&gt;
&lt;!--
This is the crucial difference that makes counterfactual learning
more difficult than (fully) supervised learning.
--&gt;

&lt;p&gt;&lt;strong&gt;Estimating &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;&lt;/strong&gt; [V1]: We obtain an unbiased estimator of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;
with
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta)
\approx \hat{J}_{\!\text{IS}}(\theta)
= \frac{1}{m} \sum_{j=1}^m r^{(j)} \!\cdot\! w^{(j)}_{\theta}
\quad \text{ where } w^{(j)}_{\theta} = \frac{p_{\theta}(x^{(j)}) }{ q(x^{(j)}) }.
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; This estimator is remarkable: it uses importance sampling as a function
approximator! We have an &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt; for any value of
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that we like. &lt;em&gt;The catch&lt;/em&gt; is that we have to pick &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; a priori,
i.e., with no knowledge of the sample.&lt;/p&gt;
&lt;!--
We also require that the usual 'support
conditions' for importance sampling conditions ($p_{\theta}(x)&gt;0 \Rightarrow
q(x)&gt;0$ for all $x \in \mathcal{X}$, which is why we made assumption A1.
--&gt;

&lt;p&gt;After we've collected a (large) sample it's possible to optimize
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; using any optimization algorithm (e.g., L-BFGS). Of
course, we risk overfitting to the sample if we evaluate
&lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt;. Actually, it's a bit worse: this objective tends to
favor regions of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, which are not well represented in the sample because
the importance sampling estimator has high variance in these regions resulting
from large importance weights (when &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; is small and &lt;span class="math"&gt;\(p_{\theta}(x)\)&lt;/span&gt; is
large, &lt;span class="math"&gt;\(w(x)\)&lt;/span&gt; is large and consequently so is &lt;span class="math"&gt;\(\hat{J}_{\!\text{IS}}\)&lt;/span&gt; regardless
of whether &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; is high!). Thus, we want some type of "regularization" to keep
the optimizer in regions which are sufficiently well-represented by the sample.&lt;/p&gt;
&lt;!--
**Visual example**: We can visualize this phenomena in a simple example. Let $q
= \mathcal{N}(0, \sigma=5)$, $r(x) = 1 \text{ if } x \in [2, 3], 0.2 \text{
otherwise},$ and $p_\theta = \mathcal{N}(\theta, \sigma=1)$.  This example is
nice because it let's us plot $x$ and $\theta$ in the same space. This is
generally not the case, because $\mathcal{X}$ may have no connection to $\theta$
space, e.g., $\mathcal{X}$ may be discrete.

**TODO** add plot
--&gt;

&lt;p&gt;&lt;strong&gt;Better surrogate&lt;/strong&gt; [V2]: There are many ways to improve the variance of the
estimator and &lt;em&gt;confidently&lt;/em&gt; obtain improvements to the system. One of my
favorites is Bottou et al.'s lower bound on &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;, which we get by
clipping importance weights, replace &lt;span class="math"&gt;\(w^{(j)}_{\theta}\)&lt;/span&gt; with &lt;span class="math"&gt;\(\min(R,
w^{(j)}_{\theta})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Confidence intervals&lt;/strong&gt; [V3]: We can augment the V2 lower bound with confidence
intervals derived from the empirical Bernstein bound (EBB). We'll require that
&lt;span class="math"&gt;\(r\)&lt;/span&gt; is bounded and that we know its max/min values. The EBB &lt;em&gt;penalizes&lt;/em&gt;
hypotheses (values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;) which have higher sample variance. (Note: a
Hoeffding bound wouldn't change the &lt;em&gt;shape&lt;/em&gt; of the objective, but EBB does
thanks to the sample variance penalty. EBB tends to be tighter.). The EBB
introduces an additional "confidence" hyperparameter, &lt;span class="math"&gt;\((1-\delta)\)&lt;/span&gt;. Bottou et
al. recommend maximizing the lower bound as it provides safer improvements. See
the original paper for the derivation.&lt;/p&gt;
&lt;!--
An important benefit of having upper *and* lower is that the bounds tell
us whether or not we should collect more data
--&gt;

&lt;p&gt;Both V2 and V3 are &lt;em&gt;biased&lt;/em&gt; (as they are lower bounds), but we can mitigate the
bias by &lt;em&gt;tuning&lt;/em&gt; the hyperparameter &lt;span class="math"&gt;\(R\)&lt;/span&gt; on a heldout sample (we can even tune
&lt;span class="math"&gt;\(\delta\)&lt;/span&gt;, if desired). Additionally, V2 and V3 are 'valid' when &lt;span class="math"&gt;\(q\)&lt;/span&gt; has limited
support since they prevent the importance weights from exploding (of course, the
bias can be arbitrarily bad, but probably unavoidable given the
learning-from-only-logged data setup).&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;!--
**Be warned**: This may be considered an idealized setting. Much of the research
in counterfactual and causal reasoning targets (often subtle) deviations from
these assumptions (and some different questions, of course). Some extensions and
discussion appear towards the end of the post.
--&gt;

&lt;p&gt;&lt;strong&gt;Unknown environment factors&lt;/strong&gt;: Consider the contextual bandit setting
(mentioned above). Here &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt; share an &lt;em&gt;unknown&lt;/em&gt; environment factor: the
distribution of contexts. Luckily, we do not need to know the value of this
factor in order to apply any of our estimators because they are all based on
likelihood &lt;em&gt;ratios&lt;/em&gt;, thus the shared unknown factors cancel out!  Some specific
examples are given below. Of course, these factors do influence the estimators
because they are crucial in &lt;em&gt;sampling&lt;/em&gt;, they just aren't necessary in
&lt;em&gt;evaluation&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In contextual bandit example, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a state-action pair, &lt;span class="math"&gt;\(w_{\theta}(x) =
    \frac{p_{\theta}(x)}{q(x)} = \frac{ p_{\theta}(s,a) }{ q(s,a) } =
    \frac{p_{\theta}(a|s) p(s)}{q(a|s) p(s)} = \frac{p_{\theta}(a|s) }{ q(a|s)
    }\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In a Markov decision process, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a sequence of state-action pairs,
    &lt;span class="math"&gt;\(w_{\theta}(x) = \frac{p_{\theta}(x)}{q(x)} = \frac{ p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) p_\theta(a_t|s_t) } { p(s_0) \prod_{t=0}^T
    p(s_{t+1}|s_t,a_t) q(a_t|s_t) } = \frac{\prod_{t=0}^T \pi_\theta(a_t|s_t)}
    {\prod_{t=0}^T q(a_t|s_t)}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: These estimators can all be improved with variance
reduction techniques. Probably the most effective technique is using
&lt;a href="https://en.wikipedia.org/wiki/Control_variates"&gt;control variates&lt;/a&gt; (of which
baseline functions are a special case). These are random variables correlated
with &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; for which we know their expectations (or at least they are estimated
separately). A great example is how ad clicks depend strongly on time-of-day
(fewer people are online late at night so we get fewer clicks), thus the
time-of-day covariate explains a large part of the variation in &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimation instead of optimization&lt;/strong&gt;: You can use this general setup for
estimation instead of optimization, in which case it's fine to let &lt;span class="math"&gt;\(r\)&lt;/span&gt; have
real-valued multivariate output. The confidence intervals are probably useful in
that setting too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: Often &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an existing complex system, which does not record
its probabilities. It is possible to use regression to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; from the
samples, which is called the &lt;strong&gt;propensity score&lt;/strong&gt; (PS). PS attempts to account
for &lt;strong&gt;confounding variables&lt;/strong&gt;, which are hidden causes that control variation in
the data. Failing to account for confounding variables may lead to
&lt;a href="https://en.wikipedia.org/wiki/Simpson's_paradox"&gt;incorrect conclusions&lt;/a&gt;. Unfortunately,
PS results in a biased estimator because we're using a 'ratio of expectations'
(we'll divide by the PS estimate) instead of an 'expectation of ratios'. PS is
only statistically consistent in the (unlikely) event that the density estimate
is correctly specified (i.e., we can eventually get &lt;span class="math"&gt;\(q\)&lt;/span&gt; correct). In the unknown
&lt;span class="math"&gt;\(q\)&lt;/span&gt; setting, it's often better to use the &lt;strong&gt;doubly-robust estimator&lt;/strong&gt; (DR) which
combines &lt;em&gt;two&lt;/em&gt; estimators: a density estimator for &lt;span class="math"&gt;\(q\)&lt;/span&gt; and a function
approximation for &lt;span class="math"&gt;\(r\)&lt;/span&gt;. A great explanation for the bandit case is in
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Dudík et al. (2011)&lt;/a&gt;. The DR estimator is also
biased, but it has a better bias-variance tradeoff than PS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; doesn't have support everywhere?&lt;/strong&gt; This is an especially important
setting because it is often the case that data collection policies abide by some
&lt;strong&gt;safety regulations&lt;/strong&gt;, which prevent known bad configurations. In many
situations, evaluating &lt;span class="math"&gt;\(r(x)\)&lt;/span&gt; corresponds to executing an action &lt;span class="math"&gt;\(x\)&lt;/span&gt; in the real
world so terrible outcomes could occur, such as, breaking a system, giving a
patient a bad treatment, or losing money. V1 is ok to use as long as we satisfy
the importance sampling support conditions, which might mean rejecting certain
values for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; (might be non-trivial to enforce) and consequently finding a
less-optimal policy. V2 and V3 are ok to use without an explicit constraint, but
additional care may be needed to ensure specific safety constraints are
satisfied by the learned policy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if &lt;span class="math"&gt;\(q\)&lt;/span&gt; is deterministic?&lt;/strong&gt; This is related to the point above. This is a
hard problem. Essentially, this trying to learn without any exploration /
experimentation! In general, we need exploration to learn. Randomization isn't
the only way to perform exploration, there are many systematic types of
experimentation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are some cases of systematic experimentation that are ok. For example,
    enumerating all elements of &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt; (almost certainly
    infeasible). Another example is a contextual bandit where &lt;span class="math"&gt;\(q\)&lt;/span&gt; assigns
    actions to contexts deterministically via a hash function (this setting is
    fine because &lt;span class="math"&gt;\(q\)&lt;/span&gt; is essentially a uniform distribution over actions, which
    is independent of the state). In other special cases, we &lt;em&gt;may&lt;/em&gt; be able to
    characterize systematic exploration as
    &lt;a href="https://en.wikipedia.org/wiki/Stratified_sampling"&gt;stratified sampling&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A generic solution might be to apply the doubly-robust estimator, which
    "smooths out" deterministic components (by pretending they are random) and
    accounting for confounds (by explicitly modeling them in the propensity
    score).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What if we control data collection (&lt;span class="math"&gt;\(q\)&lt;/span&gt;)?&lt;/strong&gt; This is an interesting
setting. Essentially, it asks "how do we explore/experiment optimally (and
safely)?". In general, this is an open question and depends on many
considerations, such as, how much control, exploration cost (safety constraints)
and prior knowledge (of &lt;span class="math"&gt;\(r\)&lt;/span&gt; and unknown factors in the environment). I've seen
some papers cleverly design &lt;span class="math"&gt;\(q\)&lt;/span&gt;. The first that comes to mind is
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Levine &amp;amp; Koltun (2013)&lt;/a&gt;. Another
setting is &lt;em&gt;online&lt;/em&gt; contextual bandits, in which algorithms like
&lt;a href="http://jmlr.org/proceedings/papers/v15/beygelzimer11a/beygelzimer11a.pdf"&gt;EXP4&lt;/a&gt;
and
&lt;a href="http://www.research.rutgers.edu/~lihong/pub/Chapelle12Empirical.pdf"&gt;Thompson sampling&lt;/a&gt;,
prescribe certain types of exploration and work interactively (i.e., they don't
have a fixed training sample). Lastly, I'll mention that there are many
techniques for variance reduction by importance sampling, which may apply.&lt;/p&gt;
&lt;h2&gt;Further reading&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max
Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson.
&lt;a href="https://arxiv.org/abs/1209.2355"&gt;Counterfactual reasoning in learning systems&lt;/a&gt;.
JMLR 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The source for the majority of this post. It includes many other interesting
ideas and goes more in depth into some of the details.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Miroslav Dudík, John Langford, Lihong Li.
&lt;a href="https://arxiv.org/abs/1103.4601"&gt;Doubly robust policy evaluation and learning&lt;/a&gt;.
ICML 2011.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discussed in extensions section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Philip S. Thomas.
&lt;a href="http://psthomas.com/papers/Thomas2015c.pdf"&gt;Safe reinforcement learning&lt;/a&gt;.
PhD Thesis 2015.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Covers confidence intervals for policy evaluation similar to Bottou et al., as
well as learning algorithms for RL with safety guarantees (e.g., so we don't
break the robot).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Peshkin and Shelton.
&lt;a href="http://www.cs.ucr.edu/~cshelton/papers/docs/icml02.pdf"&gt;Learning from scarce experience&lt;/a&gt;.
ICML 2002.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An older RL paper, which covers learning from logged data. This is one of the
earliest papers on learning from logged data that I could find.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Levine and Koltun.
&lt;a href="https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf"&gt;Guided policy search&lt;/a&gt;.
ICML 2013.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses clever choices for &lt;span class="math"&gt;\(q\)&lt;/span&gt; to better-guide learning in the RL setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Corinna Cortes, Yishay Mansour, Mehryar Mohri.
&lt;a href="https://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;Learning bounds for importance weighting&lt;/a&gt;.
NIPS 2010.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Discusses &lt;em&gt;generalization bounds&lt;/em&gt; for the counterfactual objective. Includes an
alternative weighting scheme to keep importance weights from exploding.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 19 Dec 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-12-19:blog/post/2016/12/19/counterfactual-reasoning-and-learning-from-logged-data/</guid><category>counterfactual-reasoning</category><category>importance-sampling</category><category>machine-learning</category></item><item><title>Heaps for incremental computation</title><link>http://timvieira.github.io/blog/post/2016/11/21/heaps-for-incremental-computation/</link><description>&lt;p&gt;In this post, I'll describe a neat trick for maintaining a summary quantity
(e.g., sum, product, max, log-sum-exp, concatenation, cross-product) under
changes to its inputs. The trick and it's implementation are inspired by the
well-known max-heap datastructure. I'll also describe a really elegant
application to fast sampling under an evolving categorical distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Suppose we'd like to efficiently compute a summary quantity under
changes to its &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional input vector &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;. The particular
form of the quantity we're going to compute is &lt;span class="math"&gt;\(z = \bigoplus_{i=1}^n w_i\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; is some associative binary operator with identity element
&lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button class="toggle-button" onclick="toggle('#operator-mathy');"&gt;more formally...&lt;/button&gt;
&lt;div id="operator-mathy" class="derivation" style="display:none"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\boldsymbol{w} \in \boldsymbol{K}^n\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\oplus: \boldsymbol{K} \times \boldsymbol{K} \mapsto \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associative: &lt;span class="math"&gt;\((a \oplus b) \oplus c = a \oplus (b \oplus c)\)&lt;/span&gt; for all &lt;span class="math"&gt;\(a,b,c
  \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identity element: &lt;span class="math"&gt;\(\boldsymbol{0} \in \boldsymbol{K}\)&lt;/span&gt; such that &lt;span class="math"&gt;\(k \oplus
  \boldsymbol{0} = \boldsymbol{0} \oplus k = k\)&lt;/span&gt;, for all &lt;span class="math"&gt;\(k \in \boldsymbol{K}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;The trick&lt;/strong&gt;: Essentially, the trick boils down to &lt;em&gt;parenthesis placement&lt;/em&gt; in
the expression which computes &lt;span class="math"&gt;\(z\)&lt;/span&gt;. A freedom we assumed via the associative
property.&lt;/p&gt;
&lt;p&gt;I'll demonstrate by example with &lt;span class="math"&gt;\(n=8\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Linear structure: We generally compute something like &lt;span class="math"&gt;\(z\)&lt;/span&gt; with a simple
loop. This looks like a right-branching binary tree when we think about the
order of operations,&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((((((w_1 \oplus w_2) \oplus w_3) \oplus w_4) \oplus w_5) \oplus w_6) \oplus w_7) \oplus w_8).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt; Heap structure: Here the parentheses form a balanced tree, which looks
much more like a recursive implementation that computes the left and right
halves and &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;s the results (divide-and-conquer style),&lt;/p&gt;
&lt;div class="math"&gt;$$
z = (((w_1 \oplus w_2) \oplus (w_3 \oplus w_4)) \oplus ((w_5 \oplus w_6) \oplus (w_7 \oplus w_8))).
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;
The benefit of the heap structure is that there are &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;
intermediate quantities that depend on any input, whereas the linear structure
has &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt;. The intermediate quantities correspond to the values of each of the
parenthesized expressions.&lt;/p&gt;
&lt;p&gt;Since fewer intermediate quantities depend on a given input, fewer intermediates
need to be adjusted upon a change to the input. Therefore, we get faster
algorithms for &lt;em&gt;maintaining&lt;/em&gt; the output quantity &lt;span class="math"&gt;\(z\)&lt;/span&gt; as the inputs change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Heap datastructure&lt;/strong&gt; (aka
&lt;a href="https://en.wikipedia.org/wiki/Fenwick_tree"&gt;binary index tree or Fenwick tree&lt;/a&gt;):
We're going to store the values of the intermediates quantities and inputs in a
heap datastructure, which is a &lt;em&gt;complete&lt;/em&gt; binary tree. In our case, the tree has
depth &lt;span class="math"&gt;\(1 + \lceil \log_2 n \rceil\)&lt;/span&gt;, with the values of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at it's
leaves (aligned left) and padding with &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt; for remaining
leaves. Thus, the array's length is &lt;span class="math"&gt;\(&amp;lt; 4 n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This structure makes our implementation really nice and efficient because we
don't need pointers to find the parent or children of a node (i.e., no need to
wrap elements into a "node" class like in a general tree data structure). So, we
can pack everything into an array, which means our implementation has great
memory/cache locality and low storage overhead.&lt;/p&gt;
&lt;p&gt;Traversing the tree is pretty simple: Let &lt;span class="math"&gt;\(d\)&lt;/span&gt; be the number of internal nodes,
nodes &lt;span class="math"&gt;\(1 \le i \le d\)&lt;/span&gt; are interal. For node &lt;span class="math"&gt;\(i\)&lt;/span&gt;, left child &lt;span class="math"&gt;\(\rightarrow {2
\cdot i},\)&lt;/span&gt; right child &lt;span class="math"&gt;\(\rightarrow {2 \cdot i + 1},\)&lt;/span&gt; parent &lt;span class="math"&gt;\(\rightarrow
\lfloor i / 2 \rfloor.\)&lt;/span&gt; (Note that these operations assume the array's indices
start at &lt;span class="math"&gt;\(1\)&lt;/span&gt;. We generally fake this by adding a dummy node at position &lt;span class="math"&gt;\(0\)&lt;/span&gt;,
which makes implementation simpler.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initializing the heap&lt;/strong&gt;: Here's code that initializes the heap structure we
  just described.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sumheap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Create sumheap from weights `w` in O(n) time.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  &lt;span class="c1"&gt;# number of intermediates&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                &lt;span class="c1"&gt;# intermediates + leaves&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;                     &lt;span class="c1"&gt;# store `w` at leaves.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Updating &lt;span class="math"&gt;\(w_k\)&lt;/span&gt;&lt;/strong&gt; boils down to fixing intermediate sums that (transitively)
  depend on &lt;span class="math"&gt;\(w_k.\)&lt;/span&gt; I won't go into all of the details here, instead I'll give
  code (below). I'd like to quickly point out that the term "parents" is not
  great for our purposes because they are actually the &lt;em&gt;dependents&lt;/em&gt;: when an
  input changes the value the parents, grand parents, great grand parents, etc,
  become stale and need to be recomputed bottom up (from the leaves). The code
  below implements the update method for changing the value of &lt;span class="math"&gt;\(w_k\)&lt;/span&gt; and runs in
  &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Update w[k] = v` in time O(log n).&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# fix parents in the tree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;//=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical stability&lt;/strong&gt;: If the operations are noisy (e.g., floating point
   operator), then the heap version may be better behaved. For example, if
   operations have an independent, additive noise rate &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; then noise
   of &lt;span class="math"&gt;\(z_{\text{heap}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot \log n)\)&lt;/span&gt;, whereas
   &lt;span class="math"&gt;\(z_{\text{linear}}\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathcal{O}(\varepsilon \cdot n)\)&lt;/span&gt;. (Without further
   assumptions about the underlying operator, I don't believe you can do better
   than that.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relationship to max-heap&lt;/strong&gt;: In the case of a max or min heap, we can avoid
   allocating extra space for intermediate quantities because all intermediates
   values are equal to exactly one element of &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change propagation&lt;/strong&gt;: The general idea of &lt;em&gt;adjusting&lt;/em&gt; cached intermediate
   quantities is a neat idea. In fact, we encounter it each time we type
   &lt;code&gt;make&lt;/code&gt; at the command line! The general technique goes by many
   names&amp;mdash;including change propagation, incremental maintenance, and
   functional reactive programming&amp;mdash;and applies to basically &lt;em&gt;any&lt;/em&gt;
   side-effect-free computation. However, it's most effective when the
   dependency structure of the computation is sparse and requires little
   overhead to find and refresh stale values. In our example of computing &lt;span class="math"&gt;\(z\)&lt;/span&gt;,
   these considerations manifest themselves as the heap vs linear structures and
   our fast array implementation instead of a generic tree datastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Generalizations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No zero? No problem. We don't &lt;em&gt;actually&lt;/em&gt; require a zero element. So, it's
   fair to augment &lt;span class="math"&gt;\(\boldsymbol{K} \cup \{ \textsf{null} \}\)&lt;/span&gt; where
   &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; is distinguished value (i.e., &lt;span class="math"&gt;\(\textsf{null} \notin
   \boldsymbol{K}\)&lt;/span&gt;) that &lt;em&gt;acts&lt;/em&gt; just like a zero after we overload &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt; to
   satisfy the definition of a zero (e.g., by adding an if-statement).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to an arbitrary maps instead of fixed vectors is possible with
   a "locator" map, which a bijective map from elements to indices in a dense
   array.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for growing and shrinking: We support &lt;strong&gt;growing&lt;/strong&gt; by maintaining an
   underlying array that is always slightly larger than we need&amp;mdash;which
   we're &lt;em&gt;already&lt;/em&gt; doing in the heap datastructure. Doubling the size of the
   underlying array (i.e., rounding up to the next power of two) has the added
   benefit of allowing us to grow &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt; at no asymptotic cost!  This
   is because the resize operation, which requires an &lt;span class="math"&gt;\(\mathcal{O}(n)\)&lt;/span&gt; time to
   allocate a new array and copying old values, happens so infrequently that
   they can be completely amortized. We get of effect of &lt;strong&gt;shrinking&lt;/strong&gt; by
   replacing the old value with &lt;span class="math"&gt;\(\textsf{null}\)&lt;/span&gt; (or &lt;span class="math"&gt;\(\boldsymbol{0}\)&lt;/span&gt;). We can
   shrink the underlying array when the fraction of nonzeros dips below
   &lt;span class="math"&gt;\(25\%\)&lt;/span&gt;. This prevents "thrashing" between shrinking and growing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Application&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sampling from an evolving distribution&lt;/strong&gt;: Suppose that &lt;span class="math"&gt;\(\boldsymbol{w}\)&lt;/span&gt;
corresponds to a categorical distributions over &lt;span class="math"&gt;\(\{1, \ldots, n\}\)&lt;/span&gt; and that we'd
like to sample elements from in proportion to this (unnormalized) distribution.&lt;/p&gt;
&lt;p&gt;Other methods like the &lt;a href="http://www.keithschwarz.com/darts-dice-coins/"&gt;alias&lt;/a&gt; or
inverse CDF methods are efficient after a somewhat costly initialization
step. But! they are not as efficient as the heap sampler when the distribution
is being updated. (I'm not sure about whether variants of alias that support
updates exist.)&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Update&lt;/th&gt;
&lt;th&gt;Init&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;alias&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;td&gt;O(n)?&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;i-CDF&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heap&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Use cases include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Gibbs_sampling"&gt;Gibbs sampling&lt;/a&gt;, where
  distributions are constantly modified and sampled from (changes may not be
  sparse so YMMV). The heap sampler is used in
  &lt;a href="https://arxiv.org/abs/1412.4986"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/"&gt;EXP3&lt;/a&gt;
  (&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;mutli-armed bandit algorithm&lt;/a&gt;)
  is an excellent example of an algorithm that samples and modifies a single
  weight in the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Stochastic priority queues&lt;/em&gt; where we sample proportional to priority and the
  weights on items in the queue may change, elements are possibly removed after
  they are sampled (i.e., sampling without replacement), and elements are added.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, I won't spell out all of the details of these algorithms. Instead, I'll
just give the code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inverse CDF sampling&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Ordinary sampling method, O(n) init, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;            &lt;span class="c1"&gt;# build cdf, O(n)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;     &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# binary search, O(log n)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Heap sampling&lt;/strong&gt; is essentially the same, except the cdf is stored as heap,
which is perfect for binary search!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hsample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Sample from sumheap, O(log n) per sample.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="c1"&gt;# number of internal nodes.&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# random probe, p ~ Uniform(0, z)&lt;/span&gt;
    &lt;span class="c1"&gt;# Use binary search to find the index of the largest CDF (represented as a&lt;/span&gt;
    &lt;span class="c1"&gt;# heap) value that is less than a random probe.&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Determine if the value is in the left or right subtree.&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;         &lt;span class="c1"&gt;# Point at left child&lt;/span&gt;
        &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="c1"&gt;# Probability mass under left subtree.&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;   &lt;span class="c1"&gt;# Value is in right subtree.&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;  &lt;span class="c1"&gt;# Subtract mass from left subtree&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;     &lt;span class="c1"&gt;# Point at right child&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Complete code and test cases for heap sampling are available in this
&lt;a href="https://gist.github.com/timvieira/da31b56436045a3122f5adf5aafec515"&gt;gist&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 21 Nov 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-11-21:blog/post/2016/11/21/heaps-for-incremental-computation/</guid><category>sampling</category><category>datastructures</category></item><item><title>Reversing a sequence with sublinear space</title><link>http://timvieira.github.io/blog/post/2016/10/01/reversing-a-sequence-with-sublinear-space/</link><description>&lt;p&gt;Suppose we have a computation which generates sequence of states &lt;span class="math"&gt;\(s_1 \ldots
s_n\)&lt;/span&gt; according to &lt;span class="math"&gt;\(s_{t} = f(s_{t-1})\)&lt;/span&gt; where &lt;span class="math"&gt;\(s_0\)&lt;/span&gt; is given.&lt;/p&gt;
&lt;p&gt;We'd like to devise an algorithm, which can reconstruct each point in the
sequence efficiently as we traverse it backwards. You can think of this as
"hitting undo" from the end of the sequence or reversing a singly-liked list.&lt;/p&gt;
&lt;p&gt;Obviously, we &lt;em&gt;could&lt;/em&gt; just record the entire sequence, but if &lt;span class="math"&gt;\(n\)&lt;/span&gt; is large &lt;em&gt;or&lt;/em&gt;
the size of each state is large, this will be infeasible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 0&lt;/strong&gt;: Rerun the forward pass &lt;span class="math"&gt;\(n\)&lt;/span&gt; times. Runtime &lt;span class="math"&gt;\(\mathcal{O}(n^2)\)&lt;/span&gt;, space
  &lt;span class="math"&gt;\(\mathcal{O}(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 1&lt;/strong&gt;: Suppose we save &lt;span class="math"&gt;\(0 &amp;lt; k \le n\)&lt;/span&gt; evenly spaced "checkpoint" states.
Clearly, this gives us &lt;span class="math"&gt;\(\mathcal{O}(k)\)&lt;/span&gt; space, but what does it do to the
runtime?  Well, if we are at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; the we have to "replay" computation from
the last recorded checkpoint to get &lt;span class="math"&gt;\(s_t\)&lt;/span&gt;, which takes &lt;span class="math"&gt;\(O(n/k)\)&lt;/span&gt; time. Thus, the
overall runtimes becomes &lt;span class="math"&gt;\(O(n^2/k)\)&lt;/span&gt;. This runtime is not ideal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea 2&lt;/strong&gt;: &lt;em&gt;Idea 1&lt;/em&gt; did something kind of silly, within a chunk of size &lt;span class="math"&gt;\(n/k\)&lt;/span&gt;,
it does each computation multiple times! Suppose we increase the memory
requirement &lt;em&gt;just a little bit&lt;/em&gt; to remember the current chunk we're working on,
making it now &lt;span class="math"&gt;\(\mathcal{O}(k + n/k)\)&lt;/span&gt;. Now, we compute each state at most &lt;span class="math"&gt;\(2\)&lt;/span&gt;
times: once in the initial sequence and once in the reverse. This implies a
&lt;em&gt;linear&lt;/em&gt; runtime.  Now, the question: how should we set &lt;span class="math"&gt;\(k\)&lt;/span&gt; so that we minimize
extra space? Easy! Solve the following little optimization problem:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{k}{\textrm{argmin}}\ k+n/k = \sqrt{n}
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-optimal-space')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-optimal-space" style="display:none;" class="derivation"&gt;
To get the minimum, we solve for &lt;span class="math"&gt;\(k\)&lt;/span&gt; that sets the derivative to zero.
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}
    0 &amp;amp;=&amp;amp; \frac{\partial}{\partial k} \left[ k+n/k \right] \\
      &amp;amp;=&amp;amp; 1-n/k^2 \\
n/k^2 &amp;amp;=&amp;amp; 1 \\
  k^2 &amp;amp;=&amp;amp; n \\
    k &amp;amp;=&amp;amp; \sqrt{n}
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Since it's safe to assume that &lt;span class="math"&gt;\(n,k \ge 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial^2}{\partial k\,
\partial k} = 2 n / k^3 &amp;gt; 0\)&lt;/span&gt; this is indeed a minimum. It's also global minimum
because &lt;span class="math"&gt;\(k+n/k\)&lt;/span&gt; is convex in &lt;span class="math"&gt;\(k\)&lt;/span&gt; when &lt;span class="math"&gt;\(n,k &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;That's nuts! We get away with &lt;em&gt;sublinear&lt;/em&gt; space &lt;span class="math"&gt;\(\mathcal{O}(\sqrt{n})\)&lt;/span&gt; and we
only blow up our runtime by a factor of 2. Also, I really love the "introduce a
parameter then optimize it out" trick.&lt;/p&gt;
&lt;p&gt;&lt;button onclick="toggle('#code-sqrt-space')"&gt;Code&lt;/button&gt;
&lt;div id="code-sqrt-space" style="display:none;"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sqrt_space&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# last chunk may be shorter than k.&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
            &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Take `k` steps from state `s`, save path. Cost: O(k) space, O(k) time.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Idea 3&lt;/strong&gt;: What if we apply "the remember &lt;span class="math"&gt;\(k\)&lt;/span&gt; states" trick &lt;em&gt;recursively&lt;/em&gt;? I'm
going to work this out for &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; (and then claim that the value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; doesn't
matter).&lt;/p&gt;
&lt;p&gt;Run forward to get the midpoint at &lt;span class="math"&gt;\(s_{m}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(m=b + \lfloor n/2
\rfloor\)&lt;/span&gt;. Next, recurse on the left and right chunks &lt;span class="math"&gt;\([b,m)\)&lt;/span&gt; and &lt;span class="math"&gt;\([m,e)\)&lt;/span&gt;.
We hit the base case when the width of the interval is
one.&lt;/p&gt;
&lt;p&gt;Note that we implicitly store midpoints as we recurse (thanks to the stack
frame).  The max depth of the recursion is &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt;, which gives us
a &lt;span class="math"&gt;\(\mathcal{O}(\log n)\)&lt;/span&gt; space bound.&lt;/p&gt;
&lt;p&gt;We can characterize runtime with the following recurrence relation, &lt;span class="math"&gt;\(T(n) = 2
\cdot T(n/2) + \mathcal{O}(n)\)&lt;/span&gt;. Since we recognize this as the recurrence for
mergesort, we know that it flattens to &lt;span class="math"&gt;\(\mathcal{O}(n \log n)\)&lt;/span&gt; time. Also, just
like in the case of sorting, the branching factor doesn't matter so we're happy
with or initial assumption that &lt;span class="math"&gt;\(k=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;button onclick="toggle('#code-recursive')"&gt;Code&lt;/button&gt;
&lt;div id="code-recursive" style="display:none;"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# do O(n/2) work to find the midpoint with O(1) space.&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;The algorithms describe in this post are generic algorithmic tricks, which has
been used in a number of place, including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The classic computer science interview problem of reversing a singly-linked list
  under a tight budget on &lt;em&gt;additional&lt;/em&gt; memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backpropagation for computing gradients in sequence models, including HMMs (&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2000/01/icslp00_logspace.pdf"&gt;Zweig &amp;amp; Padmanabhan, 2000&lt;/a&gt;)
  and RNNs (&lt;a href="https://arxiv.org/abs/1604.06174v2"&gt;Chen et al., 2016&lt;/a&gt;). I have
  sample code that illustrates the basic idea below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory-efficient &lt;a href="https://arxiv.org/pdf/cs/0310016v1"&gt;omniscient debugging&lt;/a&gt;,
  which allows a user to inspect program state while moving forward &lt;em&gt;and
  backward&lt;/em&gt; in time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sample code&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/timvieira/d2ac72ec3af7972d2471035011cbf1e2"&gt;The basics&lt;/a&gt;:
  Simple implementation complete with test cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/timvieira/aceb64047aed1b13bf4e4da3b9a4c0ea"&gt;Memory-efficient backprop in an RNN&lt;/a&gt;:
  A simple application with test cases, of course.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sat, 01 Oct 2016 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-10-01:blog/post/2016/10/01/reversing-a-sequence-with-sublinear-space/</guid><category>algorithms</category></item><item><title>Evaluating ∇f(x) is as fast as f(x)</title><link>http://timvieira.github.io/blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/</link><description>&lt;p&gt;Automatic differentiation ('autodiff' or 'backprop') is great&amp;mdash;not just
because it makes it easy to rapidly prototype deep networks with plenty of
doodads and geegaws, but because it means that evaluating the gradient &lt;span class="math"&gt;\(\nabla
f(x)\)&lt;/span&gt; is as fast of computing &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;. In fact, the gradient provably requires at
most a &lt;em&gt;small&lt;/em&gt; constant factor more arithmetic operations than the function
itself.  Furthermore, autodiff tells us how to derive and implement the gradient
efficiently. This is a fascinating result that is perhaps not emphasized enough
in machine learning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The gradient should never be asymptotically slower than the function.&lt;/strong&gt; In my
recent &lt;a href="/doc/2016-emnlp-vocrf.pdf"&gt;EMNLP'16 paper&lt;/a&gt;, my coauthors and I found a
line of work on variable-order CRFs
(&lt;a href="https://papers.nips.cc/paper/3815-conditional-random-fields-with-high-order-features-for-sequence-labeling.pdf"&gt;Ye+'09&lt;/a&gt;;
&lt;a href="http://www.jmlr.org/papers/volume15/cuong14a/cuong14a.pdf"&gt;Cuong+'14&lt;/a&gt;), which
had an unnecessarily slow and complicated algorithm for computing gradients,
which was asymptotically (and practically) slower than their forward
algorithm. Without breaking a sweat, we derived a simpler and more efficient
gradient algorithm by simply applying backprop to the forward algorithm (and
made some other contributions).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Many algorithms are just backprop.&lt;/strong&gt; For example, forward-backward and
inside-outside, are actually just instances of automatic differentiation
(&lt;a href="https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf"&gt;Eisner,'16&lt;/a&gt;) (i.e.,
outside is just backprop on inside). This shouldn't be a surprise because these
algorithms are used to compute gradients. Basically, if you know backprop and
the inside algorithm, then you can derive the outside algorithm by applying the
backprop transform manually. I find it easier to understand the outside
algorithm via its connection to backprop, then via
&lt;a href="https://www.cs.jhu.edu/~jason/465/iobasics.pdf"&gt;the usual presentation&lt;/a&gt;. Note
that inside-outside and forward-backward pre-date backpropagation and have
additional uses beyond computing gradients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once you've grokked backprop, the world is your oyster!&lt;/strong&gt; You can backprop
through many approximate inference algorithms, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v15/stoyanov11a/stoyanov11a.pdf"&gt;Stoyanov+'11&lt;/a&gt;
and many of Justin Domke's papers, to avoid issues I've mentioned
&lt;a href="http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/"&gt;before&lt;/a&gt;. You
can even backprop through optimization algorithms to get gradients of dev loss wrt
hyperparameters, e.g.,
&lt;a href="http://www.jmlr.org/proceedings/papers/v22/domke12/domke12.pdf"&gt;Domke'12&lt;/a&gt; and
&lt;a href="https://arxiv.org/abs/1502.03492"&gt;Maclaurin+'15&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's at least one catch!&lt;/strong&gt; Although the &lt;em&gt;time&lt;/em&gt; complexity of computing the
gradient is as good as the function, the &lt;em&gt;space&lt;/em&gt; complexity may be much larger
because the autodiff recipe (at least the default reverse-mode one) requires memoizing
all intermediate quantities (e.g., the quantities you overwrite in a
loop). There are generic methods for balancing the time-space tradeoff in
autodiff, since you can (at least in theory) reconstruct the intermediate
quantities by playing the forward computation again from intermediate
checkpoints (at a cost to runtime, of course). A recent example is
&lt;a href="https://arxiv.org/abs/1606.03401"&gt;Gruslys+'16&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A final remark&lt;/strong&gt;. Despite the name "automatic" differentiation, there is no
need to rely on software to "automatically" give you gradient routines. Applying
the backprop transformation is generally easy to do manually and sometimes more
efficient than using a library. Many autodiff libraries lack good support for
dynamic computation graph, i.e., when the structure depends on quantities that
vary with the input (e.g., sentence length).&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 25 Sep 2016 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-09-25:blog/post/2016/09/25/evaluating-fx-is-as-fast-as-fx/</guid><category>calculus</category><category>automatic-differentiation</category><category>rant</category></item><item><title>Fast sigmoid sampling</title><link>http://timvieira.github.io/blog/post/2016/07/04/fast-sigmoid-sampling/</link><description>&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we describe a simple trick for efficiently sampling a Bernoulli random variable $Y$ from a sigmoid-defined distribution, $p(Y = 1) = (1 + \exp(-x))^{-1}$, where $x \in \mathbb{R}$ is the only parameter of the distribution ($x$ is often defined as the dot product of features and weights).&lt;/p&gt;
&lt;p&gt;The "slow" method for sampling from a sigmoid,&lt;/p&gt;
$$
u \sim \textrm{Uniform}(0,1)
$$$$
Y = sigmoid(x) &gt; u
$$&lt;p&gt;This method is slow because it calls the sigmoid function for every value of $x$. It is slow because $\exp$ is 2-3x slower than basic arithmetic operations.&lt;/p&gt;
&lt;p&gt;In this post, I'll describe a simple trick, which is well-suited to vectorized computations (e.g., numpy, matlab). The way it works is by &lt;em&gt;precomputing&lt;/em&gt; the expensive stuff (i.e., calls to expensive functions like $\exp$).&lt;/p&gt;
$$
sigmoid(x) &gt; u \Leftrightarrow logit(sigmoid(x)) &gt; logit(u) \Leftrightarrow x &gt; logit(u).
$$&lt;p&gt;Some details worth mentioning: (a) &lt;a href="https://en.wikipedia.org/wiki/Logit"&gt;logit&lt;/a&gt; is the inverse of sigmoid and (b) logit is strictly monotonic increasing you can apply it both sides of the greater than and preserves ordering (there's a plot in the appendix).&lt;/p&gt;
&lt;p&gt;The "fast" method derives it's advantage by leveraging the fact that expensive computation can be done independently of the data (i.e., specific values of $x$). The fast method is also interesting as just cute math. In the bonus section of this post, we'll make a connection to the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel max trick&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How fast is it in practice?&lt;/strong&gt; Below, we run a quick experiment to test that the method is correct and how fast it is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;matplotlib&lt;/span&gt; inline
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy.random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.special&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;expit&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.timer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;    &lt;span class="c1"&gt;# https://github.com/timvieira/arsenal&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;timers&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# These are the sigmoid parameters we&amp;#39;re going to sample from.&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# number of runs to average over.&lt;/span&gt;
&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="c1"&gt;# Used for plotting average p(Y=1)&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Temporary array for saving on memory allocation, cf. method slow-2.&lt;/span&gt;
&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# Let&amp;#39;s use the same random variables for all methods. This allows &lt;/span&gt;
    &lt;span class="c1"&gt;# for a lower variance comparsion and equivalence testing.&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# used in fast method: precompute expensive stuff.&lt;/span&gt;

    &lt;span class="c1"&gt;# Requires computing sigmoid for each x.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slow1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;           
        
    &lt;span class="c1"&gt;# Avoid memory allocation in slow-1 by using the out option to sigmoid&lt;/span&gt;
    &lt;span class="c1"&gt;# function. It&amp;#39;s a little bit faster than slow-1.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slow2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;           
        &lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;

    &lt;span class="c1"&gt;# Rolling our sigmoid is a bit slower than using the library function.&lt;/span&gt;
    &lt;span class="c1"&gt;# Not to mention this implementation isn&amp;#39;t as numerically stable.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;slow3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;
        
    &lt;span class="c1"&gt;# The fast method.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;
    
    &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;    
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compare&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;fast is 28.4239x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.00114061 fast: 4.01285e-05)&lt;/span&gt;
slow2 is 1.0037x faster than slow1 &lt;span class="ansi-yellow-fg"&gt;(avg: slow1: 0.00114061 slow2: 0.0011364)&lt;/span&gt;
slow1 is 1.0840x faster than slow3 &lt;span class="ansi-yellow-fg"&gt;(avg: slow3: 0.00123637 slow1: 0.00114061)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;It looks like our trick is about $28$x faster than the fastest competing slow method!&lt;/p&gt;
&lt;p&gt;We also see that the assert statements passed, which means that the methods tested produce precisely the same samples.&lt;/p&gt;
&lt;p&gt;The final plot demonstrates that we get the right expected value (red curve) as we sweep the distributions parameter (x-axis).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Bonus"&gt;Bonus&lt;a class="anchor-link" href="#Bonus"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We could alternatively use the &lt;a href="http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/"&gt;Gumbel max trick&lt;/a&gt; to derive a similar algorithm. If we ground out the trick for a sigmoid instead of a general mutlinomal distributions, we end up with&lt;/p&gt;
$$
Z_0 \sim \textrm{Gumbel}(0,1)
$$$$
Z_1 \sim \textrm{Gumbel}(0,1)
$$$$
Y = x &gt; Z_0 - Z_1
$$&lt;p&gt;Much like our new trick, this one benefits from the fact that all expensive stuff is done independent of the data (i.e., the value of $x$). However, it seems silly that we "need" to generate &lt;em&gt;two&lt;/em&gt; Gumbel RVs to get one sample from the sigmoid. With a little bit of Googling, we discover that the difference of $\textrm{Gumbel}(0,1)$ RVs is a &lt;a href="https://en.wikipedia.org/wiki/Logistic_distribution"&gt;logistic&lt;/a&gt; RV (specifically $\textrm{Logistic}(0,1)$).&lt;/p&gt;
&lt;p&gt;It turns out that $\textrm{logit}(\textrm{Uniform}(0,1))$ is a $\textrm{Logistic}(0,1)$ RV.&lt;/p&gt;
&lt;p&gt;Voila! Our fast sampling trick and the Gumbel max trick are connected!&lt;/p&gt;
&lt;h2 id="Related-tricks"&gt;Related tricks&lt;a class="anchor-link" href="#Related-tricks"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Another trick is Justin Domke's &lt;a href="https://justindomke.wordpress.com/2014/01/08/reducing-sigmoid-computations-by-at-least-88-0797077977882/"&gt;trick&lt;/a&gt; to reduce calls to $\exp$ by $\approx 88\%$. The &lt;em&gt;disadvantage&lt;/em&gt; of this approach is that it's harder to implement with vectorization. The &lt;em&gt;advantage&lt;/em&gt; is that we don't need to precompute any expensive things.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Appendix"&gt;Appendix&lt;a class="anchor-link" href="#Appendix"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logit-plot"&gt;Logit plot&lt;a class="anchor-link" href="#Logit-plot"&gt;&amp;#182;&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Logistic-random-variable"&gt;Logistic random variable&lt;a class="anchor-link" href="#Logistic-random-variable"&gt;&amp;#182;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Check that our sampling method is equivalent to sampling from a logistic distribution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython2"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;
&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logistic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 04 Jul 2016 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-07-04:blog/post/2016/07/04/fast-sigmoid-sampling/</guid><category>sampling</category><category>Gumbel</category></item><item><title>Sqrt-biased sampling</title><link>http://timvieira.github.io/blog/post/2016/06/28/sqrt-biased-sampling/</link><description>&lt;p&gt;The following post is about instance of "sampling in proportion to &lt;span class="math"&gt;\(p\)&lt;/span&gt; is not
optimal, but you probably think it is." It's surprising how few people seem to
know this trick. Myself included! It was brought to my attention recently by
&lt;a href="http://lowrank.net/nikos/"&gt;Nikos Karampatziakis&lt;/a&gt;. (Thanks, Nikos!)&lt;/p&gt;
&lt;p&gt;The paper credited for this trick is
&lt;a href="http://www.pnas.org/content/106/6/1716.full.pdf"&gt;Press (2008)&lt;/a&gt;. I'm borrowing
heavily from that paper as well as an email exchange from Nikos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Suppose you're an aspiring chef with a severe head injury affecting
  your long- and short- term memory trying to find a special recipe from a
  cookbook that you made one time but just can't remember exactly which recipe
  it was. So, based on the ingredients of each recipe, you come up with a prior
  probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; that recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the one you're looking for. In total, the
  cookbook has &lt;span class="math"&gt;\(n\)&lt;/span&gt; recipes and &lt;span class="math"&gt;\(\sum_{i=1}^n p_i = 1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A good strategy would be to sort recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; and cook the most promising
ones first. Unfortunately, you're not a great chef so there is some probability
that you'll mess-up the recipe. So, it's a good idea to try recipes multiple
times. Also, you have no short term memory...&lt;/p&gt;
&lt;p&gt;This suggests a &lt;em&gt;sampling with replacement&lt;/em&gt; strategy, where we sample a recipe
from the cookbook to try &lt;em&gt;independently&lt;/em&gt; of whether we've tried it before
(called a &lt;em&gt;memoryless&lt;/em&gt; strategy). Let's give this strategy the name
&lt;span class="math"&gt;\(\boldsymbol{q}.\)&lt;/span&gt; Note that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is a probability distribution over
the recipes in the cookbook, just like &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many recipes until we find the special one?&lt;/strong&gt; To start, suppose the
special recipe is &lt;span class="math"&gt;\(j.\)&lt;/span&gt; Then, the expected number of recipes we have to make
until we find &lt;span class="math"&gt;\(j\)&lt;/span&gt; under the strategy &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; is&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j} = 1/q_{j}.
$$&lt;/div&gt;
&lt;style&gt;
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
&lt;/style&gt;

&lt;script&gt;
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
&lt;/script&gt;

&lt;p&gt;&lt;button onclick="toggle('#derivation-series')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="derivation-series" style="display:none;" class="derivation"&gt;
&lt;strong&gt;Derivation&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We start with
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - q_j)^{t-1} q_{j},
$$&lt;/div&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(a = (1-q_j)\)&lt;/span&gt;, to clean up notation.
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty t \cdot a^{t-1}
$$&lt;/div&gt;
&lt;p&gt;Use the identity &lt;span class="math"&gt;\(\nabla_a [ a^t ] = t \cdot a^{t-1}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \sum_{t=1}^\infty \nabla_a[ a^{t} ].
$$&lt;/div&gt;
&lt;p&gt;Fish the gradient out of the sum and tweak summation index,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ \sum_{t=1}^\infty a^{t} \right]
= q_{j} \nabla_a\left[ -1 + \sum_{t=0}^\infty a^{t}\right]
$$&lt;/div&gt;
&lt;p&gt;Plugin in the solution to the geometric series,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \nabla_a\left[ -1 + \frac{1}{1-a} \right].
$$&lt;/div&gt;
&lt;p&gt;Take derivative, expand &lt;span class="math"&gt;\(a\)&lt;/span&gt; and simplify,
&lt;/p&gt;
&lt;div class="math"&gt;$$
= q_{j} \frac{1}{(1-a)^2}
= \frac{1}{q_j}
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The equation says that expected time it takes to sample &lt;span class="math"&gt;\(j\)&lt;/span&gt; for &lt;em&gt;the first time&lt;/em&gt;
is the probability we didn't sample for &lt;span class="math"&gt;\((t-1)\)&lt;/span&gt; steps times the probability we
sample it at time &lt;span class="math"&gt;\(t.\)&lt;/span&gt; We multiply this probability by the time &lt;span class="math"&gt;\(t\)&lt;/span&gt; to get the
&lt;em&gt;expected&lt;/em&gt; time.&lt;/p&gt;
&lt;p&gt;Note that this equation assumes that we known &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the special recipe &lt;em&gt;with
certainty&lt;/em&gt; when we sample it. We'll revisit this assumption later when we
consider potential errors in executing the recipe.&lt;/p&gt;
&lt;p&gt;Since we don't known which &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the right one, we take an expectation over it
according to the prior distribution, which yields the following equation,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{q_i}.
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The first surprising thing&lt;/strong&gt;: Uniform is just as good as &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt;,
  yikes! &lt;span class="math"&gt;\(f(\boldsymbol{p}) = \sum_{i=1}^n \frac{p_i}{p_i} = n\)&lt;/span&gt; and
  &lt;span class="math"&gt;\(f(\text{uniform}(n)) = \sum_{i=1}^n \frac{p_i }{ 1/n } = n.\)&lt;/span&gt; (Assume, without
  loss of generality, that &lt;span class="math"&gt;\(p_i &amp;gt; 0\)&lt;/span&gt; since we can just drop these elements from
  &lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the &lt;em&gt;optimal&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt;?&lt;/strong&gt; We can address this question by
solving the following optimization (which will have a nice closed form
solution),&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \boldsymbol{q}^* = \underset{\boldsymbol{q}}{\operatorname{argmin}} \sum_{i=1}^n \frac{p_i}{q_i} \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \text{ s.t. } \sum_{i=1}^n q_i = 1 \\
&amp;amp;&amp;amp; \ \ \ \ \ \ \ \ \ \ \ \ \, q_1 \ldots q_n \ge 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;The optimization problem says minimize the expected time to find the special
recipe. The constraints enforce that &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; be a valid probability
distribution.&lt;/p&gt;
&lt;p&gt;The optimal strategy, which we get via Lagrange multipliers, turns out to be,
&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;p&gt;&lt;button onclick="toggle('#Lagrange')" class="toggle-button"&gt;Derivation&lt;/button&gt;
&lt;div id="Lagrange" style="display:none;" class="derivation"&gt;
To solve this constrained optimization problem, we form the
Lagrangian,&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\boldsymbol{q}, \lambda) = \sum_{i=1}^n \frac{p_i}{q_i} - \lambda\cdot \left(1 - \sum_{i=1}^n q_i\right),$$&lt;/div&gt;
&lt;p&gt;and solve for &lt;span class="math"&gt;\(\boldsymbol{q}\)&lt;/span&gt; and multiplier &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; such that partial
derivatives are all equal to zero. This gives us the following system of
nonlinear equations,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
&amp;amp;&amp;amp; \lambda - \frac{p_i}{q_i^2} = 0 \ \ \ \text{for } 1 \le i \le n \\
&amp;amp;&amp;amp; \lambda \cdot \left(1 - \sum_{i=1}^n q_i \right) = 0.
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;We see that &lt;span class="math"&gt;\(q_i = \pm \sqrt{\frac{p_i}{\lambda}}\)&lt;/span&gt; works for the first set of
equations, but since we need &lt;span class="math"&gt;\(q_i \ge 0\)&lt;/span&gt;, we take the positive one. Solving for
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; and plugging it in, we get a normalized distribution,&lt;/p&gt;
&lt;div class="math"&gt;$$
q^*_i = \frac{ \sqrt{p_i} }{ \sum_{j=1}^n \sqrt{p_j} }.
$$&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How much better is &lt;span class="math"&gt;\(q^*\)&lt;/span&gt;?&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
f(q^*) = \sum_i \frac{p_i}{q^*_i}
= \sum_i \frac{p_i}{ \frac{\sqrt{p_i} }{ \sum_j \sqrt{p_j}} }
= \left( \sum_i \frac{p_i}{ \sqrt{p_i} } \right) \left( \sum_j \sqrt{p_j} \right)
= \left( \sum_i \sqrt{p_i} \right)^2
$$&lt;/div&gt;
&lt;p&gt;which sometimes equals &lt;span class="math"&gt;\(n\)&lt;/span&gt;, e.g., when &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; is uniform, but is never
bigger than &lt;span class="math"&gt;\(n.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's the intuition?&lt;/strong&gt; The reason why the &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-scheme is preferred is
because we save on &lt;em&gt;additional&lt;/em&gt; cooking experiments. For example, if a recipe
has &lt;span class="math"&gt;\(k\)&lt;/span&gt; times higher prior probability than the average recipe, then we will try
that recipe &lt;span class="math"&gt;\(\sqrt{k}\)&lt;/span&gt; times more often; compared to &lt;span class="math"&gt;\(k\)&lt;/span&gt;, which we'd get under
&lt;span class="math"&gt;\(\boldsymbol{p}.\)&lt;/span&gt; Additional cooking experiments are not so advantageous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Allowing for noise in the cooking process&lt;/strong&gt;: Suppose that for each recipe we
  had a prior belief about how hard that recipe is for us to cook. Denote that
  belief &lt;span class="math"&gt;\(s_i\)&lt;/span&gt;, these belief are between zero (never get it right) and one
  (perfect every time) and do not sum to one over the cookbook.&lt;/p&gt;
&lt;p&gt;Following a similar derivation to before, the time to cook the special recipe
&lt;span class="math"&gt;\(j\)&lt;/span&gt; and cook it correctly is,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{t=1}^\infty t \cdot (1 - \color{red}{s_j} q_j)^{t-1} q_{j} \color{red}{s_j} = \frac{1}{s_j \cdot q_j}
$$&lt;/div&gt;
&lt;p&gt;
That gives rise to a modified objective,
&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(\boldsymbol{q}) = \sum_{i=1}^n \frac{p_i}{\color{red}{s_i} \cdot q_i}
$$&lt;/div&gt;
&lt;p&gt;This is exactly the same as the previous objective, except we've replaced &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;
with &lt;span class="math"&gt;\(p_i/s_i.\)&lt;/span&gt; Thus, we can reuse our previous derivation to get the optimal
strategy, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i / s_i}.\)&lt;/span&gt; If noise is constant, then we
recover the original solution, &lt;span class="math"&gt;\(q^*_i \propto \sqrt{p_i}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extension to finding multiple tasty recipes&lt;/strong&gt;: Suppose we're trying to find
  several tasty recipes, not just a single special one. Now, &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is our prior
  belief that we'll like the recipe at all. How do we minimize the time until we
  find a tasty one? It turns out the same trick works without modification
  because all derivations apply to each recipe independently. The same trick
  works if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; does not sums to one over &lt;span class="math"&gt;\(n.\)&lt;/span&gt; For example, if &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; is the
  independent probability that you'll like recipe &lt;span class="math"&gt;\(i\)&lt;/span&gt; at all, not the
  probability that it's the special one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond memoryless policies&lt;/strong&gt;: Clearly, our choice of a memoryless policy can
  be beat by a policy family that balances exploration (trying new recipes) and
  exploitation (trying our best guess).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Overall, the problem we've posed is similar to a
    &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;multi-armed bandit&lt;/a&gt;. In
    our case, the arms are the recipes, pulling the arm is trying the recipe and
    the reward is whether or not we liked the recipe (possibly noisy). The key
    difference between our setup and multi-armed bandits is that we trust our
    prior distribution &lt;span class="math"&gt;\(\boldsymbol{p}\)&lt;/span&gt; and noise model &lt;span class="math"&gt;\(\boldsymbol{s}.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the amount of noise &lt;span class="math"&gt;\(s_i\)&lt;/span&gt; is known and we trust the prior &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; then
    there is an optimal deterministic (without-replacement) strategy that we can
    get by sorting the recipes by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; accounting for the error rates
    &lt;span class="math"&gt;\(s_i.\)&lt;/span&gt; This approach is described in the original paper.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;A more realistic application&lt;/strong&gt;: In certain language modeling applications, we
  avoid computing normalization constants (which require summing over a massive
  vocabulary) by using importance sampling, negative sampling or noise
  contrastive estimation techniques (e.g.,
  &lt;a href="https://arxiv.org/pdf/1511.06909.pdf"&gt;Ji+,16&lt;/a&gt;;
  &lt;a href="http://www.aclweb.org/anthology/Q15-1016"&gt;Levy+,15&lt;/a&gt;). These techniques depend
  on a proposal distribution, which folks often take to be the unigram
  distribution. Unfortunately, this gives too many samples of stop words (e.g.,
  "the", "an", "a"), so practitioners "anneal" the unigram distribution (to
  increase the entropy), that is sample from &lt;span class="math"&gt;\(q_i \propto
  p_{\text{unigram},i}^\alpha.\)&lt;/span&gt; Typically, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is set by grid search and
  (no surprise) &lt;span class="math"&gt;\(\alpha \approx 1/2\)&lt;/span&gt; tends to work best! The &lt;span class="math"&gt;\(\sqrt{p}\)&lt;/span&gt;-sampling
  trick is possibly a reverse-engineered justification in favor of annealing as
  "the right thing to do" (e.g., why not do additive smoothing?) and it even
  tells us how to set the annealing parameter &lt;span class="math"&gt;\(\alpha.\)&lt;/span&gt; The key assumption is
  that we want to sample the actual word at a given position as often as
  possible while still being diverse thanks to the coverage of unigram
  prior. (Furthermore, memoryless sampling leads to simpler algorithms.)&lt;/p&gt;
&lt;!--
Actually, many word2vec papers use $\alpha=3/4$, which was suggested in
[Levy+,15](http://www.aclweb.org/anthology/Q15-1016), including the default
value in
[gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L462). So,
[Ryan Cotterell](https://ryancotterell.github.io/) ran a quick experiment with
gensim, which confirmed the suspicion that $1/2$ may be better than $3/4.$

    Word similarity accuracy (avg of 10 runs)
    | alpha | accuracy |
    +==================+
    |  0.00 |    0.354 |
    |  0.25 |    0.403 |
    |  0.50 |    0.414 |
    |  0.75 |    0.395 |
    |  1.00 |    0.345 |
--&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Tue, 28 Jun 2016 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-06-28:blog/post/2016/06/28/sqrt-biased-sampling/</guid><category>sampling</category><category>decision-making</category></item><item><title>The optimal proposal distribution is not p</title><link>http://timvieira.github.io/blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/</link><description>&lt;p&gt;The following is a quick rant about
&lt;a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/"&gt;importance sampling&lt;/a&gt;
(see that post for notation).&lt;/p&gt;
&lt;p&gt;I've heard the following &lt;strong&gt;incorrect&lt;/strong&gt; statement one too many times,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We chose &lt;span class="math"&gt;\(q \approx p\)&lt;/span&gt; because &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is the "optimal" proposal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While it is certainly a good idea to pick &lt;span class="math"&gt;\(q\)&lt;/span&gt; to be as similar as possible to
&lt;span class="math"&gt;\(p\)&lt;/span&gt;, it is by no means &lt;em&gt;optimal&lt;/em&gt; because it is oblivious to &lt;span class="math"&gt;\(f\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;With importance sampling, it is possible to achieve a variance reduction over
Monte Carlo estimation. The optimal proposal distribution, assuming &lt;span class="math"&gt;\(f(x) \ge 0\)&lt;/span&gt;
for all &lt;span class="math"&gt;\(x\)&lt;/span&gt;, is &lt;span class="math"&gt;\(q(x) \propto p(x) f(x).\)&lt;/span&gt; This choice of &lt;span class="math"&gt;\(q\)&lt;/span&gt; gives us a &lt;em&gt;zero
variance&lt;/em&gt; estimate &lt;em&gt;with a single sample&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Of course, this is an unreasonable distribution to use because the normalizing
constant &lt;em&gt;is the thing you are trying to estimate&lt;/em&gt;, but it is proof that &lt;em&gt;better
proposal distributions exist&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The key to doing better than &lt;span class="math"&gt;\(q=p\)&lt;/span&gt; is to take &lt;span class="math"&gt;\(f\)&lt;/span&gt; into account. Look up
"importance sampling for variance reduction" to learn more.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sat, 28 May 2016 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-05-28:blog/post/2016/05/28/the-optimal-proposal-distribution-is-not-p/</guid><category>statistics</category><category>sampling</category><category>importance-sampling</category></item><item><title>Dimensional analysis of gradient ascent</title><link>http://timvieira.github.io/blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/</link><description>&lt;p&gt;In physical sciences, numbers are paired with units and called quantities. In
this augmented number system, dimensional analysis provides a crucial sanity
check, much like type checking in a programming language. There are simple rules
for building up units and constraints on what operations are allowed. For
example, you can't multiply quantities which are not conformable or add
quantities with different units. Also, we generally know the units of the input
and desired output, which allows us to check that our computations at least
produce the right units.&lt;/p&gt;
&lt;p&gt;In this post, we'll discuss the dimensional analysis of gradient ascent, which
will hopefully help us understand why the "step size" is parameter so finicky
and why it even exists.&lt;/p&gt;
&lt;p&gt;Gradient ascent is an iterative procedure for (locally) maximizing a function,
&lt;span class="math"&gt;\(f: \mathbb{R}^d \mapsto \mathbb{R}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
x_{t+1} = x_t + \alpha \frac{\partial f(x_t)}{\partial x}
$$&lt;/div&gt;
&lt;p&gt;In general, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a &lt;span class="math"&gt;\(d \times d\)&lt;/span&gt; matrix, but often we constrain the matrix
to be simple, e.g., &lt;span class="math"&gt;\(a\cdot I\)&lt;/span&gt; for some scalar &lt;span class="math"&gt;\(a\)&lt;/span&gt; or &lt;span class="math"&gt;\(\text{diag}(a)\)&lt;/span&gt; for some
vector &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at the units of the change in &lt;span class="math"&gt;\(\Delta x=x_{t+1} - x_t\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\Delta x) = \left(\textbf{units }\alpha\cdot \frac{\partial f(x_t)}{\partial x}\right) = (\textbf{units }\alpha) \frac{(\textbf{units }f)}{(\textbf{units }x)}.
$$&lt;/div&gt;
&lt;p&gt;The units of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; must be &lt;span class="math"&gt;\((\textbf{units }x)\)&lt;/span&gt;. However, if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt;
is unit free, we're happy with &lt;span class="math"&gt;\((\textbf{units }x) / (\textbf{units }f)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Solving for the units of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; we get,
&lt;/p&gt;
&lt;div class="math"&gt;$$
(\textbf{units }\alpha) = \frac{(\textbf{units }x)^2}{(\textbf{units }f)}.
$$&lt;/div&gt;
&lt;p&gt;This gives us an idea for what &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; should be.&lt;/p&gt;
&lt;p&gt;For example, the inverse Hessian passes the unit check (if we assume &lt;span class="math"&gt;\(f\)&lt;/span&gt; unit
free). The disadvantages of the Hessian is that it needs to be positive-definite
(or at least invertible) in order to be a valid "step size" (i.e., we need
step sizes to be &lt;span class="math"&gt;\(&amp;gt; 0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Another method for handling step sizes is line search. However, line search
won't let us run online. Furthermore, line search would be too slow in the case
where we want a step size for each dimension.&lt;/p&gt;
&lt;p&gt;In machine learning, we've become fond of online methods, which adapt the step
size as they go. The general idea is to estimate a step size matrix that passes
the unit check (for each dimension of &lt;span class="math"&gt;\(x\)&lt;/span&gt;). Furthermore, we want do as little
extra work as possible to get this estimate (e.g., we want to avoid computing a
Hessian because that would be extra work). So, the step size should be based
only iterates and gradients up to time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf"&gt;AdaGrad&lt;/a&gt; doesn't doesn't
  pass the unit check. This motivated AdaDelta.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1212.5701"&gt;AdaDelta&lt;/a&gt; uses the ratio of (running
  estimates of) the root-mean-squares of &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\partial f / \partial
  x\)&lt;/span&gt;. The mean is taken using an exponentially weighted moving average. See
  paper for actual implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; came later and made some tweaks to
  remove (unintended) bias in the AdaDelta estimates of the numerator and
  denominator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, it's important/useful to analyze the units of numerical algorithms
in order to get a sanity check (i.e., catch mistakes) as well as to develop an
understanding of why certain parameters exist and how properties of a problem
affect the values we should use for them.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Fri, 27 May 2016 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-05-27:blog/post/2016/05/27/dimensional-analysis-of-gradient-ascent/</guid><category>optimization</category></item><item><title>Gradient-based Hyperparameter Optimization and the Implicit Function Theorem</title><link>http://timvieira.github.io/blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/</link><description>&lt;p&gt;The most approaches to hyperparameter optimization can be viewed as a bi-level
optimization---the "inner" optimization optimizes training loss (wrt &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;),
while the "outer" optimizes hyperparameters (&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(
\underset{\theta}{\textbf{argmin}}\
\mathcal{L}_{\text{train}}(\theta, \lambda) \right)
$$&lt;/div&gt;
&lt;p&gt;Can we estimate &lt;span class="math"&gt;\(\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}\)&lt;/span&gt; so
that we can run gradient-based optimization over &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Well, what does it mean to have an &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; inside a function?&lt;/p&gt;
&lt;p&gt;Well, it means that there is a &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; that gets passed to
&lt;span class="math"&gt;\(\mathcal{L}_{\text{dev}}\)&lt;/span&gt;. And, &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, denoted
&lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt;. Furthermore, &lt;span class="math"&gt;\(\textbf{argmin}\)&lt;/span&gt; must set the derivative of the
inner optimization is zero in order to be a local optimum of the inner
function. So we can rephrase the problem as&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda^* = \underset{\lambda}{\textbf{argmin}}\
\mathcal{L}_{\text{dev}}\left(\theta(\lambda) \right),
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\theta(\lambda)\)&lt;/span&gt; is the solution to,
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{train}}(\theta, \lambda)}{\partial \theta} = 0.
$$&lt;/div&gt;
&lt;p&gt;Now how does &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; change as the result of an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The constraint on the derivative implies a type of "equilibrium"---the inner
optimization process will continue to optimize regardless of how we change
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Assuming we don't change &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; too much, then the inner
optimization shouldn't change &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; too much and it will change in a
predictable way.&lt;/p&gt;
&lt;p&gt;To do this, we'll appeal to the implicit function theorem. Let's looking the
general case to simplify notation. Suppose &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are related through a
function &lt;span class="math"&gt;\(g\)&lt;/span&gt; as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$g(x,y) = 0.$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;span class="math"&gt;\(g\)&lt;/span&gt; is a smooth function in &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we can perturb either
argument, say &lt;span class="math"&gt;\(x\)&lt;/span&gt; by a small amount &lt;span class="math"&gt;\(\Delta_x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; by &lt;span class="math"&gt;\(\Delta_y\)&lt;/span&gt;. Because
system preserves the constraint, i.e.,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x + \Delta_x, y + \Delta_y) = 0.
$$&lt;/div&gt;
&lt;p&gt;We can solve for the change of &lt;span class="math"&gt;\(x\)&lt;/span&gt; as a result of an infinitesimal change in
&lt;span class="math"&gt;\(y\)&lt;/span&gt;. We take the first-order expansion,&lt;/p&gt;
&lt;div class="math"&gt;$$
g(x, y) + \Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(g(x,y)\)&lt;/span&gt; is already zero,&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} + \Delta_y \frac{\partial g}{\partial y} = 0.
$$&lt;/div&gt;
&lt;p&gt;Next, we solve for &lt;span class="math"&gt;\(\frac{\Delta_x}{\Delta_y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\Delta_x \frac{\partial g}{\partial x} = - \Delta_y \frac{\partial g}{\partial y}.
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\Delta_x}{\Delta_y}  = -\left( \frac{\partial g}{\partial y} \right)^{-1} \frac{\partial g}{\partial x}.
$$&lt;/div&gt;
&lt;p&gt;Back to the original problem: Now we can use the implicit function theorem to
estimate how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; varies in &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; by plugging in &lt;span class="math"&gt;\(g \mapsto
\frac{\partial \mathcal{L}_{\text{train}}}{\partial \theta}\)&lt;/span&gt;, &lt;span class="math"&gt;\(x \mapsto \theta\)&lt;/span&gt;
and &lt;span class="math"&gt;\(y \mapsto \lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \theta}{\partial \lambda} = - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top}
$$&lt;/div&gt;
&lt;p&gt;This tells us how &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; changes with respect to an infinitesimal change to
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. Now, we can apply the chain rule to get the gradient of the whole
optimization problem wrt &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathcal{L}_{\text{dev}}}{\partial \lambda}
= \frac{\partial \mathcal{L}_{\text{dev}}}{\partial \theta} \left( - \left( \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \lambda^\top} \right)
$$&lt;/div&gt;
&lt;p&gt;Since we don't like (explicit) matrix inverses, we compute &lt;span class="math"&gt;\(- \left( \frac{
\partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top
} \right)^{-1} \frac{ \partial^2 \mathcal{L}_{\text{train}} }{ \partial \theta\,
\partial \lambda^\top}\)&lt;/span&gt; as the solution to &lt;span class="math"&gt;\(\left( \frac{ \partial^2
\mathcal{L}_{\text{train}} }{ \partial \theta\, \partial \theta^\top } \right) x
= -\frac{ \partial^2 \mathcal{L}_{\text{train}}}{ \partial \theta\, \partial
\lambda^\top}\)&lt;/span&gt;. When the Hessian is positive definite, the linear system can be
solved with conjugate gradient, which conveniently only requires matrix-vector
products---i.e., you never have to materialize the Hessian. (Apparently,
&lt;a href="https://en.wikipedia.org/wiki/Matrix-free_methods"&gt;matrix-free linear algebra&lt;/a&gt;
is a thing.) In fact, you don't even have to implement the Hessian-vector and
Jacobian-vector products because they are accurately and efficiently
approximated with centered differences (see
&lt;a href="/blog/post/2014/02/10/gradient-vector-product/"&gt;earlier post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;At the end of the day, this is an easy algorithm to implement! However, the
estimate of the gradient can be temperamental if the linear system is
ill-conditioned.&lt;/p&gt;
&lt;p&gt;In a later post, I'll describe a more-robust algorithms based on automatic
differentiation through the inner optimization algorithm, which make fewer and
less-brittle assumptions about the inner optimization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://justindomke.wordpress.com/2014/02/03/truncated-bi-level-optimization/"&gt;Truncated Bi-Level Optimization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://ai.stanford.edu/~chuongdo/papers/learn_reg.pdf"&gt;Efficient multiple hyperparameter learning for log-linear models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1502.03492"&gt;Gradient-based Hyperparameter Optimization through Reversible Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://fa.bianp.net/blog/2016/hyperparameter-optimization-with-approximate-gradient/"&gt;Hyperparameter optimization with approximate gradient&lt;/a&gt;
   (&lt;a href="https://arxiv.org/pdf/1602.02355.pdf"&gt;paper&lt;/a&gt;): This paper looks at the implicit
   differentiation approach where you have an &lt;em&gt;approximate&lt;/em&gt;
   solution to the inner optimization problem. They are able to provide error bounds and
   convergence guarantees under some reasonable conditions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sat, 05 Mar 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-03-05:blog/post/2016/03/05/gradient-based-hyperparameter-optimization-and-the-implicit-function-theorem/</guid><category>calculus</category></item><item><title>Multidimensional array index</title><link>http://timvieira.github.io/blog/post/2016/01/17/multidimensional-array-index/</link><description>&lt;p&gt;This is a simple note on how to compute a bijective mapping between the indices
of an &lt;span class="math"&gt;\(n\)&lt;/span&gt;-dimensional array and a flat, one-dimensional array. We'll look at
both directions of the mapping: &lt;code&gt;(tuple-&amp;gt;int)&lt;/code&gt; and &lt;code&gt;(int -&amp;gt; tuple)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We'll assume each dimension &lt;span class="math"&gt;\(a, b, c, \ldots\)&lt;/span&gt; is a positive integer and bounded
&lt;span class="math"&gt;\(a \le A, b \le B, c \le C, \ldots\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Start small&lt;/h3&gt;
&lt;p&gt;Let's start by looking at &lt;span class="math"&gt;\(n = 3\)&lt;/span&gt; and generalize from there.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's our test case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;-&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inverse_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;index_3&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
            &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: This is not the only bijective mapping from &lt;code&gt;tuple&lt;/code&gt; to &lt;code&gt;int&lt;/code&gt; that we
could have come up with. The one we chose corresponds to the particular layout,
which is apparent in the test case.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(n=4\)&lt;/span&gt; the pattern is &lt;span class="math"&gt;\(((a \cdot B + b) \cdot C + d) \cdot D + d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Sidenote: We don't actually need the bound &lt;span class="math"&gt;\(a \le A\)&lt;/span&gt; in either &lt;code&gt;index&lt;/code&gt; or
&lt;code&gt;inverse&lt;/code&gt;. This gives us a little extra flexibility because our first
dimension can be infinite/unknown.&lt;/p&gt;
&lt;h3&gt;General case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Map tuple ``a`` to index with known bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# the pattern:&lt;/span&gt;
    &lt;span class="c1"&gt;# ((i*J + j)*K + k)*L + l&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Find key given index ``ix`` and bounds ``A``.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;
        &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Testing the general case&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Test that `index` produces the layout we expect.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;got&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;got&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;test_layout&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;test_inverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 17 Jan 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2016-01-17:blog/post/2016/01/17/multidimensional-array-index/</guid><category>misc</category></item><item><title>Gradient of a product</title><link>http://timvieira.github.io/blog/post/2015/07/29/gradient-of-a-product/</link><description>&lt;div class="math"&gt;$$
\newcommand{\gradx}[1]{\grad{x}{ #1 }}
\newcommand{\grad}[2]{\nabla_{\! #1}\! \left[ #2 \right]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigo}[0]{\mathcal{O}}
$$&lt;/div&gt;
&lt;p&gt;In this post we'll look at how to compute the gradient of a product. This is
such a common subroutine in machine learning that it's worth careful
consideration. In a later post, I'll describe the gradient of a
sum-over-products, which is another interesting and common pattern in machine
learning (e.g., exponential families, CRFs, context-free grammar, case-factor
diagrams, semiring-weighted logic programming).&lt;/p&gt;
&lt;p&gt;Given a collection of functions with a common argument &lt;span class="math"&gt;\(f_1, \cdots, f_n \in \{
\R^d \mapsto \R \}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define their product &lt;span class="math"&gt;\(p(x) = \prod_{i=1}^n f_i(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose, we'd like to compute the gradient of the product of these functions
with respect to their common argument, &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \gradx{ \prod_{i=1}^n f_i(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \gradx{f_i(x)} \prod_{i \ne j} f_j(x)  \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;As you can see in the equation above, the gradient takes the form of a
"leave-one-out product" sometimes called a "cavity."&lt;/p&gt;
&lt;p&gt;A naive method for computing the gradient computes the leave-one-out products
from scratch for each &lt;span class="math"&gt;\(i\)&lt;/span&gt; (outer loop)---resulting in a overall runtime of
&lt;span class="math"&gt;\(O(n^2)\)&lt;/span&gt; to compute the gradient. Later, we'll see a dynamic program for
computing this efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Division trick&lt;/strong&gt;: Before going down the dynamic programming rabbit hole, let's
consider the following relatively simple method for computing the gradient,
which uses division:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \frac{\gradx{f_i(x)} }{ f_i(x) } \prod_{j=1}^n f_j(x) \right)
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \frac{\gradx{f_i(x)} }{ f_i(x) } \right) \left( \prod_{j=1}^n f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f \ne 0\)&lt;/span&gt;. No worries, we can handle zeros with three cases: (1) If
   no zeros: the division trick works fine. (2) Only one zero: implies that only
   one term in the sum will have a nonzero gradient, which we compute via
   leave-one-out product. (3) Two or more zeros: all gradients are zero and
   there is no work to be done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requires multiplicative inverse operator (division) &lt;em&gt;and&lt;/em&gt;
   associative-commutative multiplication, which means it's not applicable to
   matrices.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Log trick&lt;/strong&gt;: Suppose &lt;span class="math"&gt;\(f_i\)&lt;/span&gt; are very small numbers (e.g., probabilities), which
we'd rather not multiply together because we'll quickly lose precision (e.g.,
for large &lt;span class="math"&gt;\(n\)&lt;/span&gt;). It's common practice (especially in machine learning) to replace
&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log f_i\)&lt;/span&gt;, which turns products into sums, &lt;span class="math"&gt;\(\prod_{j=1}^n f_j(x) =
\exp \left( \sum_{j=1}^n \log f_j(x) \right)\)&lt;/span&gt;, and tiny numbers (like
&lt;span class="math"&gt;\(\texttt{3.72e-44}\)&lt;/span&gt;) into large ones (like &lt;span class="math"&gt;\(\texttt{-100}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Furthermore, using the identity &lt;span class="math"&gt;\((\nabla g) = g \cdot \nabla \log g\)&lt;/span&gt;, we can
operate exclusively in the "&lt;span class="math"&gt;\(\log\)&lt;/span&gt;-domain".&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\gradx{ p(x) }
&amp;amp;=&amp;amp; \left( \sum_{i=1}^n \gradx{ \log f_i(x) } \right) \exp\left( \sum_{j=1}^n \log f_j(x) \right)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Pro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Numerically stable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; with space &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't require multiplicative inverse assuming you can compute &lt;span class="math"&gt;\(\gradx{ \log
   f_i(x) }\)&lt;/span&gt; without it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Requires &lt;span class="math"&gt;\(f &amp;gt; 0\)&lt;/span&gt;. But, we can use
   &lt;a href="http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/"&gt;LogReal number class&lt;/a&gt;
   to represent negative numbers in log-space, but we still need to be careful
   about zeros (like in the division trick).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn't easily generalize to other notions of multiplication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Dynamic programming trick&lt;/strong&gt;: &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; runtime and &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space. You may
recognize this as forward-backward algorithm for linear chain CRFs
(cf. &lt;a href="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf"&gt;Wallach (2004)&lt;/a&gt;,
section 7).&lt;/p&gt;
&lt;p&gt;The trick is very straightforward when you think about it in isolation. Compute
the products of all prefixes and suffixes. Then, multiply them together.&lt;/p&gt;
&lt;p&gt;Here are the equations:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
\alpha_0(x) &amp;amp;=&amp;amp; 1 \\
\alpha_t(x)
   &amp;amp;=&amp;amp; \prod_{i \le t} f_i(x)
   = \alpha_{t-1}(x) \cdot f_t(x) \\
\beta_{n+1}(x) &amp;amp;=&amp;amp; 1 \\
\beta_t(x)
  &amp;amp;=&amp;amp; \prod_{i \ge t} f_i(x) = f_t(x) \cdot \beta_{t+1}(x)\\
\gradx{ p(x) }
&amp;amp;=&amp;amp; \sum_{i=1}^n \left( \prod_{j &amp;lt; i} f_j(x) \right) \gradx{f_i(x)} \left( \prod_{j &amp;gt; i} f_j(x) \right) \\
&amp;amp;=&amp;amp; \sum_{i=1}^n \alpha_{i-1}(x) \cdot \gradx{f_i(x)} \cdot \beta_{i+1}(x)
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;Clearly, this requires &lt;span class="math"&gt;\(O(n)\)&lt;/span&gt; additional space.&lt;/p&gt;
&lt;p&gt;Only requires an associative operator (i.e., Does not require it to be
commutative or invertible like earlier strategies).&lt;/p&gt;
&lt;p&gt;Why do we care about the non-commutative multiplication? A common example is
matrix multiplication where &lt;span class="math"&gt;\(A B C \ne B C A\)&lt;/span&gt;, even if all matrices have the
conformable dimensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Connections to automatic differentiation&lt;/strong&gt;: The theory behind reverse-mode
automatic differentiation says that if you can compute a function, then you
&lt;em&gt;can&lt;/em&gt; compute it's gradient with the same asymptotic complexity, &lt;em&gt;but&lt;/em&gt; you might
need more space. That's exactly what we did here: We started with a naive
algorithm for computing the gradient with &lt;span class="math"&gt;\(\bigo(n^2)\)&lt;/span&gt; time and &lt;span class="math"&gt;\(\bigo(1)\)&lt;/span&gt; space
(other than the space to store the &lt;span class="math"&gt;\(n\)&lt;/span&gt; functions) and ended up with a &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt;
time &lt;span class="math"&gt;\(\bigo(n)\)&lt;/span&gt; space algorithm with a little clever thinking. What I'm saying
is autodiff---even if you don't use a magical package---tells us that an
efficient algorithm for the gradient always exists. Furthermore, it tells you
how to derive it manually, if you are so inclined. The key is to reuse
intermediate quantities (hence the increase in space).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sketch&lt;/em&gt;: In the gradient-of-a-product case, assuming we implemented
multiplication left-to-right (forward pass) that already defines the prefix
products (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;). It turns out that the backward pass gives us &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; as
adjoints. Lastly, we'd propagate gradients through the &lt;span class="math"&gt;\(f\)&lt;/span&gt;'s to get
&lt;span class="math"&gt;\(\frac{\partial p}{\partial x}\)&lt;/span&gt;. Essentially, we end up with exactly the dynamic
programming algorithm we came up with.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Wed, 29 Jul 2015 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2015-07-29:blog/post/2015/07/29/gradient-of-a-product/</guid><category>calculus</category><category>numerical</category><category>automatic-differentiation</category></item><item><title>Multiclass logistic regression and conditional random fields are the same thing</title><link>http://timvieira.github.io/blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/</link><description>&lt;p&gt;A short rant: multiclass logistic regression and conditional random fields (CRF)
are the same thing. This comes to a surprise to many people because CRFs tend to
be surrounded by additional "stuff."&lt;/p&gt;
&lt;p&gt;Multiclass logistic regression is simple. The goal is to predict the correct
label &lt;span class="math"&gt;\(y^*\)&lt;/span&gt; from handful of labels &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt; given the observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; based
on features &lt;span class="math"&gt;\(\phi(x,y)\)&lt;/span&gt;. Training this model typically requires computing the
gradient:&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,y^*) - \sum_{y \in \mathcal{Y}} p(y|x) \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
p(y|x) &amp;amp;=&amp;amp; \frac{1}{Z(x)} \exp(\theta^\top \phi(x,y)) &amp;amp; \ \ \ \ \text{and} \ \ \ \ &amp;amp;
Z(x) &amp;amp;=&amp;amp; \sum_{y \in \mathcal{Y}} \exp(\theta^\top \phi(x,y))
\end{eqnarray*}
$$&lt;/div&gt;
&lt;p&gt;At test-time, we often take the highest-scoring label under the model.&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{y}(x) = \textbf{argmax}_{y \in \mathcal{Y}} \theta^\top \phi(x,y)
$$&lt;/div&gt;
&lt;p&gt;A conditional random field is exactly multiclass logistic regression. The only
difference is that the sum and argmax are inefficient to compute naively (i.e.,
by enumeration). This point is often lost when people first learn about
CRFs. Some people never make this connection.&lt;/p&gt;
&lt;p&gt;Here's some stuff you'll see once we start talking about CRFs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inference algorithms (e.g., Viterbi decoding, forward-backward, Junction
   tree)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graphical models (factor graphs, Bayes nets, Markov random fields)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model templates (i.e., repeated feature functions)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the logistic regression case, we'd never use the term "inference" to describe
the "sum" and "max" over a handful of categories. Once we move to a structured
label space, this term gets throw around. (BTW, this isn't "statistical
inference," just algorithms to compute sum and max over &lt;span class="math"&gt;\(\mathcal{Y}\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;Graphical models establish a notation and structural properties which allow
efficient inference -- things like cycles and treewidth.&lt;/p&gt;
&lt;p&gt;Model templating is the only essential trick to move from logistic regression to
a CRF. Templating "solves" the problem that not all training examples have the
same "size" -- the set of outputs &lt;span class="math"&gt;\(\mathcal{Y}(x)\)&lt;/span&gt; now depends on &lt;span class="math"&gt;\(x\)&lt;/span&gt;. A model
template specifies how to compute the features for an entire output, by looking
at interactions between subsets of variables.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(x,\boldsymbol{y}) = \sum_{\alpha \in A(x)} \phi_\alpha(x,
\boldsymbol{y}_\alpha)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is a labeled subset of variables often called a factor and
&lt;span class="math"&gt;\(\boldsymbol{y}_\alpha\)&lt;/span&gt; is the subvector containing values of variables
&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. Basically, the feature function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gets to look at some subset of
the variables being predicted &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the entire input &lt;span class="math"&gt;\(x\)&lt;/span&gt;. The ability to look
at more of &lt;span class="math"&gt;\(y\)&lt;/span&gt; allows the model to make more coherent predictions.&lt;/p&gt;
&lt;p&gt;Anywho, it's often useful to take a step back and think about what you are
trying to compute instead of how you're computing it. In this post, this allowed
us see the similarity between logistic regression and CRFs even though they seem
quite different.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Wed, 29 Apr 2015 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2015-04-29:blog/post/2015/04/29/multiclass-logistic-regression-and-conditional-random-fields-are-the-same-thing/</guid><category>machine-learning</category><category>rant</category><category>crf</category></item><item><title>Conditional random fields as Deep learning models?</title><link>http://timvieira.github.io/blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/</link><description>&lt;p&gt;This post is intended to convince conditional random field (CRF) lovers that
deep learning might not be as crazy as it seems. And maybe even convince some
deep learning lovers that the graphical models might have interesting things to
offer.&lt;/p&gt;
&lt;p&gt;In the world of structured prediction, we are plagued by the high-treewidth
problem -- models with loopy factors are "bad" because exact inference is
intractable. There are three common approaches for dealing with this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Limit the expressiveness of the model (i.e., don't use to model you want)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the training objective&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approximate inference&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Approximate inference is tricky. Things can easily go awry.&lt;/p&gt;
&lt;p&gt;For example, structured perceptron training with loopy max-product BP instead of
exact max product can diverge
&lt;a href="http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf"&gt;(Kulesza &amp;amp; Pereira, 2007)&lt;/a&gt;. Another
example: using approximate marginals from sum-product loopy BP in place of the
true marginals in the gradient of the log-likelihood. This results in a
different nonconvex objective function. (Note:
&lt;a href="http://aclweb.org/anthology/C/C12/C12-1122.pdf"&gt;sometimes&lt;/a&gt; these loopy BP
approximations work fine.)&lt;/p&gt;
&lt;p&gt;It looks like using approximate inference during training changes the training
objective.&lt;/p&gt;
&lt;p&gt;So, here's a simple idea: learn a model which makes accurate predictions given
the approximate inference algorithm that will be used at test-time. Furthermore,
we should minimize empirical risk instead of log-likelihood because it is robust
to model miss-specification and approximate inference. In other words, make
training conditions as close as possible to test-time conditions.&lt;/p&gt;
&lt;p&gt;Now, as long as everything is differentiable, you can apply automatic
differentiation (backprop) to train the end-to-end system. This idea appears in
a few publications, including a handful of papers by Justin Domke, and a few by
Stoyanov &amp;amp; Eisner.&lt;/p&gt;
&lt;p&gt;Unsuprisingly, it works pretty well.&lt;/p&gt;
&lt;p&gt;I first saw this idea in Stoyanov &amp;amp; Eisner (2011). They use loopy belief
propagation as their approximate inference algorithm. At the end of the day,
their model is essentially a deep recurrent network, which came from unrolling
inference in a graphical model. This idea really struck me because it's clearly
right in the middle between graphical models and deep learning.&lt;/p&gt;
&lt;p&gt;You can immediately imagine swapping in other approximate inference algorithms
in place of loopy BP.&lt;/p&gt;
&lt;p&gt;Deep learning approaches get a bad reputation because there are a lot of
"tricks" to get nonconvex optimization to work and because model structures are
more open ended. Unlike graphical models, deep learning models have more
variation in model structures. Maybe being more open minded about model
structures is a good thing. We seem to have hit a brick wall with
likelihood-based training. At the same time, maybe we can port over some of the
good work on approximate inference as deep architectures.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Thu, 05 Feb 2015 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2015-02-05:blog/post/2015/02/05/conditional-random-fields-as-deep-learning-models/</guid><category>machine-learning</category><category>deep-learning</category><category>structured-prediction</category></item><item><title>Log-Real number class</title><link>http://timvieira.github.io/blog/post/2015/02/01/log-real-number-class/</link><description>&lt;p&gt;Most people know how to avoid numerical underflow in probability computations by
representing intermediate quantities in the log-domain. This trick turns
"multiplication" into "addition", "addition" into "logsumexp", "0" into
&lt;span class="math"&gt;\(-\infty\)&lt;/span&gt; and "1" into &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Most importantly, it turns really small numbers into
reasonable-size numbers.&lt;/p&gt;
&lt;p&gt;Unfortunately, without modification, this trick is limited to positive numbers
because &lt;code&gt;log&lt;/code&gt; of a negative number is &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Well, there is good news! For the cost of an extra bit, we can extend this trick
to the negative reals and furthermore, we get a bonafide ring instead of a mere
semiring.&lt;/p&gt;
&lt;p&gt;I first saw this trick in
&lt;a href="http://www.aclweb.org/anthology/D09-1005"&gt;Li and Eisner (2009)&lt;/a&gt;. The trick is
nicely summarized in Table 3 of that paper, which I've pasted below.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
&lt;img src="/blog/images/logreal.png"/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Why do I care?&lt;/strong&gt; When computing gradients (e.g., gradient of risk),
intermediate values are rarely all positive. Furthermore, we're often
multiplying small things together. I've recently found log-reals to be effective
at squeaking a bit more numerical accuracy.&lt;/p&gt;
&lt;p&gt;This trick is useful for almost all backprop computations because backprop is
essentially:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;adjoint(u) += adjoint(v) * dv/du.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only tricky bit is lifting all &lt;code&gt;du/dv&lt;/code&gt; computations into the log-reals.&lt;/p&gt;
&lt;p&gt;Implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This trick is better suited to programming languages with structs. Using
  objects will probably in an horrible slow down and using parallel arrays to
  store the sign bit and double is probably too tedious and error prone. (Sorry
  java folks.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here's a &lt;a href="https://github.com/andre-martins/TurboParser/blob/master/src/util/logval.h"&gt;C++ implementation&lt;/a&gt;
  with operator overloading from Andre Martins&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that log-real &lt;code&gt;+=&lt;/code&gt; involves calls to &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;, which will
  definitely slow your code down a bit (these functions are much slower than
  addition).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 01 Feb 2015 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2015-02-01:blog/post/2015/02/01/log-real-number-class/</guid><category>numerical</category></item><item><title>Importance Sampling</title><link>http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/</link><description>&lt;p&gt;Importance sampling is a powerful and pervasive technique in statistics, machine
learning and randomized algorithms.&lt;/p&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;Importance sampling is a technique for estimating the expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; of a
random variable &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; under distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; from samples of a different
distribution &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The key observation is that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is can expressed as the expectation of a
different random variable &lt;span class="math"&gt;\(f^*(x)=\frac{p(x)}{q(x)}\! \cdot\! f(x)\)&lt;/span&gt; under &lt;span class="math"&gt;\(q.\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}_{q}\! \left[ f^*(x) \right] = \mathbb{E}_{q}\! \left[ \frac{p(x)}{q(x)} f(x) \right] = \sum_{x} q(x) \frac{p(x)}{q(x)} f(x) = \sum_{x} p(x) f(x) = \mathbb{E}_{p}\! \left[ f(x) \right] = \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Technical condition: &lt;span class="math"&gt;\(q\)&lt;/span&gt; must have support everywhere &lt;span class="math"&gt;\(p\)&lt;/span&gt; does, &lt;span class="math"&gt;\(p(x) &amp;gt; 0
\Rightarrow q(x) &amp;gt; 0.\)&lt;/span&gt; Without this condition, the equation is biased! Note: &lt;span class="math"&gt;\(q\)&lt;/span&gt;
can support things that &lt;span class="math"&gt;\(p\)&lt;/span&gt; doesn't.&lt;/p&gt;
&lt;p&gt;Terminology: The quantity &lt;span class="math"&gt;\(w(x) = \frac{p(x)}{q(x)}\)&lt;/span&gt; is often referred to as the
"importance weight" or "importance correction". We often refer to &lt;span class="math"&gt;\(p\)&lt;/span&gt; as the
target density and &lt;span class="math"&gt;\(q\)&lt;/span&gt; the proposal density.&lt;/p&gt;
&lt;p&gt;Now, given samples &lt;span class="math"&gt;\(\{ x^{(i)} \}_{i=1}^{n}\)&lt;/span&gt; from &lt;span class="math"&gt;\(q,\)&lt;/span&gt; we can use the Monte
Carlo estimate, &lt;span class="math"&gt;\(\hat{\mu} \approx \frac{1}{n} \sum_{i=1}^n f^{*}(x^{(i)}),\)&lt;/span&gt; as
an unbiased estimator of &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;There are a few reasons we might want use importance sampling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convenience&lt;/strong&gt;: It might be trickier to sample directly from &lt;span class="math"&gt;\(p.\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bias-correction&lt;/strong&gt;: Suppose, we're developing an algorithm which requires
     samples to satisfy some "safety" condition (e.g., a minimum support
     threshold) and be unbiased. Importance sampling can be used to remove bias,
     while satisfying the condition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variance reduction&lt;/strong&gt;: It might be the case that sampling directly from
     &lt;span class="math"&gt;\(p\)&lt;/span&gt; would require more samples to estimate &lt;span class="math"&gt;\(\mu.\)&lt;/span&gt; Check out these
     &lt;a href="http://www.columbia.edu/~mh2078/MCS04/MCS_var_red2.pdf"&gt;great notes&lt;/a&gt; for
     more.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Off-policy evaluation and learning&lt;/strong&gt;: We might want to collect some
     "exploratory data" from &lt;span class="math"&gt;\(q\)&lt;/span&gt; and evaluate different "policies", &lt;span class="math"&gt;\(p\)&lt;/span&gt; (e.g.,
     to pick the best one). Here's a link to a future post on
     &lt;a href="http://timvieira.github.io/blog/post/2016/12/13/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation and counterfactual reasoning&lt;/a&gt;
     and some cool papers:
     &lt;a href="http://arxiv.org/abs/1209.2355"&gt;counterfactual reasoning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/cs/0204043"&gt;reinforcement learning&lt;/a&gt;,
     &lt;a href="http://arxiv.org/abs/1103.4601"&gt;contextual bandits&lt;/a&gt;,
     &lt;a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf"&gt;domain adaptation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few common cases for &lt;span class="math"&gt;\(q\)&lt;/span&gt; worth separate consideration:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: This is the case in experimental design, variance
     reduction, active learning and reinforcement learning. It's often difficult
     to design &lt;span class="math"&gt;\(q,\)&lt;/span&gt; which results in an estimator with "reasonable" variance. A
     very difficult case is in off-policy evaluation because it (essentially)
     requires a good exploratory distribution for every possible policy. (I have
     much more to say on this topic.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Little to no control over &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: For example, you're given some dataset
     (e.g., new articles) and you want to estimate performance on a different
     dataset (e.g., Twitter).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unknown &lt;span class="math"&gt;\(q\)&lt;/span&gt;&lt;/strong&gt;: In this case, we want to estimate &lt;span class="math"&gt;\(q\)&lt;/span&gt; (typically referred
     to as the propensity score) and use it in the importance sampling
     estimator. This technique, as far as I can tell, is widely used to remove
     selection bias when estimating effects of different treatments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Drawbacks&lt;/strong&gt;: The main drawback of importance sampling is variance. A few bad
samples with large weights can drastically throw off the estimator. Thus, it's
often the case that a biased estimator is preferred, e.g.,
&lt;a href="https://hips.seas.harvard.edu/blog/2013/01/14/unbiased-estimators-of-partition-functions-are-basically-lower-bounds/"&gt;estimating the partition function&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/1209.2355"&gt;clipping weights&lt;/a&gt;,
&lt;a href="http://arxiv.org/abs/cs/0204043"&gt;indirect importance sampling&lt;/a&gt;. A secondary
drawback is that both densities must be normalized, which is often intractable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What's next?&lt;/strong&gt; I plan to cover "variance reduction" and
&lt;a href="http://timvieira.github.io/blog/post/2016/12/13/counterfactual-reasoning-and-learning-from-logged-data/"&gt;off-policy evaluation&lt;/a&gt;
in more detail in future posts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Sun, 21 Dec 2014 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-12-21:blog/post/2014/12/21/importance-sampling/</guid><category>statistics</category><category>importance-sampling</category><category>sampling</category></item><item><title>Numerically-stable p-norms</title><link>http://timvieira.github.io/blog/post/2014/11/10/numerically-stable-p-norms/</link><description>&lt;p&gt;Consider the p-norm
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$&lt;/div&gt;
&lt;p&gt;In python this translates to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;First-pass implementation of p-norm.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, suppose &lt;span class="math"&gt;\(|x_i|^p\)&lt;/span&gt; causes overflow (for some &lt;span class="math"&gt;\(i\)&lt;/span&gt;). This will occur for sufficiently large &lt;span class="math"&gt;\(p\)&lt;/span&gt; or sufficiently large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;---even if &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; is representable (i.e., not NaN or &lt;span class="math"&gt;\(\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;big&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e300&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c1"&gt;# expected: 1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This fails because we can't square &lt;code&gt;big&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;big&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;A little math&lt;/h2&gt;
&lt;p&gt;There is a way to avoid overflowing because of a few large &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here's a little fact about p-norms: for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \alpha \cdot \boldsymbol{x} ||_p = |\alpha| \cdot || \boldsymbol{x}   ||_p
$$&lt;/div&gt;
&lt;p&gt;We'll use the following version (harder to remember)
&lt;/p&gt;
&lt;div class="math"&gt;$$
|| \boldsymbol{x} ||_p  = |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
$$&lt;/div&gt;
&lt;p&gt;Don't believe it? Here's some algebra:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray*}
|| \boldsymbol{x} ||_p
&amp;amp;=&amp;amp; \left( \sum_i |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; \left( \sum_i \frac{|\alpha|^p}{|\alpha|^p} \cdot |x_i|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left( \frac{|x_i| }{|\alpha|} \right)^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot \left( \sum_i \left| \frac{x_i }{\alpha} \right|^p \right)^{\frac{1}{p}} \\
&amp;amp;=&amp;amp; |\alpha| \cdot || \boldsymbol{x} / \alpha ||_p
\end{eqnarray*}
$$&lt;/div&gt;
&lt;h2&gt;Back to numerical stability&lt;/h2&gt;
&lt;p&gt;Suppose we pick &lt;span class="math"&gt;\(\alpha = \max_i |x_i|\)&lt;/span&gt;. Now, the largest number we have to take
the power of is one --- making it very difficult to overflow on the account of
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;. This should remind you of the infamous log-sum-exp trick.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;norm1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, our example from before works :-)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;1e+300&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It appears as if &lt;code&gt;scipy.linalg.norm&lt;/code&gt; is robust to overflow, while &lt;code&gt;numpy.linalg.norm&lt;/code&gt; is not. Note that &lt;code&gt;scipy.linalg.norm&lt;/code&gt; appears to be a bit slower.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;logsumexp&lt;/code&gt; trick is nearly identical, but operates in the log-domain, i.e., &lt;span class="math"&gt;\(\text{logsumexp}(\log(|x|) \cdot p) / p = \log || x ||_p\)&lt;/span&gt;. You can implement both tricks with the same code, if you use different number classes for log-domain and real-domain---a trick you might have seen before.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;arsenal.math&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;robust_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="mf"&gt;1.91432069824&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 10 Nov 2014 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-11-10:blog/post/2014/11/10/numerically-stable-p-norms/</guid><category>numerical</category></item><item><title>KL-divergence as an objective function</title><link>http://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/</link><description>&lt;p&gt;It's well-known that
&lt;a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL-divergence&lt;/a&gt;
is not symmetric, but which direction is right for fitting your model?&lt;/p&gt;
&lt;h4&gt;Which KL is which? A cheat sheet&lt;/h4&gt;
&lt;p&gt;If we're fitting &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; to &lt;span class="math"&gt;\(p\)&lt;/span&gt; using&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mean-seeking, &lt;em&gt;inclusive&lt;/em&gt; (more principled because approximates the &lt;em&gt;full&lt;/em&gt; distribution)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;requires normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; (i.e., often &lt;em&gt;not&lt;/em&gt; computationally convenient)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mode-seeking, &lt;em&gt;exclusive&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;no normalization wrt &lt;span class="math"&gt;\(p\)&lt;/span&gt; (i.e., computationally convenient)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Mnemonic&lt;/strong&gt;: "When the truth comes first, you get the whole truth" (h/t
&lt;a href="https://www.umiacs.umd.edu/~resnik/"&gt;Philip Resnik&lt;/a&gt;). Here "whole truth"
corresponds to the &lt;em&gt;inclusiveness&lt;/em&gt; of &lt;span class="math"&gt;\(\textbf{KL}(p || q)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As far as remembering the equation, I pretend that "&lt;span class="math"&gt;\(||\)&lt;/span&gt;" is a division symbol,
which happens to correspond nicely to a division symbol in the equation (I'm not
sure it's intentional).&lt;/p&gt;
&lt;h2&gt;Inclusive vs. exclusive divergence&lt;/h2&gt;
&lt;div style="background-color: #f2f2f2; border: 2px solid #ggg; padding: 10px;"&gt;

&lt;img src="http://timvieira.github.io/blog/images/KL-inclusive-exclusive.png" /&gt;
Figure by &lt;a href="http://www.johnwinn.org/"&gt;John Winn&lt;/a&gt;.
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Computational perspecive&lt;/h2&gt;
&lt;p&gt;Let's look at what's involved in fitting a model &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; in each
direction. In this section, I'll describe the gradient and pay special attention
to the issue of normalization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;: &lt;span class="math"&gt;\(p,q_\theta\)&lt;/span&gt; are probabilty distributions. &lt;span class="math"&gt;\(p = \bar{p} / Z_p\)&lt;/span&gt;,
where &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; is the normalization constant. Similarly for &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The easy direction &lt;span class="math"&gt;\(\textbf{KL}(q_\theta || p)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(q_\theta || p)
&amp;amp;= \sum_d q(d) \log \left( \frac{q(d)}{p(d)} \right) \\
&amp;amp;= \sum_d q(d) \left( \log q(d) - \log p(d) \right) \\
&amp;amp;= \underbrace{\sum_d q(d) \log q(d)}_{-\text{entropy}} - \underbrace{\sum_d q(d) \log p(d)}_{\text{cross-entropy}} \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's look at normalization of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, the entropy term is easy because there is no &lt;span class="math"&gt;\(p\)&lt;/span&gt; in it.
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d q(d) \log p(d)
&amp;amp;= \sum_d q(d) \log (\bar{p}(d) / Z_p) \\
&amp;amp;= \sum_d q(d) \left( \log \bar{p}(d) - \log Z_p) \right) \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \sum_d q(d) \log Z_p \\
&amp;amp;= \sum_d q(d) \log \bar{p}(d) - \log Z_p
\end{align*}&lt;/div&gt;
&lt;p&gt;In this case, &lt;span class="math"&gt;\(-\log Z_p\)&lt;/span&gt; is an additive constant, which can be dropped because
we're optimizing.&lt;/p&gt;
&lt;p&gt;This leaves us with the following optimization problem:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \underset{\theta}{\text{argmin}}\, \textbf{KL}(q_\theta || p) \\
&amp;amp;\qquad = \underset{\theta}{\text{argmin}}\, \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d)
\end{align*}&lt;/div&gt;
&lt;p&gt;Let's work out the gradient
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
&amp;amp; \nabla\left[ \sum_d q_\theta(d) \log q_\theta(d) - \sum_d q_\theta(d) \log \bar{p}(d) \right] \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \log q_\theta(d) \right] - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) \right) - \sum_d \nabla\left[ q_\theta(d) \right] \log \bar{p}(d) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( 1 + \log q_\theta(d) - \log \bar{p}(d) \right) \\
&amp;amp;\qquad = \sum_d \nabla \left[ q_\theta(d) \right] \left( \log q_\theta(d) - \log \bar{p}(d) \right) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;We killed the one in the last equality because &lt;span class="math"&gt;\(\sum_d \nabla
\left[ q(d) \right] = \nabla \left[ \sum_d q(d) \right] = \nabla
\left[ 1 \right] = 0\)&lt;/span&gt;, for any &lt;span class="math"&gt;\(q\)&lt;/span&gt; which is a probability distribution.&lt;/p&gt;
&lt;p&gt;This direction is convenient because we don't need to normalize
&lt;span class="math"&gt;\(p\)&lt;/span&gt;. Unfortunately, the "easy" direction is nonconvex in general&amp;mdash;unlike
the "hard" direction, which (as we'll see shortly) is convex.&lt;/p&gt;
&lt;h3&gt;Harder direction &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;\begin{align*}
\textbf{KL}(p || q_\theta)
&amp;amp;= \sum_d p(d) \log \left( \frac{p(d)}{q(d)} \right) \\
&amp;amp;= \sum_d p(d) \left( \log p(d) - \log q(d) \right) \\
&amp;amp;= \sum_d p(d) \log p(d) - \sum_d p(d) \log q(d) \\
\end{align*}&lt;/div&gt;
&lt;p&gt;Clearly the first term (entropy) won't matter if we're just trying optimize wrt
&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. So, let's focus on the second term (cross-entropy).
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\sum_d p(d) \log q(d)
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \log \left( \bar{q}(d)/Z_q \right) \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \left( \log \bar{q}(d) - \log Z_q \right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log Z_q\right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \left( \log Z_q \right) \left( \frac{1}{Z_p} \sum_d \bar{p}(d)\right) \\
&amp;amp;= \left(\frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d)\right) - \log Z_q
\end{align*}&lt;/div&gt;
&lt;p&gt;The gradient, when &lt;span class="math"&gt;\(q\)&lt;/span&gt; is in the exponential family, is intuitive:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align*}
\nabla \left[ \frac{1}{Z_p} \sum_d \bar{p}(d) \log \bar{q}(d) - \log Z_q \right]
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \nabla \left[ \log \bar{q}(d) \right] - \nabla \log Z_q \\
&amp;amp;= \frac{1}{Z_p} \sum_d \bar{p}(d) \phi_q(d) - \mathbb{E}_q \left[ \phi_q \right] \\
&amp;amp;= \mathbb{E}_p \left[ \phi_q \right] - \mathbb{E}_q \left[ \phi_q \right]
\end{align*}&lt;/div&gt;
&lt;p&gt;Why do we say this is hard to compute? Well, for most interesting models, we
can't compute &lt;span class="math"&gt;\(Z_p = \sum_d \bar{p}(d)\)&lt;/span&gt;. This is because &lt;span class="math"&gt;\(p\)&lt;/span&gt; is presumed to be a
complex model (e.g., the real world, an intricate factor graph, a complicated
Bayesian posterior). If we can't compute &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt;, it's highly unlikely that we can
compute another (nontrivial) integral under &lt;span class="math"&gt;\(\bar{p}\)&lt;/span&gt;, e.g., &lt;span class="math"&gt;\(\sum_d \bar{p}(d)
\log \bar{q}(d)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Nonetheless, optimizing KL in this direction is still useful. Examples include:
expectation propagation, variational decoding, and maximum likelihood
estimation. In the case of maximum likelihood estimation, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the empirical
distribution, so technically you don't have to compute its normalizing constant,
but you do need samples from it, which can be just as hard to get as computing a
normalization constant.&lt;/p&gt;
&lt;p&gt;Optimization problem is &lt;em&gt;convex&lt;/em&gt; when &lt;span class="math"&gt;\(q_\theta\)&lt;/span&gt; is an exponential
family&amp;mdash;i.e., for any &lt;span class="math"&gt;\(p\)&lt;/span&gt; the &lt;em&gt;optimization&lt;/em&gt; problem is "easy." You can
think of maximum likelihood estimation (MLE) as a method which minimizes KL
divergence based on samples of &lt;span class="math"&gt;\(p\)&lt;/span&gt;. In this case, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the true data
distribution! The first term in the gradient is based on a sample instead of an
exact estimate (often called "observed feature counts"). The downside, of
course, is that computing &lt;span class="math"&gt;\(\mathbb{E}_p \left[ \phi_q \right]\)&lt;/span&gt; might not be
tractable or, for MLE, require tons of samples.&lt;/p&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In many ways, optimizing exclusive KL makes no sense at all! Except for the
  fact that it's computable when inclusive KL is often not. Exclusive KL is
  generally regarded as "an approximation" to inclusive KL. This bias in this
  approximation can be quite large.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inclusive vs. exclusive is an important distinction: Inclusive divergences
  require &lt;span class="math"&gt;\(q &amp;gt; 0\)&lt;/span&gt; whenever &lt;span class="math"&gt;\(p &amp;gt; 0\)&lt;/span&gt; (i.e., no "false negatives"), whereas
  exclusive divergences favor a single mode (i.e., only a good fit around a that
  mode).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(q\)&lt;/span&gt; is an exponential family, &lt;span class="math"&gt;\(\textbf{KL}(p || q_\theta)\)&lt;/span&gt; will be convex
  in &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, no matter how complicated &lt;span class="math"&gt;\(p\)&lt;/span&gt; is, whereas &lt;span class="math"&gt;\(\textbf{KL}(q_\theta
  || p)\)&lt;/span&gt; is generally nonconvex (e.g., if &lt;span class="math"&gt;\(p\)&lt;/span&gt; is multimodal).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computing the value of either KL divergence requires normalization. However,
  in the "easy" (exclusive) direction, we can optimize KL without computing
  &lt;span class="math"&gt;\(Z_p\)&lt;/span&gt; (as it results in only an additive constant difference).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both directions of KL are special cases of
  &lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"&gt;&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence&lt;/a&gt;. For a
  unified account of both directions consider looking into &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;-divergence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;I'd like to thank the following people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/_shrdlu_"&gt;Ryan Cotterell&lt;/a&gt; for an email exchange which
  spawned this article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/adveisner"&gt;Jason Eisner&lt;/a&gt; for teaching me all this stuff.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/florian_shkurti"&gt;Florian Shkurti&lt;/a&gt; for a useful email
  discussion, which caugh a bug in my explanation of why inclusive KL is hard to
  compute/optimize.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/sjmielke"&gt;Sebastian Mielke&lt;/a&gt; for the suggesting the
  "inclusive vs. exclusive" figure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 06 Oct 2014 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-10-06:blog/post/2014/10/06/kl-divergence-as-an-objective-function/</guid><category>statistics</category><category>machine-learning</category></item><item><title>Complex-step derivative</title><link>http://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/</link><description>&lt;p&gt;Estimate derivatives by simply passing in a complex number to your function!&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Recall, the centered-difference approximation is a fairly accurate method for
approximating derivatives of a univariate function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, which only requires two
function evaluations. A similar derivation, based on the Taylor series expansion
with a complex perturbation, gives us a similarly-accurate approximation with a
single (complex) function evaluation instead of two (real-valued) function
evaluations. Note: &lt;span class="math"&gt;\(f\)&lt;/span&gt; must support complex inputs (in frameworks, such as numpy
or matlab, this often requires no modification to source code).&lt;/p&gt;
&lt;p&gt;This post is based on
&lt;a href="http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf"&gt;Martins+'03&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;: Start with the Taylor series approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x + i \cdot \varepsilon) =
  \frac{i^0 \varepsilon^0}{0!} f(x)
+ \frac{i^1 \varepsilon^1}{1!} f'(x)
+ \frac{i^2 \varepsilon^2}{2!} f''(x)
+ \frac{i^3 \varepsilon^3}{3!} f'''(x)
+ \cdots
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;Take the imaginary part of both sides and solve for &lt;span class="math"&gt;\(f'(x)\)&lt;/span&gt;. Note: the &lt;span class="math"&gt;\(f\)&lt;/span&gt; and
&lt;span class="math"&gt;\(f''\)&lt;/span&gt; term disappear because &lt;span class="math"&gt;\(i^0\)&lt;/span&gt; and &lt;span class="math"&gt;\(i^2\)&lt;/span&gt; are real-valued.&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) = \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big] + \frac{\varepsilon^2}{3!} f'''(x) + \cdots
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;As usual, using a small &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; let's us throw out higher-order
terms. And, we arrive at the following approximation:&lt;/p&gt;
&lt;div class="math"&gt;$$
f'(x) \approx \frac{1}{\varepsilon} \text{Im}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;If instead, we take the real part and solve for &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, we get an approximation
to the function's value at &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x) \approx \text{Re}\Big[ f(x + i \cdot \varepsilon) \Big]
$$&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;In other words, a single (complex) function evaluations computes both the
function's value and the derivative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Higher-order function takes univariate function which computes a value and&lt;/span&gt;
&lt;span class="sd"&gt;    returns a function which returns value-derivative pair approximation.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;complex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;         &lt;span class="c1"&gt;# convert input to complex number&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imag&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# return function value and gradient&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A simple test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;  &lt;span class="c1"&gt;# function&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# gradient&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;complex_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Other comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Using the complex-step method to estimate the gradients of multivariate
  functions requires independent approximations for each dimension of the
  input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although the complex-step approximation only requires a single function
  evaluation, it's unlikely faster than performing two function evaluations
  because operations on complex numbers are generally much slower than on floats
  or doubles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Check out the
&lt;a href="https://gist.github.com/timvieira/3d3db3e5e78e17cdd103"&gt;gist&lt;/a&gt; for this post.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Thu, 07 Aug 2014 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-08-07:blog/post/2014/08/07/complex-step-derivative/</guid><category>calculus</category></item><item><title>Gumbel max trick and weighted reservoir sampling</title><link>http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</link><description>&lt;p&gt;A while back, &lt;a href="http://people.cs.umass.edu/~wallach/"&gt;Hanna&lt;/a&gt; and I stumbled upon
the following blog post:
&lt;a href="http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm"&gt;Algorithms Every Data Scientist Should Know: Reservoir Sampling&lt;/a&gt;,
which got us excited about reservior sampling.&lt;/p&gt;
&lt;p&gt;Around the same time, I attended a talk by
&lt;a href="http://cs.haifa.ac.il/~tamir/"&gt;Tamir Hazan&lt;/a&gt; about some of his work on
perturb-and-MAP
&lt;a href="http://cs.haifa.ac.il/~tamir/papers/mean-width-icml12.pdf"&gt;(Hazan &amp;amp; Jaakkola, 2012)&lt;/a&gt;,
which is inspired by the
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;Gumbel-max-trick&lt;/a&gt;
(see &lt;a href="/blog/post/2014/07/31/gumbel-max-trick/"&gt;previous post&lt;/a&gt;). The apparent
similarity between weighted reservior sampling and the Gumbel max trick lead us
to make some cute connections, which I'll describe in this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We're given a stream of unnormalized probabilities, &lt;span class="math"&gt;\(x_1,
x_2, \cdots\)&lt;/span&gt;. At any point in time &lt;span class="math"&gt;\(t\)&lt;/span&gt; we'd like to have a sampled index &lt;span class="math"&gt;\(i\)&lt;/span&gt;
available, where the probability of &lt;span class="math"&gt;\(i\)&lt;/span&gt; is given by &lt;span class="math"&gt;\(\pi_t(i) = \frac{x_i}{
\sum_{j=1}^t x_j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume, without loss of generality, that &lt;span class="math"&gt;\(x_i &amp;gt; 0\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;. (If any element
has a zero weight we can safely ignore it since it should never be sampled.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Gumbel-max sampler&lt;/strong&gt;: I came up with the following algorithm, which
is a simple "modification" of the Gumbel-max-trick for handling streaming data:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}  \ \ \text{# maximum value and index}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# Compute log-unnormalized probabilities&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(w_i = \log(x_i)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Additively perturb each weight by a Gumbel random variate&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(z_i \sim \text{Gumbel}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(k_i = w_i + z_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; (i.e. the argmax)&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If we interrupt this algorithm at any point, we have a sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After convincing myself this algorithm was correct, I sat down to try to
understand the algorithm in the blog post, which is due to Efraimidis and
Spirakis (2005) (&lt;a href="http://dl.acm.org/citation.cfm?id=1138834"&gt;paywall&lt;/a&gt;,
&lt;a href="http://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf"&gt;free summary&lt;/a&gt;). They
looked similar in many ways but used different sorting keys / perturbations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efraimidis and Spirakis (2005)&lt;/strong&gt;: Here is the ES algorithm for weighted
reservior sampling&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span class="math"&gt;\(a = -\infty; b = \text{null}\)&lt;/span&gt;&lt;/dt&gt;
&lt;dt&gt;for &lt;span class="math"&gt;\(i=1,2,\cdots;\)&lt;/span&gt; do:&lt;/dt&gt;
&lt;dd&gt;# compute randomized key&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(u_i \sim \text{Uniform}(0,1)\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(e_i = u_i^{(\frac{1}{x_i})}\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;# Keep around the largest &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;if &lt;span class="math"&gt;\(k_i &amp;gt; a\)&lt;/span&gt;:&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ a = k_i\)&lt;/span&gt;&lt;/dd&gt;
&lt;dd&gt;&lt;span class="math"&gt;\(\ \ \ \ b = i\)&lt;/span&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Again, if interrupt this algorithm at any point, we have our sample &lt;span class="math"&gt;\(b\)&lt;/span&gt;. Note
that you can simplify &lt;span class="math"&gt;\(e_i\)&lt;/span&gt; so that you don't have to compute pow (which is nice
because pow is pretty slow). It's equivalent to use &lt;span class="math"&gt;\(e'_i = \log(e_i) =
\log(u_i)/x_i\)&lt;/span&gt; because &lt;span class="math"&gt;\(\log\)&lt;/span&gt; is monotonic. (Note that &lt;span class="math"&gt;\(-e'_i \sim
\textrm{Exponential}(x_i)\)&lt;/span&gt;.)&lt;/p&gt;
&lt;!--
I find this version of the algorithm more intuitive, since it's well-known that
$\left(\underset{{i=1 \ldots t}}{\min} \textrm{Exponential}(x_i) \right) =
\textrm{Exponential}\left(\sum_{i=1}^t x_i \right)$. This version makes it clear
that minimizing is actually summing. However, we want the argmin, which is
distributed according to $\pi_t$.
--&gt;

&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: Let's try to relate these algorithms. At a high level, both
algorithms compute a randomized key and take an argmax. What's the relationship
between the keys?&lt;/p&gt;
&lt;p&gt;First, note that a &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; variate can be generated via
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1)))\)&lt;/span&gt;. This is a straightforward application of
the
&lt;a href="http://en.wikipedia.org/wiki/Inverse_transform_sampling"&gt;inverse transform sampling&lt;/a&gt;
method for random number generation. This means that if we use the same sequence
of uniform random variates then, &lt;span class="math"&gt;\(z_i = -\log(-\log(u_i))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, this does not give use equality between &lt;span class="math"&gt;\(k_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(e_i\)&lt;/span&gt;, but it does
turn out that &lt;span class="math"&gt;\(k_i = -\log(-\log(e_i))\)&lt;/span&gt;, which is useful because this is a
monotonic transformation on the interval &lt;span class="math"&gt;\((0,1)\)&lt;/span&gt;. Since monotonic
transformations preserve ordering, the sequences &lt;span class="math"&gt;\(k\)&lt;/span&gt; and &lt;span class="math"&gt;\(e\)&lt;/span&gt; result in the same
comparison decisions, as well as, the same argmax. In summary, the algorithms
are the same!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extensions&lt;/strong&gt;: After reading a little further along in the ES paper, we see
that the same algorithm can be used to perform &lt;em&gt;sampling without replacement&lt;/em&gt; by
sorting and taking the elements with the highest keys. This same modification is
applicable to the Gumbel-max-trick because the keys have exactly the same
ordering as ES. In practice we don't sort the key, but instead use a bounded
priority queue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closing&lt;/strong&gt;: To the best of my knowledge, the connection between the
Gumbel-max-trick and ES is undocumented. Furthermore, the Gumbel-max-trick is
not known as a streaming algorithm, much less known to perform sampling without
replacement! If you know of a reference let me know. Perhaps, we'll finish
turning these connections into a
&lt;a href="https://github.com/timvieira/gumbel"&gt;short tech report&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Fri, 01 Aug 2014 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-08-01:blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/</guid><category>sampling</category><category>Gumbel</category><category>reservoir-sampling</category></item><item><title>Gumbel max trick</title><link>http://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/</link><description>&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Sampling from a discrete distribution parametrized by unnormalized
log-probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_k = \frac{1}{z} \exp(x_k)   \ \ \ \text{where } z = \sum_{j=1}^K \exp(x_j)
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The usual way&lt;/strong&gt;: Exponentiate and normalize (using the
&lt;a href="/blog/post/2014/02/11/exp-normalize-trick/"&gt;exp-normalize trick&lt;/a&gt;), then use the
an algorithm for sampling from a discrete distribution (aka categorical):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;usual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cdf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;     &lt;span class="c1"&gt;# the exp-normalize trick&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;searchsorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;The Gumbel max trick&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \underset{ i \in \{1,\cdots,K\} }{\operatorname{argmax}} x_i + z_i
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_1 \cdots z_K\)&lt;/span&gt; are i.i.d. &lt;span class="math"&gt;\(\text{Gumbel}(0,1)\)&lt;/span&gt; random variates. It
turns out that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is distributed according to &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. (See the short derivations
in this
&lt;a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/"&gt;blog post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Implementing the Gumbel max trick is remarkable easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gumbel_max_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gumbel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't have access to a Gumbel random variate generator, you can use
&lt;span class="math"&gt;\(-\log(-\log(\text{Uniform}(0,1))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparison&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gumbel max requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; samples from a uniform. Usual requires only &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gumbel is one-pass because it does not require normalization (or a pass to
     compute the max for use in the exp-normalize trick). More on this in a
     later post!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Gumbel max trick requires &lt;span class="math"&gt;\(2K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\log\)&lt;/span&gt;, whereas ordinary
     requires &lt;span class="math"&gt;\(K\)&lt;/span&gt; calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; and &lt;span class="math"&gt;\(\log\)&lt;/span&gt; are expensive
     function, we'd like to avoid calling them. What gives? Well, Gumbel's calls
     to &lt;span class="math"&gt;\(\log\)&lt;/span&gt; do not depend on the data so they can be precomputed; this is
     handy for implementations which rely on vectorization for efficiency,
     e.g. python+numpy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;: I have a few posts relating to the Gumbel-max trick. Have a
look at &lt;a href="/blog/tag/gumbel.html"&gt;posts tagged with Gumbel&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Thu, 31 Jul 2014 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-07-31:blog/post/2014/07/31/gumbel-max-trick/</guid><category>sampling</category><category>Gumbel</category></item><item><title>Rant against grid search</title><link>http://timvieira.github.io/blog/post/2014/07/22/rant-against-grid-search/</link><description>&lt;p&gt;Grid search is a simple and intuitive algorithm for optimizing and/or exploring
the effects of parameters to a function. However, given its rigid definition
grid search is susceptible to degenerate behavior. One type of unfortunate
behavior occurs in the presence of unimportant parameters, which results in many
(potentially expensive) function evaluations being wasted.&lt;/p&gt;
&lt;p&gt;This is a very simple point, but nonetheless I'll illustrate with a simple
example.&lt;/p&gt;
&lt;p&gt;Consider the following simple example, let's find the argmax of &lt;span class="math"&gt;\(f(x,y) = -x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we search over a &lt;span class="math"&gt;\(10\)&lt;/span&gt;-by-&lt;span class="math"&gt;\(10\)&lt;/span&gt; grid, resulting in a total of &lt;span class="math"&gt;\(100\)&lt;/span&gt;
function evaluations. For this function, we expect precision which proportional
of the number of samples in the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dimension, which is only &lt;span class="math"&gt;\(10\)&lt;/span&gt; samples! On
the other hand, randomly sampling points over the same space results in &lt;span class="math"&gt;\(100\)&lt;/span&gt;
samples in every dimension.&lt;/p&gt;
&lt;p&gt;In other words, randomly sample instead of using a rigid grid. If you have
points, which are not uniformly spaced, I'm willing to bet that an appropriate
probability distribution exists.&lt;/p&gt;
&lt;p&gt;This type of problem is common on hyperparameter optimizations. For futher
reading see
&lt;a href="http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"&gt;Bergstra &amp;amp; Bengio (2012)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other thoughts&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Local search is often much more effective. For example, gradient-based
   optimization, Nelder-Mead, stochastic local search, coordinate ascent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grid search tends to produce nicer-looking plots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What about variance in the results? Two things: (a) This is a concern for
   replicability, but is easily remedied by making sampled parameters
   available. (b) There is always some probability that the sampling gives you a
   terrible set of points. This shouldn't be a problem if you use enough
   samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Tue, 22 Jul 2014 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-07-22:blog/post/2014/07/22/rant-against-grid-search/</guid><category>misc</category></item><item><title>Expected value of a quadratic and the Delta method</title><link>http://timvieira.github.io/blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</link><description>&lt;p&gt;&lt;strong&gt;Expected value of a quadratic&lt;/strong&gt;: Suppose we'd like to compute the expectation
of a quadratic function, i.e.,
&lt;span class="math"&gt;\(\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]\)&lt;/span&gt; , where &lt;span class="math"&gt;\(x\)&lt;/span&gt; is
a random vector and &lt;span class="math"&gt;\(A\)&lt;/span&gt; is deterministic &lt;em&gt;symmetric&lt;/em&gt; matrix. Let &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and
&lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; be the mean and variance of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. It turns out the expected value of a
quadratic has the following simple form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{E}\left[ x^{\top}\negthinspace\negthinspace A x \right]
=
\text{trace}\left( A \Sigma \right) + \mu^{\top}\negthinspace A \mu
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Delta Method&lt;/strong&gt;: Suppose we'd like to compute expected value of a nonlinear
function &lt;span class="math"&gt;\(f\)&lt;/span&gt; applied our random variable &lt;span class="math"&gt;\(x\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\mathbb{E}\left[ f(x) \right]\)&lt;/span&gt;. The Delta method approximates this expection by
replacing &lt;span class="math"&gt;\(f\)&lt;/span&gt; by it's second-order Talylor approximation &lt;span class="math"&gt;\(\hat{f_{a}}\)&lt;/span&gt; taken at
some point &lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{f_{a}}(x) = f(a) + \nabla f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^\top H(a) (x - a)
$$&lt;/div&gt;
&lt;p&gt;The expectation of this Talyor approximation is a quadratic function! Let's try
to apply our new equation for the expected value of quadratic. We can use the
trick from above with &lt;span class="math"&gt;\(A=H(a)\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = (x-a)\)&lt;/span&gt;. Note, covariance matrix is shift
invariant and the Hessian is a symmetric matrix!&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\mathbb{E}\left[ \hat{f_{a}}(x) \right]
 &amp;amp; = \mathbb{E} \left[ f(a) + \nabla\negthinspace f(a)^{\top} (x - a) + \frac{1}{2} (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) + \frac{1}{2} \mathbb{E} \left[ (x - a)^{\top} H(a)\, (x - a) \right] \\\
 &amp;amp; = f(a) + \nabla\negthinspace f(a)^{\top} ( \mu - a ) +
   \frac{1}{2}\left( \text{trace}\left( H(a) \, \Sigma \right) + (\mu - a)^{\top} H(a)\, (\mu - a) \right)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;It's common to take the Taylor expansion around &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. This simplifies the equation&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\mathbb{E}\left[ \hat{f_{\mu}} (x) \right]
&amp;amp;= \mathbb{E}\left[ f(\mu) + \nabla\negthinspace f(\mu) (x - \mu) + \frac{1}{2} (x - \mu)^{\top} H(\mu)\, (x - \mu) \right] \\\
&amp;amp;= f(\mu) + \frac{1}{2} \, \text{trace}\Big( H(\mu) \, \Sigma \Big)
\end{aligned}&lt;/div&gt;
&lt;p&gt;That looks much more tractable! Error bounds are possible to derive, but outside
to scope of this post. For a nice use of the delta method in machine learning
see &lt;a href="http://arxiv.org/pdf/1307.1493v2.pdf"&gt;(Wager+,'13)&lt;/a&gt; and
&lt;a href="http://cs.jhu.edu/~jason/papers/smith+eisner.acl06-risk.pdf"&gt;(Smith &amp;amp; Eisner,'06)&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 21 Jul 2014 00:00:00 -0400</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-07-21:blog/post/2014/07/21/expected-value-of-a-quadratic-and-the-delta-method/</guid><category>statistics</category></item><item><title>Visualizing high-dimensional functions with cross-sections</title><link>http://timvieira.github.io/blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</link><description>&lt;p&gt;Last September, I gave a talk which included a bunch of two-dimensional plots of
a high-dimensional objective I was developing specialized algorithms for
optimizing. A month later, at least three of my colleagues told me that my plots
had inspired them to make similar plots. The plotting trick is really simple and
not original, but nonetheless I'll still write it up for all to enjoy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example plot&lt;/strong&gt;: This image shows cross-sections of two related functions: a
non-smooth (black) and a smooth approximating function (blue). The plot shows
that the approximation is faithful to the overall shape, but sometimes
over-smooths. In this case, we miss the maximum, which happens near the middle
of the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/blog/images/cross-section.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;: Let &lt;span class="math"&gt;\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)&lt;/span&gt; be a high-dimensional
function (&lt;span class="math"&gt;\(d \gg 2\)&lt;/span&gt;), which you'd like to visualize. Unfortunately, you are like
me and can't see in high-dimensions what do you do?&lt;/p&gt;
&lt;p&gt;One simple thing to do is take a nonzero vector &lt;span class="math"&gt;\(\boldsymbol{d} \in
\mathbb{R}^d\)&lt;/span&gt;, take a point of interest &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;, and build a local
picture of &lt;span class="math"&gt;\(f\)&lt;/span&gt; by evaluating it at various intervals along the chosen direction
as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$
f_i = f(\boldsymbol{x} + \alpha_i \ \boldsymbol{d}) \ \ \text{for } \alpha_i \in [\alpha_\min, \alpha_\max]
$$&lt;/div&gt;
&lt;p&gt;Of course, you'll have to pick a reasonable range and discretize it. Note,
&lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; are fixed for all &lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;. Now, you can
plot &lt;span class="math"&gt;\((\alpha_i,f_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Picking directions&lt;/strong&gt;: There are many alternatives for picking
&lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;, my favorites are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Gradient (if it exists), this direction is guaranteed to show a local
    increase/decrease in the objective, unless it's zero.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Coordinate vectors. Varying one dimension per plot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random. I recommend directions drawn from a spherical
    Gaussian.&lt;sup id="fnref:sphericalgaussian"&gt;&lt;a class="footnote-ref" href="#fn:sphericalgaussian" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; The reason being that such a vector is
    uniformly distributed across all unit-length directions (i.e., the angle of
    the vector, not it's length). We will vary the length ourselves via
    &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. It's probably best that our plots don't randomly vary in scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Extension to 3d&lt;/strong&gt;: It's pretty easy to extend this generating 3d plots by
using 2 vectors, &lt;span class="math"&gt;\(\boldsymbol{d_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{d_2}\)&lt;/span&gt;, and varying two
parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
f(\boldsymbol{x} + \alpha \ \boldsymbol{d_1} + \beta \ \boldsymbol{d_2})
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Closing remarks&lt;/strong&gt;: These types of plots are probably best used to: empirically
verify/explore properties of an objective function, compare approximations, test
sensitivity to certain parameters/hyperparameters, visually debug optimization
algorithms.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:sphericalgaussian"&gt;
&lt;p&gt;More formally, vectors drawn from a spherical Gaussian are
points uniformly distributed on the surface of a &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional unit sphere,
&lt;span class="math"&gt;\(\mathbb{S}^d\)&lt;/span&gt;. Sampling a vector from a spherical Gaussian is straightforward:
sample &lt;span class="math"&gt;\(\boldsymbol{d'} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})\)&lt;/span&gt;,
&lt;span class="math"&gt;\(\boldsymbol{d} = \boldsymbol{d'} / \| \boldsymbol{d'} \|_2\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:sphericalgaussian" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Wed, 12 Feb 2014 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-02-12:blog/post/2014/02/12/visualizing-high-dimensional-functions-with-cross-sections/</guid><category>visualization</category></item><item><title>Exp-normalize trick</title><link>http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/</link><description>&lt;p&gt;This trick is the very close cousin of the infamous log-sum-exp trick
(&lt;a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.logsumexp.html"&gt;scipy.misc.logsumexp&lt;/a&gt;),&lt;/p&gt;
&lt;p&gt;Supposed you'd like to evaluate a probability distribution &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt;
parametrized by a vector &lt;span class="math"&gt;\(\boldsymbol{x} \in \mathbb{R}^n\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i = \frac{ \exp(x_i) }{ \sum_{j=1}^n \exp(x_j) }
$$&lt;/div&gt;
&lt;p&gt;The exp-normalize trick leverages the following identity to avoid numerical
overflow. For any &lt;span class="math"&gt;\(b \in \mathbb{R}\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$
\pi_i
= \frac{ \exp(x_i - b) \exp(b) }{ \sum_{j=1}^n \exp(x_j - b) \exp(b) }
= \frac{ \exp(x_i - b) }{ \sum_{j=1}^n \exp(x_j - b) }
$$&lt;/div&gt;
&lt;p&gt;In other words, the &lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is shift-invariant. A reasonable choice
is &lt;span class="math"&gt;\(b = \max_{i=1}^n x_i\)&lt;/span&gt;. With this choice, overflow due to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt; is
impossible&lt;span class="math"&gt;\(-\)&lt;/span&gt;the largest number exponentiated after shifting is &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exp-normalize v. log-sum-exp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If what you want to remain in log-space, that is, compute
&lt;span class="math"&gt;\(\log(\boldsymbol{\pi})\)&lt;/span&gt;, you should use logsumexp. However, if
&lt;span class="math"&gt;\(\boldsymbol{\pi}\)&lt;/span&gt; is your goal, then exp-normalize trick is for you! Since it
avoids additional calls to &lt;span class="math"&gt;\(\exp\)&lt;/span&gt;, which would be required if using log-sum-exp
and more importantly exp-normalize is more numerically stable!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log-sum-exp for computing the log-distibution&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log \pi_i = x_i - \mathrm{logsumexp}(\boldsymbol{x})
$$&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathrm{logsumexp}(\boldsymbol{x}) = b + \log \sum_{j=1}^n \exp(x_j - b)
$$&lt;/div&gt;
&lt;p&gt;Typically with the same choice for &lt;span class="math"&gt;\(b\)&lt;/span&gt; as above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numerically-stable sigmoid function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sigmoid function can be computed with the exp-normalize trick in order to
avoid numerical overflow. In the case of &lt;span class="math"&gt;\(\text{sigmoid}(x)\)&lt;/span&gt;, we have a
distribution with unnormalized log probabilities &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt;, where we are only
interested in the probability of the first event. From the exp-normalize
identity, we know that the distributions &lt;span class="math"&gt;\([x,0]\)&lt;/span&gt; and &lt;span class="math"&gt;\([0,-x]\)&lt;/span&gt; are equivalent (to
see why, plug in &lt;span class="math"&gt;\(b=\max(0,x)\)&lt;/span&gt;). This is why sigmoid is often expressed in one
of two equivalent ways:&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{sigmoid}(x) = 1/(1+\exp(-x)) = \exp(x) / (\exp(x) + 1)
$$&lt;/div&gt;
&lt;p&gt;Interestingly, each version covers an extreme case: &lt;span class="math"&gt;\(x=\infty\)&lt;/span&gt; and &lt;span class="math"&gt;\(x=-\infty\)&lt;/span&gt;,
respectively. Below is some python code which implements the trick:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Numerically-stable sigmoid function.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# if x is less than zero then z will be small, denom can&amp;#39;t be&lt;/span&gt;
        &lt;span class="c1"&gt;# zero because it&amp;#39;s 1+z.&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Tue, 11 Feb 2014 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-02-11:blog/post/2014/02/11/exp-normalize-trick/</guid><category>numerical</category></item><item><title>Gradient-vector product</title><link>http://timvieira.github.io/blog/post/2014/02/10/gradient-vector-product/</link><description>&lt;p&gt;We've all written the following test for our gradient code (known as the
finite-difference approximation).&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial x_i} f(\boldsymbol{x}) \approx
 \frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{e_i})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{e_i})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varepsilon &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt; is a vector of zeros except at
&lt;span class="math"&gt;\(i\)&lt;/span&gt; where it is &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This approximation is exact in the limit, and accurate to
&lt;span class="math"&gt;\(o(\varepsilon^2)\)&lt;/span&gt; additive error.&lt;/p&gt;
&lt;p&gt;This is a specific instance of a more general approximation! The dot product of
the gradient and any (conformable) vector &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; can be approximated
with the following formula,&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla f(\boldsymbol{x})^{\top} \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
   f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
 - f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
 \Big)
$$&lt;/div&gt;
&lt;p&gt;We get the special case above when &lt;span class="math"&gt;\(\boldsymbol{d}=\boldsymbol{e_i}\)&lt;/span&gt;. This also
exact in the limit and just as accurate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Runtime?&lt;/strong&gt; Finite-difference approximation is probably too slow for
  approximating a high-dimensional gradient because the number of function
  evaluations required is &lt;span class="math"&gt;\(2 n\)&lt;/span&gt; where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the dimensionality of &lt;span class="math"&gt;\(x\)&lt;/span&gt;. However,
  if the end goal is to approximate a gradient-vector product, a mere &lt;span class="math"&gt;\(2\)&lt;/span&gt;
  function evaluations is probably faster than specialized code for computing
  the gradient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to set &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;?&lt;/strong&gt; The second approach is more sensitive to
  &lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; because &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; is arbitrary, unlike
  &lt;span class="math"&gt;\(\boldsymbol{e_i}\)&lt;/span&gt;, which is a simple unit-norm vector. Luckily some guidance
  is available. Andrei (2009) reccommends&lt;/p&gt;
&lt;div class="math"&gt;$$
\varepsilon = \sqrt{\epsilon_{\text{mach}}} (1 + \|\boldsymbol{x} \|_{\infty}) / \| \boldsymbol{d} \|_{\infty}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\epsilon_{\text{mach}}\)&lt;/span&gt; is
&lt;a href="http://en.wikipedia.org/wiki/Machine_epsilon"&gt;machine epsilon&lt;/a&gt;. (Numpy users:
&lt;code&gt;numpy.finfo(x.dtype).eps&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Why do I care?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Well, I tend to work on sparse, but high-dimensional problems where
   finite-difference would be too slow. Thus, my usual solution is to only test
   several randomly selected dimensions&lt;span class="math"&gt;\(-\)&lt;/span&gt;biasing samples toward dimensions
   which should be nonzero. With the new trick, I can effectively test more
   dimensions at once by taking random vectors &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt;. I recommend
   sampling &lt;span class="math"&gt;\(\boldsymbol{d}\)&lt;/span&gt; from a spherical Gaussian so that we're uniform on
   the angle of the vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sometimes the gradient-vector dot product is the end goal. This is the case
   with Hessian-vector products, which arises in many optimization algorithms,
   such as stochastic meta descent. Hessian-vector products are an instance of
   the gradient-vector dot product because the Hessian is just the gradient of
   the gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Hessian-vector product&lt;/h2&gt;
&lt;p&gt;Hessian-vector products are an instance of the gradient-vector dot product
because since the Hessian is just the gradient of the gradient! Now you only
need to remember one formula!&lt;/p&gt;
&lt;div class="math"&gt;$$
H(\boldsymbol{x})\, \boldsymbol{d} \approx
\frac{1}{2 \varepsilon} \Big(
  \nabla f(\boldsymbol{x} + \varepsilon \cdot \boldsymbol{d})
- \nabla f(\boldsymbol{x} - \varepsilon \cdot \boldsymbol{d})
\Big)
$$&lt;/div&gt;
&lt;p&gt;With this trick you never have to actually compute the gnarly Hessian! More on
&lt;a href="http://justindomke.wordpress.com/2009/01/17/hessian-vector-products/"&gt;Justin Domke's blog&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vieira</dc:creator><pubDate>Mon, 10 Feb 2014 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:timvieira.github.io,2014-02-10:blog/post/2014/02/10/gradient-vector-product/</guid><category>calculus</category></item></channel></rss>