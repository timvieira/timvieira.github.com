<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Hot take: Expectation maximization &mdash; Graduate Descent</title>

  <link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap"
        rel="stylesheet">

  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="robots" content="noindex, nofollow" />

    <link href="https://timvieira.github.io/blog/favicon.png" rel="icon">

  <link href="https://timvieira.github.io/blog/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://timvieira.github.io/blog/">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <!--
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Feed</a></li>
  -->
</ul>


<ul class="main-navigation">
    <li><a href="https://timvieira.github.io/">About</a></li>
    <li><a href="/blog/index.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Hot take: Expectation maximization</h1>
    <p class="meta">
<time datetime="2019-11-20T00:00:00-05:00" pubdate>Nov 20, 2019</time>  <span class="byline author vcard">
    by <span class="fn">
        Tim Vieira
    </span>
  </span>
  <!--
<time datetime="2019-11-20T00:00:00-05:00" pubdate>Nov 20, 2019</time>  <span class="categories">
    <a class='category' href='https://timvieira.github.io/blog/category/misc.html'>misc</a>
  </span>
  -->

  <span class="tags">
    <a class="tag" href="https://timvieira.github.io/blog/tag/machine-learning.html">machine-learning</a><!--,-->    <a class="tag" href="https://timvieira.github.io/blog/tag/statistics.html">statistics</a><!--,-->    <a class="tag" href="https://timvieira.github.io/blog/tag/hot-take.html">hot-take</a>  </span>
    </p>
</header>

  <div class="entry-content"><div class="math">$$
\newcommand{\ind}[1]{\boldsymbol{1}\left[ #1 \right]}
\newcommand{\defeq}[0]{\overset{\scriptsize\text{def}}{=}}
$$</div>
<p>The expectation maximization algorithm (EM) is a poorly understood and poorly
explained.</p>
<ul>
<li>
<p>Explanations are littered with distracting model-specific details and messy
   variational inference notation that make it hard to see the signal in the
   noise.<sup id="sf-hot-take-expectation-maximization-1-back"><a href="#sf-hot-take-expectation-maximization-1" class="simple-footnote" title="Why such a messy presentation?  My guess is that it is      because the theoretical foundations for EM are based on an iterative      lower-bound maximization algorithm.  The theory is bleeding into the      presentation.  This view is useful because they tie EM to a theoretical      optimization foundation that is used to prove convergence properties, but      at the end of the day they are just some dude's hot take.">1</a></sup></p>
</li>
<li>
<p>Today, I will give my hot take.  I will create a bridge between EM and MLE,
   which I believe makes it easier to see the signal in the noise.</p>
</li>
</ul>
<!--
   There are no
   KL divergences, lower bounds, or convergence proofs to distract from what's
   going on.
   -->

<p>EM is an algorithm for maximum-likelihood estimation when we have "incomplete
observations."  What that means is that there is an underlying i.i.d. process
<span class="math">\(\{ X_i \}_{i=1}^n\)</span>, which we do not get to fully observe.  Instead, we see an
<strong>incomplete observation</strong>, which are (generally speaking) <em>constraints</em> on what
<span class="math">\(X_i\)</span> might have been if only we had actually observed it.  In other words, we
do not observe <span class="math">\(X_i\)</span>, we observe that <span class="math">\(g(X_i)\)</span> is true.  We assume knowledge of
the function, <span class="math">\(g\)</span> and that <span class="math">\(g(x) \in \{0, 1\}\)</span> for all <span class="math">\(x \in
\mathrm{domain}(X)\)</span>.</p>
<p>Consider the following examples:</p>
<ol>
<li>
<p>Complete observation: <span class="math">\(g(X_i) = \ind{X_i = x_i}\)</span> where <span class="math">\(\ind{\cdot}\)</span> is the
   indicator function.</p>
</li>
<li>
<p>Incomplete, interval observation: be <span class="math">\(g(X_i) = \ind{a_i \le X_i &lt; b_i}\)</span> for some
   constants <span class="math">\(a_i &lt; b_i\)</span>.  We recover the complete case as <span class="math">\(a_i\)</span> approachs <span class="math">\(b_i\)</span>
   for all <span class="math">\(i\)</span>.<sup id="sf-hot-take-expectation-maximization-2-back"><a href="#sf-hot-take-expectation-maximization-2" class="simple-footnote" title="Interval observations are used in estimation for censored      observations.">2</a></sup></p>
</li>
<li>
<p>Incomplete, subset observations: we could observe non-empty subsets,
   <span class="math">\(\mathcal{X}_i\)</span>, of <span class="math">\(X\)</span>'s domain, <span class="math">\(\ind{X_i \in \mathcal{X}_i}\)</span>.</p>
</li>
<li>
<p><span class="math">\(\ind{y_i = f(X_i)}\)</span> for a function <span class="math">\(f\)</span> that is not necessarily invertible.</p>
</li>
</ol>
<p>There are tons of weird incomplete observations types (i.e., families of <span class="math">\(g\)</span>
functions).  Every choice will result is some set of details that you will have
to work out in order to accomodate.  For example, to accommodate intervals we
can use the cumulative distribution function, <span class="math">\(F(\cdot; \theta)\)</span>, <span class="math">\(p(a_i \le X &lt;
b_i) = F(b_i; \theta) \cdot (1 - F(a_i; \theta))\)</span>.<sup id="sf-hot-take-expectation-maximization-3-back"><a href="#sf-hot-take-expectation-maximization-3" class="simple-footnote" title="To support \(a_i = b_i\), you swap in the pdf via a piecewise function when \(a_i = b_i\) happens to be observed.">3</a></sup></p>
<p>The obvious thing to optimize is the <strong>incomplete log-likelihood</strong>
</p>
<div class="math">$$
\mathcal{L}(\theta) \defeq \sum_{i=1}^n \log p_\theta( g(X_i) ) = \sum_{i=1}^n
\log \sum_{x \in \mathcal{X} } p_\theta(x) g(x)
$$</div>
<p>We can often optimize <span class="math">\(\mathcal{L}\)</span> directly with gradient-based optimization
method.  However, <span class="math">\(\mathcal{L}\)</span> is generally nonconvex so we will only get
locally optimal approximation.  The EM algorithm is an different type of search
algorithm which has different convergence guarantees.</p>
<p>EM is based on a chicken and egg type of story:</p>
<ol>
<li>
<p>if we had the complete data observations, we'd get <span class="math">\(\theta\)</span> via MLE.</p>
</li>
<li>
<p>if we had <span class="math">\(\theta\)</span>, we could get complete the data by sampling from the
    model (or, more generally, using the model's distribution over it).</p>
</li>
</ol>
<p>That's basically what EM is going to do:</p>
<p>initialize: guess model parameters, \theta.<sup id="sf-hot-take-expectation-maximization-4-back"><a href="#sf-hot-take-expectation-maximization-4" class="simple-footnote" title='Alternatively, one can   initialize with a guess of the data completions and reorder the two steps   below "M-E" rather than "E-M."'>4</a></sup></p>
<p>repeat until some convergence criterion is met</p>
<ul>
<li>
<p>E step: generate fake complete data: use the model to complete that data in
    a manner which is consistent with the observations.</p>
</li>
<li>
<p>M step: re-fit the model to that fake data,</p>
</li>
</ul>
<p>Consider a slightly dumbed down version of EM, called Monte Carlo EM (MCEM).</p>
<p>MCEM just (iteratively) fills in the missing information about the random
variables by sampling what the complete value were based on the current estimate
of the model <span class="math">\(\widehat{\theta}\)</span> (sometimes called bootstrapping).</p>
<ul>
<li>
<p>E-step: gives us "complete" dataset that satisfies the observed constraints,
  for each <span class="math">\(i\)</span> sample <span class="math">\(\widehat{x}_i \sim  p_{\widehat{\theta}}(\cdot \mid g(X))\)</span>.</p>
</li>
<li>
<p>M step: compute fully observed maximum-likelihood estimate of <span class="math">\(\theta\)</span> on the
  approximate data set <span class="math">\(\{ \widehat{x}_i \}_{i=1}^n\)</span>.</p>
</li>
</ul>
<p>From MCEM to EM: We could take more than one sample per <span class="math">\(i\)</span> in the E step as
long as we take an equal number per example (in expectation).  If we took
infinitely many samples, we match what traditional EM does.  In particular,
traditional EM uses the complete distribution, <span class="math">\(q_i\)</span>, when it is tractible to do
so.  In other words, rather than sampling from <span class="math">\(p_{\widehat{\theta}}(\cdot \mid
g(X))\)</span>, we use the distribution in the M step to reduce the approximation error.</p>
<p>Using the fully distribution introduces some ugly notion that hides the signal.
It is also more implementation: you have to extend the M-step to compute the
expectation over complete data sets.  The sampling version conveniently uses the
exact same code as fully observed MLE (i.e., a dataset of <span class="math">\(\{ x_i \}_{i=1}^n\)</span>).</p>
<p>Leveraging the distribution is generally an efficiency win so it is worth doing
if you need the extra accuracy (no sampling error that results from the
approximate E step) and efficiency (no need to take lots of samples).  That
said, the MCEM algorithm is a pretty good algorithm since it is very fast to
implement.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><ol class="simple-footnotes"><li id="sf-hot-take-expectation-maximization-1">Why such a messy presentation?  My guess is that it is
     because the theoretical foundations for EM are based on an iterative
     lower-bound maximization algorithm.  The theory is bleeding into the
     presentation.  This view is useful because they tie EM to a theoretical
     optimization foundation that is used to prove convergence properties, but
     at the end of the day they are just some dude's hot take. <a href="#sf-hot-take-expectation-maximization-1-back" class="simple-footnote-back">↩</a></li><li id="sf-hot-take-expectation-maximization-2">Interval observations are used in estimation for censored
     observations. <a href="#sf-hot-take-expectation-maximization-2-back" class="simple-footnote-back">↩</a></li><li id="sf-hot-take-expectation-maximization-3">To support <span class="math">\(a_i = b_i\)</span>, you swap in the pdf via a piecewise function when <span class="math">\(a_i = b_i\)</span> happens to be observed. <a href="#sf-hot-take-expectation-maximization-3-back" class="simple-footnote-back">↩</a></li><li id="sf-hot-take-expectation-maximization-4">Alternatively, one can
  initialize with a guess of the data completions and reorder the two steps
  below "M-E" rather than "E-M." <a href="#sf-hot-take-expectation-maximization-4-back" class="simple-footnote-back">↩</a></li></ol></div>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://timvieira.github.io/blog/post/2021/03/18/on-the-distribution-functions-of-order-statistics/">On the Distribution Functions of Order Statistics</a>
      </li>
      <li class="post">
          <a href="https://timvieira.github.io/blog/post/2020/06/30/animation-of-the-inverse-transform-method/">Animation of the inverse transform method</a>
      </li>
      <li class="post">
          <a href="https://timvieira.github.io/blog/post/2020/06/30/generating-truncated-random-variates/">Generating truncated random variates</a>
      </li>
      <li class="post">
          <a href="https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/">Algorithms for sampling without replacement</a>
      </li>
      <li class="post">
          <a href="https://timvieira.github.io/blog/post/2019/09/06/the-restart-acceleration-trick-a-cure-for-the-heavy-tail-of-wasted-time/">The restart acceleration trick: A cure for the heavy tail of wasted time</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="https://timvieira.github.io/blog/tag/swor.html">swor</a>,    <a href="https://timvieira.github.io/blog/tag/statistics.html">statistics</a>,    <a href="https://timvieira.github.io/blog/tag/notebook.html">notebook</a>,    <a href="https://timvieira.github.io/blog/tag/sampling.html">sampling</a>,    <a href="https://timvieira.github.io/blog/tag/algorithms.html">algorithms</a>,    <a href="https://timvieira.github.io/blog/tag/sampling-without-replacement.html">sampling-without-replacement</a>,    <a href="https://timvieira.github.io/blog/tag/gumbel.html">Gumbel</a>,    <a href="https://timvieira.github.io/blog/tag/decision-making.html">decision-making</a>,    <a href="https://timvieira.github.io/blog/tag/reservoir-sampling.html">reservoir-sampling</a>,    <a href="https://timvieira.github.io/blog/tag/optimization.html">optimization</a>,    <a href="https://timvieira.github.io/blog/tag/rl.html">rl</a>,    <a href="https://timvieira.github.io/blog/tag/machine-learning.html">machine-learning</a>,    <a href="https://timvieira.github.io/blog/tag/calculus.html">calculus</a>,    <a href="https://timvieira.github.io/blog/tag/automatic-differentiation.html">automatic-differentiation</a>,    <a href="https://timvieira.github.io/blog/tag/implicit-function-theorem.html">implicit-function-theorem</a>,    <a href="https://timvieira.github.io/blog/tag/lagrange-multipliers.html">Lagrange-multipliers</a>,    <a href="https://timvieira.github.io/blog/tag/testing.html">testing</a>,    <a href="https://timvieira.github.io/blog/tag/counterfactual-reasoning.html">counterfactual-reasoning</a>,    <a href="https://timvieira.github.io/blog/tag/importance-sampling.html">importance-sampling</a>,    <a href="https://timvieira.github.io/blog/tag/datastructures.html">datastructures</a>,    <a href="https://timvieira.github.io/blog/tag/incremental-computation.html">incremental-computation</a>,    <a href="https://timvieira.github.io/blog/tag/data-structures.html">data-structures</a>,    <a href="https://timvieira.github.io/blog/tag/rant.html">rant</a>,    <a href="https://timvieira.github.io/blog/tag/hyperparameter-optimization.html">hyperparameter-optimization</a>,    <a href="https://timvieira.github.io/blog/tag/numerical.html">numerical</a>,    <a href="https://timvieira.github.io/blog/tag/crf.html">crf</a>,    <a href="https://timvieira.github.io/blog/tag/deep-learning.html">deep-learning</a>,    <a href="https://timvieira.github.io/blog/tag/structured-prediction.html">structured-prediction</a>,    <a href="https://timvieira.github.io/blog/tag/visualization.html">visualization</a>  </section>



<section>
    <a href="http://twitter.com/xtimv" class="twitter-follow-button" data-show-count="true">Follow @xtimv</a>
</section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014&ndash;2021  Tim Vieira &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://timvieira.github.io/blog/theme/js/modernizr-2.0.js"></script>
  <script src="https://timvieira.github.io/blog/theme/js/ender.js"></script>
  <script src="https://timvieira.github.io/blog/theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'graduatedescent';
    var disqus_identifier = 'https://timvieira.github.io/blog/drafts/hot-take-expectation-maximization.html';
    var disqus_url = 'https://timvieira.github.io/blog/drafts/hot-take-expectation-maximization.html';
    var disqus_title = 'Hot take: Expectation maximization';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>

<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-8781169-1', 'auto');
  ga('send', 'pageview');
</script>
-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-8781169-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-8781169-1');
</script>

</body>
</html>