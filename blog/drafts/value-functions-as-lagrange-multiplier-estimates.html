<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Value functions as Lagrange multiplier estimates &mdash; Graduate Descent</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="robots" content="noindex, nofollow" />

    <link href="../favicon.png" rel="icon">

  <link href="../theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="../">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/index.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Value functions as Lagrange multiplier estimates</h1>
    <p class="meta">
<time datetime="2019-07-17T00:00:00-04:00" pubdate>Jul 17, 2019</time>    </p>
</header>

  <div class="entry-content"><p><a href="https://en.wikipedia.org/wiki/Bellman_equation">Value functions</a>, or some
variant thereof, are key concept in sequential decision-making tasks (e.g.,
reinforcement learning, planning under uncertaintly, and optimal control).  They
generally regarded as (somewhat intuitive) definitions that seem to help solve
the decision-making problem.  In this post, I will give an account of value
functions as Lagrange multiplier estimates for a specific formulation of the
policy-search problem in reinforcement learning.  This connection is pretty cool
and is closely related to my previous post on
<a href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">backpropagation and Lagrangians</a>.</p>
<p><strong>TODO</strong> this post suggests that a "policy evaluation" is not necessary: there
  are plenty of other strategies to optimize a policy.  Within the RL literature
  there is a prevalence of two-timescale methods that allow the policy updates
  and policy evaluations to be co-optimized under a careful choice to
  two-timescale step-size sequences for each problem.</p>
<p>My other post suggested that backpropagation is also not necessary.  In both
  cases, they simply "good strategies" because they can compute some
  intermediate quantities in closed form (in particular, gradients of the primal
  problem, by setting certain Lagrangian gradients to zero).  In the RL case,
  this efficiency argument is less strong because these quantities are estimated
  by sampling and have some error that can compound.  THis it is less clear that
  we should bother with the intermediate step.  Recent algorithms based on the
  primal-dual view are starting to take off (for example, SBEED, boosted actor
  with dual critic, appropos).</p>
<div style="border: thin solid black; padding: 10px; background-color: #ffffcc;
margin-bottom: 1.5em;">

The ideal version of this work would be a completed
commutation diagram of the primal and dual views of the MDP optimization problem
much like the diagrams in consumer theory.[^LM04]

I am confident that every quantity will have many connections (each one
analogous to their "cousin" in economics).  This diagram will be an elegant,
unifying view of lots of concepts in MDPs.

Making the implicit functions explicit will make a lot of shortcuts simplier as
well as providing calculus views of policy gradients (for example) than the
usual expectations views.

I am confident that all connections are "interesting" in one way or another.
I am also confident that if we discover a new one, then that's even more interesting.

These connections should even cover policy gradients and successor
representations!
Any properties of the mappings will be of interest as well.
For example, the nonconvexity of the VF polytope as the shadow of the optimal
value function polytope.  It is also interesting that the mapping is not
generally convex.

What are the interesting plots for MDPs?  (analogues of income-consumption
curves) The VF-polytope "line theorem" is like a econ theorem.
</div>

<!--
They are a mathematical operationalization of the idea that

> “Life can only be understood backwards; but it must be lived forwards.”
>
> ― [Søren Kierkegaard](https://www.goodreads.com/quotes/6812-life-can-only-be-understood-backwards-but-it-must-be)
-->

<p>Let <span class="math">\(M = \langle \mathcal{S}, \mathcal{A}, p_0(s), p(s' \mid s, a), r(s,a), \gamma \rangle\)</span>
be a Markov decision process over states <span class="math">\(S\)</span> and actions <span class="math">\(A\)</span> with <span class="math">\(0 \le \gamma &lt; 1\)</span>.  Let <span class="math">\(p_0(s)\)</span> be a
distribution over initial states and <span class="math">\(p(s' \mid s, a)\)</span> be a transition kernel
that describes how the next state <span class="math">\(s'\)</span> evolve from current state <span class="math">\(s\)</span> given an action <span class="math">\(a\)</span>
from a policy <span class="math">\(\pi(a \mid s)\)</span>.  Lastly, let <span class="math">\(r(s,a)\)</span> denote the immediate reward of taking action <span class="math">\(a\)</span> in state <span class="math">\(s\)</span>.</p>
<p>The utility of a policy <span class="math">\(\pi\)</span> is typically measured as the long-term, <span class="math">\(\gamma\)</span>-discounted reward
</p>
<div class="math">$$
U(\pi) \overset{\text{def}}{=} \mathbb{E}\left[  \sum_{t=0}^\infty \gamma^t \!\cdot\! r(s,a) \right]
$$</div>
<p>where the randomness in the expecation is taken over trajectories, <span class="math">\(\langle
\langle s_t, a_t, r_t \rangle \rangle_{t=0}^\infty\)</span>.  The next step is to
rewrite <span class="math">\(U(\pi)\)</span> as an expectation over state-visitation fequencies <span class="math">\(\delta(s)\)</span>,
also know as the occupancy measure, rather than an expectation over infinitely
long trajectories.<sup id="fnref-unnecessary-constraints"><a class="footnote-ref" href="#fn-unnecessary-constraints">1</a></sup></p>
<div class="math">$$
(1-P_{\pi})^{-1} p_0 (1-\gamma) = \lim_{T \rightarrow \infty} \sum_{t=1}^T P_{\pi}^{t} p_0 (1-\gamma)
$$</div>
<p>XXX: this explanation is incomplete. Need to define <span class="math">\(P_{\pi}\)</span>.  Need to be
careful about the definition.  It's easy to miss a transpose.</p>
<div class="math">$$
\begin{cases}
\underset{\pi, \delta}{\textrm{maximize }} &amp; \sum_{s,a} r(s,a) \cdot \delta(s) \pi(a \mid s) \\
\text{subject to } &amp;
\sum_{a'} \delta(s') \pi(a' \mid s') = (1-\gamma) p_0(s') + \gamma \sum_{s,a} \delta(s) \pi(a \mid s) \cdot P(s' \mid s,a)\quad\text{for all }s' \in \mathcal{S}
\label{eq:balance} \\
&amp; \delta(s) \pi(a \mid s) \ge 0 \quad\text{for } s \in \mathcal{S}, a \in \mathcal{A}
\end{cases}
$$</div>
<p>TODO: we could take <span class="math">\(\mu(s,a) = \delta(s') \pi(a' \mid s')\)</span> to be an extra
family of constraints in the problem.  Basically, just an intermediate quantity
in the circuit.</p>
<p>Written as above, this optimization problem is a quadratic program with
quadractic equality constraints.<sup id="fnref-qp"><a class="footnote-ref" href="#fn-qp">3</a></sup> Lucky for us, this optimization
problem&mdash;as a function of <span class="math">\(\mu(s,a) \overset{\text{def}}{=} \delta(s)\pi(a
\mid s)\)</span>&mdash;is a linear program!  Performing the substitution gives the
following optimization problem,</p>
<div class="math">$$
\begin{cases}
\underset{\mu}{\textrm{maximize }} &amp; \sum_{s,a} r(s,a) \cdot \mu(s, a) \\
\text{subject to} &amp;
\sum_{a'} \mu(s',a') = \sum_{s,a} \mu(s,a) \cdot P(s' \mid s,a)\quad\text{for all }s' \in \mathcal{S} \\
&amp; \mu(s, a) \ge 0 \quad\text{for all }s \in \mathcal{S}, a \in \mathcal{A}
\end{cases}
$$</div>
<p>The linear programming version formulation suggests allows us to use a number of
efficient (polytime) algorithms.  Once solved, we can recover our original
variables as <span class="math">\(\delta(s) \mapsto \sum_a \mu(s,a)\)</span> and <span class="math">\(\pi(a \mid s) \mapsto
\frac{\mu(s,a)}{\delta(s)}\)</span>.<sup id="fnref-ties"><a class="footnote-ref" href="#fn-ties">2</a></sup> The linear programming formulation is
attributed to Manne (1960); however, the best resource on the topic is Wang et
al. (2008).<sup id="fnref-W08"><a class="footnote-ref" href="#fn-W08">9</a></sup></p>
<p>Lemma 1 says the solution to the balance equation are always normalized.</p>
<p>The average-reward case, on the other hand, needs an explicit sum-to-one
  constraint.  This explains why the average-reward settings value functions are
  different from the discounted case.</p>
<h3>The Lagrangian</h3>
<!--
$$
\begin{eqnarray*}
\mathcal{L}(\delta, \pi, \lambda, \sigma, \zeta, \eta)
&=& \sum_{s,a} r(s,a) \delta(s) \pi(a \mid s)\\
&& + \sum_{s'} \lambda(s') \delta(s')  - \sum_{s,a} \lambda(s') \delta(s) \pi(a \mid s) p(s' \mid s,a)\\
&& + \sum_s \sigma(s)  - \sum_a \sigma(s) \pi(a \mid s)\\
&& + \sum_{s,a} \zeta(s,a) \pi(a \mid s) \\
&& + \sum_{s} \eta(s) \delta(a \mid s)
\end{eqnarray*}
$$
--->

<div class="math">$$
\begin{align*}
\mathcal{L}(\delta, \pi, \lambda, \eta) \overset{\text{def}}{=}
&amp; \sum_{s,a} r(s,a) \cdot \delta(s) \pi(a \mid s) \\
&amp; + \sum_{s'} \lambda(s') \cdot \left( (1-\gamma) p_0(s') + \gamma \sum_{s,a} \delta(s) \pi(a \mid s) \cdot P(s' \mid s,a) - \sum_{a'} \delta(s') \pi(a' \mid s')\right) \\
&amp; + \sum_{s, a} \eta(s,a) \cdot \delta(s) \pi(a \mid s)
\end{align*}
$$</div>
<p>with the constraint that <span class="math">\(\vec{\eta} \ge 0\)</span>.</p>
<p>Now, we will working out the gradient of <span class="math">\(\mathcal{L}\)</span> with respect each
parameter type.  We will also collect the first-order optimality conditions for
setting the gradient with respect to each parameter type to zero.</p>
<div class="math">$$
\begin{align*}
\nabla \mathcal{L} =
\nabla\Biggr[
&amp; \sum_{s,a} r(s,a) \cdot \delta(s) \pi(a \mid s) \\
&amp; + \sum_{s'} \lambda(s') (1-\gamma) p_0(s') \\
&amp; + \gamma \sum_{s,a,s'} \lambda(s') \delta(s) \pi(a \mid s) \cdot P(s' \mid s,a) \\
&amp; - \sum_{s', a'} \lambda(s') \delta(s') \pi(a' \mid s') \\
&amp; + \sum_{s, a} \eta(s,a) \cdot \delta(s) \pi(a \mid s) \Biggr]
\end{align*}
$$</div>
<p>All of the derivatives of the Lagrangian are very easy to take because the
Lagrangian is multi-linear in each the individual variables (it is bi-linear in
<span class="math">\(\delta \cdot \pi\)</span> and <span class="math">\(\lambda\)</span>, but tri-linear if <span class="math">\(\pi\)</span> and <span class="math">\(\delta\)</span> and
<span class="math">\(\lambda\)</span> can move independently).  The study of bi-linear forms is pretty
common in game theory.</p>
<h3>Conditions from <span class="math">\(\mu\)</span></h3>
<p>Lagrangian conditions for differentiation with respect to <span class="math">\(\mu(s^*, a^*) =
\delta(s^*) \pi(a^* \mid s^*)\)</span>.</p>
<div class="math">$$
\begin{align*}
\nabla_{\delta(s^*) \pi(a^* \mid s^*)} \mathcal{L}
=\,
&amp; r(s^*, a^*) \\
&amp; + \gamma \sum_{s'} \lambda(s') \cdot P(s' \mid s^*, a^*)
- \lambda(s^*) \\
&amp; + \eta(s^*, a^*)
\end{align*}
$$</div>
<p>Equating with <span class="math">\(0\)</span> and solving for <span class="math">\(\lambda(s^*)\)</span></p>
<div class="math">$$
\begin{eqnarray*}
\nabla_{\delta(s^*) \pi(a^* \mid s^*)} \mathcal{L} &amp;=&amp; 0  \\
&amp;\Leftrightarrow&amp; \\
\lambda(s^*) - \eta(s^*, a^*)
&amp;=&amp; r(s^*, a^*) + \gamma \sum_{s'} \lambda(s') \cdot P(s' \mid s^*, a^*)\quad\text{for all }s^*, a^*.
\end{eqnarray*}
$$</div>
<p><em>Slack interpretation</em>: Notice that <span class="math">\(\eta(s,a)\)</span> is a just a slack variable in
these equations.  Thus, we can treat interpret these equations as inequality
constraints,
</p>
<div class="math">$$
\begin{eqnarray*}
\lambda(s^*) &amp;\ge&amp; r(s^*, a^*) + \gamma \sum_{s'}
\lambda(s') \cdot P(s' \mid s^*, a^*)\quad\text{for all }s^*, a^*.
\end{eqnarray*}
$$</div>
<p>These are precisely the constraints in the optimal value-function problem.<sup id="fnref-nonlinear-VF"><a class="footnote-ref" href="#fn-nonlinear-VF">4</a></sup></p>
<h3>Conditions from <span class="math">\(\lambda\)</span></h3>
<p>As usual, setting the derivative with respect to the a Lagrange
multiplier&mdash;in our case <span class="math">\(\lambda\)</span>&mdash;to zero results in a condition
which says that the original equallity constraint should be satisfied, that is</p>
<div class="math">$$
\begin{eqnarray*}
\nabla_{\lambda(s^*)} \mathcal{L} &amp;=&amp; 0  \\
&amp;\Leftrightarrow&amp; \\
\sum_{a'} \delta(s^*) \pi(a' \mid s^*)
&amp;=&amp; (1-\gamma) p_0(s^*) + \gamma \sum_{s,a} \delta(s) \pi(a \mid s) \cdot P(s^* \mid s,a)
\end{eqnarray*}
$$</div>
<h3>What happens if we differentiate w.r.t. <span class="math">\(\delta\)</span> or <span class="math">\(\pi\)</span> separately?</h3>
<h4>Conditions from <span class="math">\(\pi( a^* \mid s^*)\)</span></h4>
<div class="math">$$
\begin{align*}
\nabla_{\pi( a^* \mid s^*)} \mathcal{L}
=\,
&amp; r(s^*, a^*) \cdot \delta(s^*) \\
&amp; +
\sum_{s'} \lambda(s') \delta(s^*) \cdot P(s' \mid s^*,a^*)
-
\lambda(s^*) \delta(s^*) \\
&amp; + \eta(s^*, a^*) \cdot \delta(s^*) \\
=\,
&amp; \delta(s^*) \left(
r(s,a) + \gamma \sum_{s'} \lambda(s') \cdot P(s' \mid s^*,a^*) - \lambda(s^*)  + \eta(s^*, a^*)
\right)
\end{align*}
$$</div>
<p>We can solve for <span class="math">\(\lambda\)</span> such that the gradient is zero for all <span class="math">\(s^*\)</span>, which
gives us (assuming that <span class="math">\(\delta \ne 0\)</span>),</p>
<div class="math">$$
\lambda(s^*) =
r(s^*,a^*) + \gamma \sum_{s'} \lambda(s') \cdot P(s' \mid s^*,a^*) + \eta(s^*, a^*)
$$</div>
<p>Same as earlier version. <span class="math">\(\eta\)</span> is a slack variable.</p>
<h4>Conditions from <span class="math">\(\delta(s^*)\)</span></h4>
<div class="math">$$
\begin{align*}
\nabla_{\delta( s^*)} \mathcal{L}
=\,
&amp; \sum_{a} r(s^*, a) \cdot \pi(a \mid s^*) \\
&amp; + \gamma \sum_{s'} \sum_{a} \lambda(s') \pi(a \mid s^*) \cdot P(s' \mid s^*, a) \\
&amp; - \lambda(s^*) \\
&amp; + \sum_{a} \eta(s^*,a) \cdot \pi(a \mid s^*)
\end{align*}
$$</div>
<p>This gives us the value of a fixed policy as an implicit function of setting
this derivative equal to zero.</p>
<h3>Summary</h3>
<p>We also see that the slack variable <span class="math">\(\eta(s^*, a^*)\)</span> is the disadvantage
function.</p>
<p>In other words,</p>
<p><span class="math">\(\lambda(s) = V(s)\)</span> is a dual variable for primal constraint <span class="math">\((s)\)</span>,</p>
<p><span class="math">\(\eta(s,a) = -A(s,a)\)</span> is the slack variable in the dual constraint <span class="math">\((s,a)\)</span></p>
<p><span class="math">\(\lambda(s) + \eta(s,a) = Q(s,a)\)</span></p>
<p>No duality gap: Show Slater's conditions hold when no function approximation is
used.  What do we want to say about the function approximation case?</p>
<h3>The Lagrangian dual problem</h3>
<p>The Lagrange dual problem simplifies (under the interpretation of <span class="math">\(\eta\)</span> as a
slack variable) to</p>
<div class="math">$$
\begin{cases}
\underset{\lambda}{\textrm{minimize }} &amp; \sum_{s} p_0(s) \cdot \lambda(s) \\
\text{subject to } &amp;
\lambda(s) \ge r(s, a)
+ \gamma \sum_{s'} P(s' \mid s,a) \lambda(s') \quad\text{for all }s \in \mathcal{S}, a \in \mathcal{A}
\end{cases}
$$</div>
<h3>Implicit functions</h3>
<p>Much like in consumer theory of economics, the optimization formulation of the
MDP problem has rich connections between variables in the primal and dual.
Theses connections are meaningful (e.g., as expectations and/or partial
derivatives) and interconnected (e.g., as implicit functions of one another).
In the case of consumer theory, almost every connection is associated with a
theorem named after a Nobel Laureate.</p>
<p>In this section, we sketch a number of connections in the MDP setting here.</p>
<p><strong>TODO</strong> how do implicit functions work in the the primal-dual case?  I think
the answer is that any partial optimization create constraints (as usual),
regardless of whether or not those constraints are in the primal or the dual,
they are coupled by the Lagrangian. The generally the primal-dual coupling is
"modulated" by Lagrange multiplier estimates.</p>
<h4>Occupancy measure: <span class="math">\(\delta\)</span> as an implicit function of <span class="math">\(\pi\)</span>:</h4>
<p>If we fix <span class="math">\(\pi\)</span> and solve for a primal feasible <span class="math">\(\delta\)</span>, then we can interpret
<span class="math">\(\delta\)</span> as <span class="math">\(\pi\)</span>'s occupancy measure.<sup id="fnref-occupancy-note"><a class="footnote-ref" href="#fn-occupancy-note">6</a></sup></p>
<p>In the Lagrangian view, we have use <span class="math">\(\nabla_{\lambda} \mathcal{L} = 0\)</span>
simultaneously for all states (a type of block-wise update / "partial
optimization").</p>
<p>This version of <span class="math">\(\delta\)</span> is indexed by <span class="math">\(\pi\)</span> because it is now an implicit
function</p>
<p>Written in a vector form,
</p>
<div class="math">$$
\delta_\pi = (1-\gamma P_\pi)^{-1} p_0
$$</div>
<p>The benefit of collapsing-out the parameters in the fashion is that we eliminate
the constraint and reduced the number of parameters.<sup id="fnref-collapse-linear"><a class="footnote-ref" href="#fn-collapse-linear">5</a></sup></p>
<p><strong>TODO</strong>: Half-baked thought: As a function of <span class="math">\(\mu\)</span>, we have an underdetermined
linear system (S constraints with SA unknowns).  What happens to the
optimization problem if we perform the usual linear-equality elimation trick?
Clearly, we reduce the number of parameters from SA to S.  The trick uses the
null space of the linear system (which is a linear mapping from <span class="math">\(\mu \in
\mathbb{R}^{S \times A}\)</span> to <span class="math">\(\mu': \mathbb{R}^{S}\)</span>).  This reparameterization
might be interesting. XXX: maybe things breakdown because of the revised
positivity constraints.</p>
<h3>A "feed-forward" view of the policy-search problem</h3>
<div class="math">$$
\textbf{input: } \pi \\
d = (1 - \gamma P_\pi)^{-\top} (1-\gamma) p_0 \\
\mu = d \pi \\
\textbf{return } U = r \mu
$$</div>
<p>An alternative circuit
</p>
<div class="math">$$
\textbf{input: } \pi \\
v = (1-\gamma P_\pi)^{-1} r \\
\textbf{return } U = p_0^\top v / (1-\gamma)
$$</div>
<p>The gradient is
</p>
<div class="math">$$
d \pi (r + \gamma P_\pi v)
$$</div>
<p>We can get that by implicit differentiation under the d-stationarity constraint.</p>
<p>max r * d * pi
s.t. stationary(d, pi)</p>
<p>max  reward(d0, pi+eps) =
     reward(d0, pi) + eps * dU/dpi
s.t.
     d0 = stationary(pi+eps)
     d = stationary(d, pi) + eps * dd/dpi = 0</p>
<div class="highlight"><pre><span></span><span class="err">#</span><span class="w"> </span><span class="n">dx</span><span class="o">/</span><span class="n">dA</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="w"></span>
<span class="n">dA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">).</span><span class="n">T</span><span class="w"> </span><span class="err">@</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">g</span><span class="p">).</span><span class="n">T</span><span class="w"> </span><span class="err">@</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"></span>
<span class="n">fdcheck</span><span class="p">(</span><span class="nl">lambda</span><span class="p">:</span><span class="w"> </span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">dA</span><span class="p">)</span><span class="w"></span>
</pre></div>


<h4>Value function: <span class="math">\(V\)</span> as an implicit function of <span class="math">\(\pi\)</span></h4>
<p>For a fixed policy, (WHAT ABOUT <span class="math">\(\delta\)</span>?)
if we set <span class="math">\(\nabla_{???} \mathcal{L} = 0\)</span>,
the Lagrange mulipliers associated with constraint <span class="math">\((\ref{eq:balance})\)</span> become
another implicit function <span class="math">\(V_\pi\)</span> of the policy</p>
<p>We can estimate the Lagrange multipler (by L2 projection into the dual space?)
for that policy.  It will be infeasible wrt to the optimization problem
(TODO:XREF), however.  The problem is a simple fully determined linear system</p>
<div class="math">$$
V_\pi = (1-\gamma P \pi)^{-1} r
$$</div>
<p>Varying <span class="math">\(\pi\)</span> under this mapping in nonconvex in the space of <span class="math">\(V_\pi\)</span>
(cite:VF-polytope paper)</p>
<h4>Successor representation?</h4>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">successor_representation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s2">&quot;Dayan&#39;s successor representation.&quot;</span>
    <span class="k">return</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">,</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">))</span>
</pre></div>


<p><strong>TODO</strong> featurized version of SR.</p>
<p><strong>TODO</strong> Describe Wang et al's policy-improvement friendly SA x SA matrix.</p>
<h2>TODO</h2>
<p><strong>TODO</strong> duality explains the why the policy gradient can be written in terms of
  the dual variables V/Q</p>
<p><strong>TODO</strong> Formulate the Lagrangian.  Take its derivatives.  Talk about all the
  implicit functions and connections.</p>
<p><strong>TODO</strong> saddle-point problems and optimization instability, best-response and
  positive response; regularization.  Go thru notes on the saddle-point
  formulation.</p>
<p><strong>TODO</strong> We can get a different Lagrangian by starting from the value function
  optimization problem -- that is the approach take in Dai et al.</p>
<p><strong>TODO</strong> Take the LP dual, note that it is exactly the optimal value function
  problem.</p>
<p><strong>TODO</strong> <a href="https://arxiv.org/pdf/1405.6757.pdf">Proximal RL</a></p>
<p><strong>TODO</strong> The S-procedure might fit in here.  The Lagrange dual of the QP
  formulation has a no duality gap, which is the main consequence of the
  S-procedure in control theory.  The reason why this is true is because once we
  take the Lagrange dual the problem can be simplified into another LP.  This is
  pretty trivial in our case because the original problem is sort of an LP too.</p>
<p><strong>TODO</strong> <a href="http://www.argmin.net/2016/05/18/mates-of-costate/">Mates of costate</a></p>
<p><strong>TODO</strong> if we write an equivalent set of constraints, what happen to the dual
  variables (e.g., do we get Q or A under different variations)?  Or are these
  other things just different implicit functions?</p>
<p><strong>TODO</strong> Block-coordinate method - is policy iteration.  Just like my backprop
  post, we can take any fixed policy and estimate the Lagrange mulipliers by
  setting the gradient of the Lagrangian wrt <span class="math">\(V\)</span> equal to zero given what every
  <span class="math">\(\pi\)</span> is.</p>
<p><strong>TODO</strong> The successor representation is the dual of this connection -- They are
  Lagrange multiplier estimates of the in the primal problem.</p>
<p><strong>TODO</strong> Value function polytope
  https://arxiv.org/pdf/1901.11524.pdf</p>
<p><strong>TODO</strong> Consider adding entropic regularization to the discussion.  At the very
  least add references to Belousov &amp; Peters. (2017)<sup id="fnref-BP17"><a class="footnote-ref" href="#fn-BP17">8</a></sup></p>
<p><strong>TODO</strong> The averge-reward formulation of V/Q functions are a little
  different.  Figure out why.</p>
<p>Q(s,a) = R(s,a) - Rbar + \sum_{s'} T(s'| s,a) V(s')</p>
<p>(Is the Rbar because of the sum-to-one constraint?)</p>
<p><strong>TODO</strong> the diagram in this consumer theory tutorial is great.
  https://policonomics.com/marshallian-hicksian-demand-curves/</p>
<p><strong>TODO</strong> Optimization-based view of MDP [^KBP13] <sup id="fnref2-BP17"><a class="footnote-ref" href="#fn-BP17">8</a></sup> <sup id="fnref2-W08"><a class="footnote-ref" href="#fn-W08">9</a></sup></p>
<p><strong>TODO</strong> the "line theorem" in VF-polytope extends to all factored probability
   model (PGMs) and case-factor diagrams (CFDs) more generally.  The laws of
   probability are such that all probabilistic statements are multi-linear
   polynomials (MPs): the basic operations are chain-rule decompositions
   (times), marginalization (sum), and Bayes rule (div); additionally, it is
   never ok to multiply p(A, ... | ...) * p(A, ... | ...).  I am not sure how
   Bayes rule works yet, but it is definitely the case that restricting to the
   semiring (times-sum) part results in MPs.  I think that CFDs don't allow
   division.  They do allow case statements unlike PGMs.</p>
<h2>Extensions</h2>
<h3>Average reward</h3>
<div style="border: thin solid black; padding: 10px; background-color: #ffffcc;
margin-bottom: 1.5em;">

The average-reward formulation
$$
U(\pi) \overset{\text{def}}{=} \lim_{T \rightarrow \infty} \frac{1}{T} \mathbb{E}\left[  \sum_{t=0}^T r(s,a) \right]
$$

The follow mathematical program formalizes the average reward optimization
problem.  The main difference is the balance equation is slightly different and
we now require an explicit sum-to-one constraint that we didn't need in the
discounted case.  Also the $1/(1-\gamma)$ is gone from the objective function.

$$
\begin{cases}
\underset{\pi, \delta}{\textrm{maximize }} & \sum_{s,a} r(s,a) \cdot \delta(s) \pi(a \mid s) \\
\text{subject to } &
\sum_{a'} \delta(s') \pi(a' \mid s') = \sum_{s,a} \delta(s) \pi(a \mid s) \cdot P(s' \mid s,a)\quad\text{for all }s' \in \mathcal{S} \\
& \sum_{s,a} \delta(s) \pi(a \mid s) = 1 \\
& \delta(s) \pi(a \mid s) \ge 0 \quad\text{for } s \in \mathcal{S}, a \in \mathcal{A}
\end{cases}
$$

Note that all of the implicit functions for the average reward case are slightly
different.  The reason why is the extra sum-to-one constraint!

</div>

<h2>Dynamic programming inference in graphical models</h2>
<p>The concept of a value function is not limited to RL: value functions arise in
the dynamic programming solutions to many other problems.</p>
<p>Building on
<a href="http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">Vieira (2017)</a>,
the gradient estimates provided by backpropagation can be viewed as Lagrange
muliplier estimates in a particular formulation of an optimization problem with
intermediate variables that are defined by equality constraints on intermediate
quantities.</p>
<p>Consider the case of the forward-backward algorithm for linear-chain CRFs or
HMMs.  As detailed in
<a href="https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf">Eisner (2016)</a>, the
backward algorithm and the outside algorithm are <em>precisely</em> the result of
backpropgation on the forward algorithm and inside algorithm respectively.</p>
<p>Much like the MDP setting, the backward algorithm and outside algorithm are
often viewed as some useful quantities.  However, when viewed as the result of
backpropagation it not only deepens our understanding of the connection, but
also establishes a bridge to the rich theory that underlies automatic
differentiation.  This connection tells us important things about the time and
space complexity of algorithms for computing these quantities and even better
gives us a recipe for efficiently computing these quantities backward/outside
quantities.</p>
<p>This connection extends to marginal inference in Bayesian networks more
generally Darwiche (2003).<sup id="fnref-D03"><a class="footnote-ref" href="#fn-D03">11</a></sup></p>
<div class="footnote">
<hr>
<ol>
<li id="fn-unnecessary-constraints">
<p>We also want <span class="math">\(\delta(s) \pi(a \mid s)\)</span> to be a valid
  joint state-action distribution and for <span class="math">\(\pi\)</span> to be a valid conditional
  distribution.
  <div class="math">$$
  \begin{cases}
  &amp;\sum_{s,a} \delta(s) \pi(a \mid s) = 1
  &amp;\textstyle\sum_a \pi(a \mid s) = 1 \quad\text{for all } s \in \mathcal{S} \\
  &amp;\pi(a \mid s) \ge 0 \quad\text{for all } s \in \mathcal{S}, a \in \mathcal{A}
  \end{cases}
  $$</div>
  It turns out that we don't need to explicitly add these constraints because
  they are implied by the existing balance constraints and assumptions that <span class="math">\(P\)</span>
  is a stochastic matrix.&#160;<a class="footnote-backref" href="#fnref-unnecessary-constraints" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn-ties">
<p>If ever <span class="math">\(\delta(s)=0\)</span>, the choice for <span class="math">\(\pi(a \mid s)\)</span> is any valid
  distribution over <span class="math">\(a\)</span>.&#160;<a class="footnote-backref" href="#fnref-ties" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn-qp">
<p>Optimization with quadratic equality constraints are generally NP-Hard to
  solve.  To see why, consider the constraint <span class="math">\(x \cdot (x
  - 1) = 0\)</span>, this constraint has exactly two solutions <span class="math">\(x \in \{0, 1\}\)</span>.  In
  other words, it is an encoding of binary variables, and thus can be used to
  solve NP-hard integer programming problems.&#160;<a class="footnote-backref" href="#fnref-qp" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn-nonlinear-VF">
<p>The optimal value function is more familiar in it's non-linear equational form,
  <div class="math">$$
  \lambda(s^*)
  = \max_a r(s^*, a) + \gamma \sum_{s'} \lambda(s') \cdot P(s' \mid s^*, a)\quad\text{for all }s^*
  $$</div>&#160;<a class="footnote-backref" href="#fnref-nonlinear-VF" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn-collapse-linear">
<p>We can always removed linear equality constraints, even if
   the linear system is under-determined.  An over-determined system is, of
   course, infeasible.  XXX: reference to linear-equality elimination trick.&#160;<a class="footnote-backref" href="#fnref-collapse-linear" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn-occupancy-note">
<p>For the average reward case, we use <span class="math">\(P(s' \mid s, a) = p(s' \mid s, a)\)</span>.
  We may run into issues with feasibility if the transition function is not unichain (ergodic for all policies).
  For discounted case use <span class="math">\(P(s' \mid s, a) = (1-\gamma) \cdot p_0(s') + \gamma \cdot p(s' \mid s,a)\)</span>.
  In the discounted cases, we can think of <span class="math">\(\delta\)</span> as <span class="math">\(\pi\)</span>'s stationary distribution if we regard <span class="math">\((1-\gamma)\)</span> as the probability of restarting the Markov chain (i.e., sampling the next state from a mixture of <span class="math">\(p_0(s')\)</span> with probability <span class="math">\((1-\gamma)\)</span> and from <span class="math">\(p(s' \mid s,a)\)</span> with probability <span class="math">\(\gamma\)</span>).  This is an equivalent in expected reward, but not higher-order moments of reward, i.e., it's not equivalent in distribution.&#160;<a class="footnote-backref" href="#fnref-occupancy-note" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn-P13">
<p>Kober, Bagnell, &amp; Peters. 2013
  <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.910.7004&amp;rep=rep1&amp;type=pdf">Reinforcement learning in robotics: A survey</a>&#160;<a class="footnote-backref" href="#fnref-P13" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn-BP17">
<p>Belousov &amp; Peters. 2017.
  <a href="https://arxiv.org/abs/1801.00056">f-Divergence constrained policy improvement</a>&#160;<a class="footnote-backref" href="#fnref-BP17" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2-BP17" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn-W08">
<p>Wang, Lizotte, Bowling, &amp; Schuurmans. 2008.
  <a href="https://webdocs.cs.ualberta.ca/~dale/papers/dualdp.pdf">Dual Representations for Dynamic Programming</a>&#160;<a class="footnote-backref" href="#fnref-W08" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2-W08" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn-LM04">
<p>Jonathan Levin and Paul Milgrom. 2004.
  <a href="https://web.stanford.edu/~jdlevin/Econ%20202/Consumer%20Theory.pdf">Consumer Theory</a>&#160;<a class="footnote-backref" href="#fnref-LM04" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn-D03">
<p>Adnan Darwiche.
  <a href="https://dl.acm.org/citation.cfm?id=765570">A differential approach to inference in Bayesian networks</a>.
  Journal of the Association for Computing Machinery, 50(3):280–305, 2003.&#160;<a class="footnote-backref" href="#fnref-D03" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Tim Vieira
    </span>
  </span>
<time datetime="2019-07-17T00:00:00-04:00" pubdate>Jul 17, 2019</time>  <span class="categories">
    <a class='category' href='../category/misc.html'>misc</a>
  </span>
  <span class="categories">
    <a class="category" href="../tag/rl.html">rl</a>,    <a class="category" href="../tag/calculus.html">calculus</a>,    <a class="category" href="../tag/lagrange-multipliers.html">Lagrange-multipliers</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="../post/2019/06/11/faster-reservoir-sampling-by-waiting/">Faster reservoir sampling by waiting</a>
      </li>
      <li class="post">
          <a href="../post/2019/04/20/the-likelihood-ratio-gradient/">The likelihood-ratio gradient</a>
      </li>
      <li class="post">
          <a href="../post/2019/04/19/steepest-ascent/">Steepest ascent</a>
      </li>
      <li class="post">
          <a href="../post/2018/03/16/black-box-optimization/">Black-box optimization</a>
      </li>
      <li class="post">
          <a href="../post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="../tag/sampling.html">sampling</a>,    <a href="../tag/reservoir-sampling.html">reservoir-sampling</a>,    <a href="../tag/gumbel.html">Gumbel</a>,    <a href="../tag/sampling-without-replacement.html">sampling-without-replacement</a>,    <a href="../tag/optimization.html">optimization</a>,    <a href="../tag/rl.html">rl</a>,    <a href="../tag/machine-learning.html">machine-learning</a>,    <a href="../tag/notebook.html">notebook</a>,    <a href="../tag/calculus.html">calculus</a>,    <a href="../tag/automatic-differentiation.html">automatic-differentiation</a>,    <a href="../tag/implicit-function-theorem.html">implicit-function-theorem</a>,    <a href="../tag/lagrange-multipliers.html">Lagrange-multipliers</a>,    <a href="../tag/statistics.html">statistics</a>,    <a href="../tag/testing.html">testing</a>,    <a href="../tag/counterfactual-reasoning.html">counterfactual-reasoning</a>,    <a href="../tag/importance-sampling.html">importance-sampling</a>,    <a href="../tag/datastructures.html">datastructures</a>,    <a href="../tag/incremental-computation.html">incremental-computation</a>,    <a href="../tag/algorithms.html">algorithms</a>,    <a href="../tag/data-structures.html">data-structures</a>,    <a href="../tag/rant.html">rant</a>,    <a href="../tag/decision-making.html">decision-making</a>,    <a href="../tag/hyperparameter-optimization.html">hyperparameter-optimization</a>,    <a href="../tag/numerical.html">numerical</a>,    <a href="../tag/crf.html">crf</a>,    <a href="../tag/deep-learning.html">deep-learning</a>,    <a href="../tag/structured-prediction.html">structured-prediction</a>,    <a href="../tag/visualization.html">visualization</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014&ndash;2019  Tim Vieira &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="../theme/js/modernizr-2.0.js"></script>
  <script src="../theme/js/ender.js"></script>
  <script src="../theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'graduatedescent';
    var disqus_identifier = '/drafts/value-functions-as-lagrange-multiplier-estimates.html';
    var disqus_url = '../drafts/value-functions-as-lagrange-multiplier-estimates.html';
    var disqus_title = 'Value functions as Lagrange multiplier estimates';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-8781169-1', 'auto');
  ga('send', 'pageview');
</script>

  
</body>
</html>