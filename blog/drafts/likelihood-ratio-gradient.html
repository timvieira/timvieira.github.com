<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Likelihood-ratio gradient &mdash; Graduate Descent</title>
  <meta name="author" content="Tim Vieira">

  <link href="/blog/atom.xml" type="application/atom+xml" rel="alternate"
        title="Graduate Descent Atom Feed" />





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="robots" content="noindex, nofollow" />

    <link href="../favicon.png" rel="icon">

  <link href="../theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="../">Graduate Descent</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-atom">Atom</a></li>
</ul>


<ul class="main-navigation">
    <li><a href="http://timvieira.github.io/">About</a></li>
    <li><a href="/blog/archives.html">Archive</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Likelihood-ratio gradient</h1>
    <p class="meta">
<time datetime="2016-06-09T00:00:00-04:00" pubdate>Jun 09, 2016</time>    </p>
</header>

  <div class="entry-content"><p><strong>Setup</strong>: We're trying to optimize a function of the form</p>
<div class="math">$$
J(\theta) = \underset{p_\theta}{\mathbb{E}} \left[ r(x) \right] = \sum_{x \in \mathcal{X}} p_\theta(x) r(x).
$$</div>
<p>The problem is that it's too costly to evaluate <span class="math">\(r(x)\)</span> for all <span class="math">\(\mathcal{X}\)</span> and
that <span class="math">\(p\)</span> times <span class="math">\(r\)</span> has no structure which we can exploit (e.g., for dynamic
programming). We also do not assume full knowledge of <span class="math">\(r(x)\)</span> and evaluations of
<span class="math">\(r(x)\)</span> might be noisy.</p>
<p>Suppose we can sample <span class="math">\(x^{(j)} \sim p_\theta\)</span>. This opens up the following
(unbiased) Monte Carlo estimators for <span class="math">\(J\)</span> and its gradient,</p>
<div class="math">$$
J(\theta) \approx \frac{1}{m} \sum_{j=1}^m r(x^{(j)})
$$</div>
<div class="math">$$
\nabla_{\!\theta} J(\theta) \approx \frac{1}{m} \sum_{j=1}^m r(x^{(j)}) \nabla_{\!\theta} \log p_{\theta}(x^{(j)}).
$$</div>
<style>
.toggle-button {
    background-color: #555555;
    border: none;
    color: white;
    padding: 10px 15px;
    border-radius: 6px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    cursor: pointer;
}
.derivation {
  background-color: #f2f2f2;
  border: thin solid #ddd;
  padding: 10px;
  margin-bottom: 10px;
}
</style>

<script>
// workaround for when markdown/mathjax gets confused by the
// javascript dollar function.
function toggle(x) { $(x).toggle(); }
</script>

<p><button class="toggle-button" onclick="toggle('#likelihood-ratio-derivation');">Derivation</button>
<div id="likelihood-ratio-derivation" class="derivation">
The derivation is pretty simple
</p>
<div class="math">$$
\begin{eqnarray}
  \nabla_{\!\theta} \, \underset{p_\theta}{\mathbb{E}}\left[ r(x) \right]
  &amp;=&amp; \nabla_{\!\theta} \left[ \sum_x p_{\theta}(x) r(x) \right] \\
  &amp;=&amp; \sum_x \nabla_{\!\theta} \left[ p_{\theta}(x) \right] r(x) \\
  &amp;=&amp; \sum_x p_{\theta}(x) \frac{\nabla_{\!\theta} \left[ p_{\theta}(x) \right] }{ p_{\theta}(x) } r(x) \\
  &amp;=&amp; \underset{p}{\mathbb{E}}\left[ r(x) \nabla_{\!\theta} \log p_\theta(x) \right]
\end{eqnarray}
$$</div>
<p><br/>
We use the identity <span class="math">\(\nabla g = g\, \nabla \log g\)</span>, assuming <span class="math">\(g &gt; 0\)</span>.</p>
<p>Importance-weighted version:
</p>
<div class="math">$$
\begin{eqnarray}
  \nabla_{\!\theta} \, \underset{p_\theta}{\mathbb{E}}\left[ r(x) \right]
  &amp;=&amp; \underset{p}{\mathbb{E}}\left[ r(x) \nabla_{\!\theta} \log p_\theta(x) \right] \\
  &amp;=&amp; \sum_x p_{\theta}(x) r(x) \nabla_{\!\theta} \log p_\theta(x) \\
  &amp;=&amp; \sum_x \frac{q(x)}{q(x)} p_{\theta}(x) r(x) \nabla_{\!\theta} \log p_\theta(x) \\
  &amp;=&amp; \underset{q}{\mathbb{E}}\left[ \frac{p_{\theta}(x)}{q(x)} r(x) \nabla_{\!\theta} \log p_\theta(x) \right]
\end{eqnarray}
$$</div>
<p>
</div></p>
<p>Two use this estimator we need two things (1) the ability to sample a joint <span class="math">\(x
\sim p_{\theta}\)</span>, (2) the ability to compute the probability of a sampled value,
i.e., evaluate <span class="math">\(\log p_{\theta}(x)\)</span>.</p>
<p>The real power of this method is when you have the ability to sample <span class="math">\(x\)</span>, but
<em>not</em> the ability to compute probability of all factors of the joint probability
of <span class="math">\(x\)</span> (i.e., you can't compute the complete score <span class="math">\(p_{\theta}(x)\)</span>). In other
words, some components of the joint probability's <em>generative process</em> might
pass through factors which are <em>only accessible through sampling</em>, e.g., because
they require performing <em>actual experiments</em> in the real world or a complex
simulation!</p>
<p>The factors that we can only sample from are what make this a true stochastic
optimization problem.</p>
<p>A classic example is a Markov decision process (MDP). In this context, the
random variable <span class="math">\(x\)</span> is an alternating sequence of states and actions, <span class="math">\(x = (s_0,
a_0, s_1, a_1, \ldots a_{T-1}, s_T)\)</span> and the generative process consists of an
unknown transition function <span class="math">\(p(s_{t+1}|s_t,a_t)\)</span> that is only accessible through
sampling and a policy <span class="math">\(p_{\theta}(a_t|s_t)\)</span> which we in control of. So the
probability of an entire sequence in an MDP is <span class="math">\(p_{\theta}(x) = p(s_0)
\prod_{t=0}^T p(s_{t+1}|s_t,a_t) \pi_\theta(a_t|s_t)\)</span>. The likelihood-ratio
method can be used to derive several "policy gradient" methods, which compute
unbiased gradient estimates with no knowledge of the transition distribution.</p>
<blockquote>
<p>The beauty of the likelihood ratio is the cancellation of unknown terms.</p>
</blockquote>
<p>This wonderful cancellation occurs in many contexts, including the
Metropolis-Hastings accept-reject criteria.</p>
<p>To make this explicit, let's consider the importance weight, <span class="math">\(w(\tau)\)</span>.
</p>
<div class="math">\begin{eqnarray}
w(\tau)
= \frac{p(\tau|\pi_\theta)}{q(\tau)}
= \frac{ p(s_0) \prod_{t=0}^T p(s_{t+1}|s_t,a_t) \pi_\theta(a_t|s_t) }
       { p(s_0) \prod_{t=0}^T p(s_{t+1}|s_t,a_t) q(a_t|s_t) }
= \frac{\prod_{t=0}^T \pi_\theta(a_t|s_t)}
       {\prod_{t=0}^T q(a_t|s_t)}
\end{eqnarray}</div>
<p><br/>
Common terms cancel! This implies that we don't need to compute them.</p>
<p>But that's only talking about the importance-weighted version... Well, another
interpretation of the log-probability is that related to cancellation.</p>
<p>Also, note that the component <span class="math">\(\nabla_{\!\theta} \log p_{\theta}(x)\)</span> also
simplifies because terms that do not depend on <span class="math">\(\theta\)</span> also disappear. Leaving
you with just a sum of log-gradient terms you know.</p>
<p>The general framework (along with a bunch of tricks and extensions) is presented
in <a href="http://arxiv.org/abs/1209.2355">Bottou et. al (2013)</a>). This is one of the
best papers I have ever read. It took mean about a year and several reads to
really grokk it.</p>
<p><hr/>
<hr/>
<hr/></p>
<p>You can improve your data efficiency and algorithm stability using off-line
optimization.</p>
<ul>
<li><strong>Off-line optimization</strong>: After you've collected a large sample (big <span class="math">\(m\)</span>) you
  can optimze <span class="math">\(\hat{J}\)</span> using your favorite deterministic optimization algorithm
  (e.g., L-BFGS). You'll definitely want some type of "regularization" which
  prefers policies in places with sufficient samples. You can measure this type
  of thing in many ways (e.g., Bottou; Levine &amp; Koltun, ; Philip Thomas, ; Tang &amp;
  Abbeel, 2010). The original paper on this topic is (probably) "Learning from
  scarce experience" (Peshkin &amp; Shelton, 2002) or Shelton's thesis. A similar
  deterministic approximation appears in PEGASUS (Ng &amp; Jordan, 2000).</li>
</ul>
<p>Another case where its tempting to apply policy gradient is in minimum risk
training of structured prediction models. Unfortunately, the likelihood-ratio
trick doesn't help us with the usual computation problems in structure
prediction, which have to do with computing normalization constants, but
assuming we can obtain good sample&mdash;preferably exact samples, but MCMC
samples might be ok&mdash;the likelihood ratio can help us with complicated
blackbox cost functions like human annotators or impenetrable perl scripts. I
had this idea back in 2012, but never got around to pushing it out. There appear
to be some recent papers that picked up on, including
<a href="http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ACL2016.pdf">Sokolov+,2016</a>
and a few papers using it for variational inference.</p>
<p>I'd like to stress an important point. Although the likelihood-ratio gives us an
unbiased estimate of the gradient, don't be fooled.</p>
<p>The particular gradient estimate used in the method has an impractical
signal-to-noise ratio, which make it very hard use in optimization.</p>
<p>this does not mean that you get the convergence rate that you might be used to
with your stochastic gradient method! In particular, the gradient estimates will
have high variance as it depends on variation in <span class="math">\(r(x)\)</span>, which might be
crazy-large if, for example, <span class="math">\(r(x)\)</span> is sparse corresponding to winning the
lottery, completing a maze, winning at Go, finding some poor sap to click on
your ad. Compare this to the benign amount of noisy you get from subsampling the
data, which is the sgd that people are familiar with.</p>
<p>This gradient estimate is "zero order" it is essentially probing the function in
<span class="math">\(x\)</span> space, which might be higher dimensional that <span class="math">\(\theta\)</span>. As a result, you
might be better off with gradient estimators that are based on perturbing
<span class="math">\(\theta\)</span> directly, e.g., zeroth-order methods (sometimes called <em>direct search</em>
or <em>gradient-free</em> optimization methods) like Nelder-Mead simplex, FDSA, SPSA,
and CMA-ES (related to the Cross Entropy method).</p>
<p>One way to combat this is to average together many samples per estimate (i.e.,
use large <span class="math">\(m\)</span>).</p>
<h1>Importance sampling version</h1>
<p>Let's look at a more general version based on
<a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/">importance sampling</a>
because it will give us some interesting freedom later on. Here <span class="math">\(x^{(j)} \sim q\)</span>
instead of <span class="math">\(p_\theta\)</span>. Note that we need the following condition on <span class="math">\(q\)</span> to hold
for all <span class="math">\(x\)</span>, <span class="math">\(p(x) &gt; 0 \Rightarrow q(x) &gt; 0\)</span>.</p>
<div class="math">$$
J(\theta) \approx \frac{1}{m} \sum_{j=1}^m w^{(j)}_{\theta} r(x^{(j)})
$$</div>
<div class="math">$$
\nabla_{\!\theta} J(\theta) \approx \frac{1}{m} \sum_{j=1}^m w^{(j)}_{\theta} r(x^{(j)}) \nabla_{\!\theta} \log p_{\theta}(x^{(j)}).
$$</div>
<p>where <span class="math">\(w^{(j)}_{\theta} = p_{\theta}(x^{(j)}) / q(x^{(j)})\)</span></p>
<p><strong>Remarks</strong></p>
<ul>
<li>
<p><strong>Relaxing discrete actions into stochastic ones</strong>: A common way to handle
   discrete decisions is to put a <em>differentiable</em> parametric density (like
   <span class="math">\(p_\theta\)</span>) over the space of possible executions (paths <span class="math">\(x\)</span>). (Note: this
   shouldn't be surprising -- it's what we do in structured prediction,
   e.g. with CRFs!)  The likelihood-ratio method describe here can be used to
   estimate gradients in this setting. In a number of settings the <span class="math">\(p_\theta\)</span>
   and <span class="math">\(r\)</span> decompose algebraically into a nice structure that is amenable to
   dynamic programming.</p>
</li>
<li>
<p><strong>Unknown environments</strong>: Dynamics cancel out! Note that we need to be able
   to get samples from the unknown factors in the environment/model.</p>
</li>
<li>
<p><strong>Bandit feedback</strong>: Learning under nondecomposable reward functions Policy
   gradient naturally handles "bandit feedback" (i.e., you only see the values
   of trajectories that you sample). In contrast with "full information" which
   tells you the reward of all possible trajectories.</p>
</li>
</ul>
<p><strong>Take home messages</strong>:</p>
<ul>
<li>
<p>If you can evaluate it, then you can take the gradient of it (assuming it
   exists). This even holds if the evaluation is based on Monte Carlo.</p>
</li>
<li>
<p>The likelihood-ratio shows up all over the place, not just RL. It even shows
   up in counterfactual / causal reasoning.</p>
</li>
<li>
<p>We described a general way to learn from watching someone else act in a world
   we don't understand. The only catch is that in order for us to learn from
   them we need them to do a little bit of "exploration" (and tell us their
   action probabilities).</p>
</li>
<li>
<p>Policy gradient is useful in many domains, but usually doesn't work out of
   the box. It's an interesting set of math tricks nonetheless.</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Tim Vieira
    </span>
  </span>
<time datetime="2016-06-09T00:00:00-04:00" pubdate>Jun 09, 2016</time>  <span class="categories">
    <a class='category' href='../category/misc.html'>misc</a>
  </span>
  <span class="categories">
    <a class="category" href="../tag/optimization.html">optimization</a>,    <a class="category" href="../tag/rl.html">rl</a>,    <a class="category" href="../tag/machine-learning.html">machine-learning</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="../post/2019/04/19/steepest-ascent/">Steepest ascent</a>
      </li>
      <li class="post">
          <a href="../post/2018/03/16/black-box-optimization/">Black-box optimization</a>
      </li>
      <li class="post">
          <a href="../post/2017/08/18/backprop-is-not-just-the-chain-rule/">Backprop is not just the chain rule</a>
      </li>
      <li class="post">
          <a href="../post/2017/07/03/estimating-means-in-a-finite-universe/">Estimating means in a finite universe</a>
      </li>
      <li class="post">
          <a href="../post/2017/04/21/how-to-test-gradient-implementations/">How to test gradient implementations</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="../tag/optimization.html">optimization</a>,    <a href="../tag/notebook.html">notebook</a>,    <a href="../tag/calculus.html">calculus</a>,    <a href="../tag/automatic-differentiation.html">automatic-differentiation</a>,    <a href="../tag/sampling.html">sampling</a>,    <a href="../tag/statistics.html">statistics</a>,    <a href="../tag/reservoir-sampling.html">reservoir-sampling</a>,    <a href="../tag/testing.html">testing</a>,    <a href="../tag/counterfactual-reasoning.html">counterfactual-reasoning</a>,    <a href="../tag/importance-sampling.html">importance-sampling</a>,    <a href="../tag/machine-learning.html">machine-learning</a>,    <a href="../tag/datastructures.html">datastructures</a>,    <a href="../tag/algorithms.html">algorithms</a>,    <a href="../tag/rant.html">rant</a>,    <a href="../tag/gumbel.html">Gumbel</a>,    <a href="../tag/decision-making.html">decision-making</a>,    <a href="../tag/hyperparameter-optimization.html">hyperparameter-optimization</a>,    <a href="../tag/misc.html">misc</a>,    <a href="../tag/numerical.html">numerical</a>,    <a href="../tag/crf.html">crf</a>,    <a href="../tag/deep-learning.html">deep-learning</a>,    <a href="../tag/structured-prediction.html">structured-prediction</a>,    <a href="../tag/visualization.html">visualization</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2014&ndash;2019  Tim Vieira &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="../theme/js/modernizr-2.0.js"></script>
  <script src="../theme/js/ender.js"></script>
  <script src="../theme/js/octopress.js" type="text/javascript"></script>
  <script type="text/javascript">
    var disqus_shortname = 'graduatedescent';
    var disqus_identifier = '/drafts/likelihood-ratio-gradient.html';
    var disqus_url = '../drafts/likelihood-ratio-gradient.html';
    var disqus_title = 'Likelihood-ratio gradient';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-8781169-1', 'auto');
  ga('send', 'pageview');
</script>

  
</body>
</html>